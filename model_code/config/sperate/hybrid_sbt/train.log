nohup: ignoring input
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:107: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:30: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

01/12 03:10:14 label: default
01/12 03:10:14 description:
  default configuration
  next line of description
  last line
01/12 03:10:14 /root/icpc/icpc/translate/__main__.py config/sperate/hybrid_sbt/config.yaml --train -v
01/12 03:10:14 commit hash 74e0554cb3eb5df835cef993ad570ff8de651f71
01/12 03:10:14 tensorflow version: 1.14.0
01/12 03:10:14 program arguments
01/12 03:10:14   aggregation_method   'sum'
01/12 03:10:14   align_encoder_id     0
01/12 03:10:14   allow_growth         True
01/12 03:10:14   attention_type       'global'
01/12 03:10:14   attn_filter_length   0
01/12 03:10:14   attn_filters         0
01/12 03:10:14   attn_prev_word       False
01/12 03:10:14   attn_size            128
01/12 03:10:14   attn_temperature     1.0
01/12 03:10:14   attn_window_size     0
01/12 03:10:14   average              False
01/12 03:10:14   baseline_activation  None
01/12 03:10:14   baseline_learning_rate 0.001
01/12 03:10:14   baseline_optimizer   'adam'
01/12 03:10:14   baseline_steps       0
01/12 03:10:14   batch_mode           'standard'
01/12 03:10:14   batch_size           64
01/12 03:10:14   beam_size            5
01/12 03:10:14   bidir                True
01/12 03:10:14   bidir_projection     False
01/12 03:10:14   binary               False
01/12 03:10:14   cell_size            256
01/12 03:10:14   cell_type            'GRU'
01/12 03:10:14   character_level      False
01/12 03:10:14   checkpoints          []
01/12 03:10:14   conditional_rnn      False
01/12 03:10:14   config               'config/sperate/hybrid_sbt/config.yaml'
01/12 03:10:14   convolutions         None
01/12 03:10:14   data_dir             'data/speratedata'
01/12 03:10:14   debug                False
01/12 03:10:14   decay_after_n_epoch  1
01/12 03:10:14   decay_every_n_epoch  1
01/12 03:10:14   decay_if_no_progress None
01/12 03:10:14   decoders             [{'max_len': 40, 'name': 'nl'}]
01/12 03:10:14   description          'default configuration\nnext line of description\nlast line\n'
01/12 03:10:14   dev_prefix           'test'
01/12 03:10:14   early_stopping       True
01/12 03:10:14   embedding_dropout    0.0
01/12 03:10:14   embedding_initializer None
01/12 03:10:14   embedding_size       256
01/12 03:10:14   embedding_weight_scale None
01/12 03:10:14   embeddings_on_cpu    True
01/12 03:10:14   encoders             [{'attention_type': 'global', 'max_len': 200, 'name': 'code'},
 {'attention_type': 'global', 'max_len': 200, 'name': 'sbt'}]
01/12 03:10:14   ensemble             False
01/12 03:10:14   eval_burn_in         0
01/12 03:10:14   feed_previous        0.0
01/12 03:10:14   final_state          'last'
01/12 03:10:14   freeze_variables     []
01/12 03:10:14   generate_first       True
01/12 03:10:14   gpu_id               1
01/12 03:10:14   highway_layers       0
01/12 03:10:14   initial_state_dropout 0.0
01/12 03:10:14   initializer          None
01/12 03:10:14   input_layer_dropout  0.0
01/12 03:10:14   input_layers         None
01/12 03:10:14   keep_best            5
01/12 03:10:14   keep_every_n_hours   0
01/12 03:10:14   label                'default'
01/12 03:10:14   layer_norm           False
01/12 03:10:14   layers               1
01/12 03:10:14   learning_rate        0.5
01/12 03:10:14   learning_rate_decay_factor 0.95
01/12 03:10:14   len_normalization    1.0
01/12 03:10:14   log_file             'log.txt'
01/12 03:10:14   loss_function        'xent'
01/12 03:10:14   max_dev_size         0
01/12 03:10:14   max_epochs           100
01/12 03:10:14   max_gradient_norm    5.0
01/12 03:10:14   max_len              50
01/12 03:10:14   max_steps            600000
01/12 03:10:14   max_test_size        0
01/12 03:10:14   max_to_keep          1
01/12 03:10:14   max_train_size       0
01/12 03:10:14   maxout_stride        None
01/12 03:10:14   mem_fraction         1.0
01/12 03:10:14   min_learning_rate    1e-06
01/12 03:10:14   model_dir            'models/sperate/hybrid_sbt'
01/12 03:10:14   moving_average       None
01/12 03:10:14   no_gpu               False
01/12 03:10:14   optimizer            'sgd'
01/12 03:10:14   orthogonal_init      False
01/12 03:10:14   output               None
01/12 03:10:14   output_dropout       0.0
01/12 03:10:14   parallel_iterations  16
01/12 03:10:14   pervasive_dropout    False
01/12 03:10:14   pooling_avg          True
01/12 03:10:14   post_process_script  None
01/12 03:10:14   pred_deep_layer      False
01/12 03:10:14   pred_edits           False
01/12 03:10:14   pred_embed_proj      True
01/12 03:10:14   pred_maxout_layer    True
01/12 03:10:14   purge                False
01/12 03:10:14   raw_output           False
01/12 03:10:14   read_ahead           1
01/12 03:10:14   reconstruction_attn_weight 0.05
01/12 03:10:14   reconstruction_decoders False
01/12 03:10:14   reconstruction_weight 1.0
01/12 03:10:14   reinforce_after_n_epoch None
01/12 03:10:14   remove_unk           False
01/12 03:10:14   reverse              False
01/12 03:10:14   reverse_input        True
01/12 03:10:14   reward_function      'sentence_bleu'
01/12 03:10:14   rnn_feed_attn        True
01/12 03:10:14   rnn_input_dropout    0.0
01/12 03:10:14   rnn_output_dropout   0.0
01/12 03:10:14   rnn_state_dropout    0.0
01/12 03:10:14   save                 False
01/12 03:10:14   score_function       'corpus_bleu'
01/12 03:10:14   score_functions      ['bleu', 'loss']
01/12 03:10:14   script_dir           'scripts'
01/12 03:10:14   sgd_after_n_epoch    None
01/12 03:10:14   sgd_learning_rate    1.0
01/12 03:10:14   shuffle              True
01/12 03:10:14   softmax_temperature  1.0
01/12 03:10:14   steps_per_checkpoint 2000
01/12 03:10:14   steps_per_eval       2000
01/12 03:10:14   swap_memory          True
01/12 03:10:14   tie_embeddings       False
01/12 03:10:14   time_pooling         None
01/12 03:10:14   train                True
01/12 03:10:14   train_initial_states True
01/12 03:10:14   train_prefix         'train'
01/12 03:10:14   truncate_lines       True
01/12 03:10:14   update_first         False
01/12 03:10:14   use_baseline         False
01/12 03:10:14   use_dropout          False
01/12 03:10:14   use_lstm_full_state  False
01/12 03:10:14   use_previous_word    True
01/12 03:10:14   verbose              True
01/12 03:10:14   vocab_prefix         'vocab'
01/12 03:10:14   weight_scale         None
01/12 03:10:14   word_dropout         0.0
01/12 03:10:14 python random seed: 3635054294652649338
01/12 03:10:14 tf random seed:     7531736100022242492
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:203: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

01/12 03:10:14 creating model
01/12 03:10:14 using device: /gpu:1
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:230: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

01/12 03:10:14 copying vocab to models/sperate/hybrid_sbt/data/vocab.code
01/12 03:10:14 copying vocab to models/sperate/hybrid_sbt/data/vocab.sbt
01/12 03:10:14 copying vocab to models/sperate/hybrid_sbt/data/vocab.nl
01/12 03:10:14 reading vocabularies
01/12 03:10:14 creating model
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:60: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:111: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:33: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7efc486ffd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7efc486ffd68>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:226: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7efc486ffd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7efc486ffd30>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7efcc95fee48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7efcc95fee48>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7efcc95fe6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7efcc95fe6a0>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc948aa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc948aa20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:838: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc7253240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc7253240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc72067b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc72067b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:432: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:435: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc713aac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc713aac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc7117828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc7117828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc708b550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc708b550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc6f79a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc6f79a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc6cebe48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc6cebe48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc6cebd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc6cebd68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc6d41cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc6d41cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc6c27358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc6c27358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc6c27358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efcc6c27358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:919: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7efcc6b2df60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7efcc6b2df60>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:131: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:223: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7efc6c2dacf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7efc6c2dacf8>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc6c1f9f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc6c1f9f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc6c1a9ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc6c1a9ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc6c125cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc6c125cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc6c125cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc6c125cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc6c12ec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc6c12ec50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc6c069be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc6c069be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc6c069be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc6c069be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
01/12 03:10:21 model parameters (45)
01/12 03:10:21   baseline_step:0 ()
01/12 03:10:21   decoder_nl/attention_code/U_a/kernel:0 (512, 128)
01/12 03:10:21   decoder_nl/attention_code/W_a/bias:0 (128,)
01/12 03:10:21   decoder_nl/attention_code/W_a/kernel:0 (256, 128)
01/12 03:10:21   decoder_nl/attention_code/v_a:0 (128,)
01/12 03:10:21   decoder_nl/attention_sbt/U_a/kernel:0 (512, 128)
01/12 03:10:21   decoder_nl/attention_sbt/W_a/bias:0 (128,)
01/12 03:10:21   decoder_nl/attention_sbt/W_a/kernel:0 (256, 128)
01/12 03:10:21   decoder_nl/attention_sbt/v_a:0 (128,)
01/12 03:10:21   decoder_nl/code_sbt/initial_state_projection/bias:0 (256,)
01/12 03:10:21   decoder_nl/code_sbt/initial_state_projection/kernel:0 (512, 256)
01/12 03:10:21   decoder_nl/gru_cell/candidate/bias:0 (256,)
01/12 03:10:21   decoder_nl/gru_cell/candidate/kernel:0 (1024, 256)
01/12 03:10:21   decoder_nl/gru_cell/gates/bias:0 (512,)
01/12 03:10:21   decoder_nl/gru_cell/gates/kernel:0 (1024, 512)
01/12 03:10:21   decoder_nl/maxout/bias:0 (256,)
01/12 03:10:21   decoder_nl/maxout/kernel:0 (1024, 256)
01/12 03:10:21   decoder_nl/softmax0/kernel:0 (128, 256)
01/12 03:10:21   decoder_nl/softmax1/bias:0 (37188,)
01/12 03:10:21   decoder_nl/softmax1/kernel:0 (256, 37188)
01/12 03:10:21   embedding_code:0 (50000, 256)
01/12 03:10:21   embedding_nl:0 (37188, 256)
01/12 03:10:21   embedding_sbt:0 (87, 256)
01/12 03:10:21   encoder_code/initial_state_bw:0 (256,)
01/12 03:10:21   encoder_code/initial_state_fw:0 (256,)
01/12 03:10:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
01/12 03:10:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
01/12 03:10:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
01/12 03:10:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
01/12 03:10:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
01/12 03:10:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
01/12 03:10:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
01/12 03:10:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
01/12 03:10:21   encoder_sbt/initial_state_bw:0 (256,)
01/12 03:10:21   encoder_sbt/initial_state_fw:0 (256,)
01/12 03:10:21   encoder_sbt/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
01/12 03:10:21   encoder_sbt/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
01/12 03:10:21   encoder_sbt/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
01/12 03:10:21   encoder_sbt/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
01/12 03:10:21   encoder_sbt/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
01/12 03:10:21   encoder_sbt/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
01/12 03:10:21   encoder_sbt/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
01/12 03:10:21   encoder_sbt/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
01/12 03:10:21   global_step:0 ()
01/12 03:10:21   learning_rate:0 ()
01/12 03:10:21 number of parameters: 34.89M
WARNING:tensorflow:From /root/icpc/icpc/translate/translation_model.py:666: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

01/12 03:10:22 global step: 0
01/12 03:10:22 baseline step: 0
01/12 03:10:22 reading training data
01/12 03:10:23 total line count: 157832
01/12 03:10:33   lines read: 100000
01/12 03:10:40 files: data/speratedata/train.code data/speratedata/train.sbt data/speratedata/train.nl
01/12 03:10:40 lines reads: 157832
01/12 03:10:40 reading development data
01/12 03:10:41 files: data/speratedata/test.code data/speratedata/test.sbt data/speratedata/test.nl
01/12 03:10:41 lines reads: 16302
01/12 03:10:42 starting training
01/12 03:43:10 step 2000 epoch 1 learning rate 0.5 step-time 0.972 loss 81.637
01/12 03:43:10 starting evaluation
01/12 03:47:29 test bleu=0.71 loss=67.44 penalty=0.805 ratio=0.821
01/12 03:47:29 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 03:47:29 finished saving model
01/12 03:47:29 new best model
01/12 03:54:58   decaying learning rate to: 0.475
01/12 04:19:41 step 4000 epoch 2 learning rate 0.475 step-time 0.963 loss 60.120
01/12 04:19:41 starting evaluation
01/12 04:23:59 test bleu=1.47 loss=63.77 penalty=1.000 ratio=1.099
01/12 04:23:59 saving model to models/sperate/hybrid_sbt/checkpoints
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
01/12 04:23:59 finished saving model
01/12 04:23:59 new best model
01/12 04:39:05   decaying learning rate to: 0.451
01/12 04:56:05 step 6000 epoch 3 learning rate 0.451 step-time 0.960 loss 53.481
01/12 04:56:05 starting evaluation
01/12 05:00:23 test bleu=2.01 loss=62.16 penalty=1.000 ratio=1.407
01/12 05:00:23 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 05:00:23 finished saving model
01/12 05:00:23 new best model
01/12 05:23:07   decaying learning rate to: 0.429
01/12 05:32:44 step 8000 epoch 4 learning rate 0.429 step-time 0.968 loss 48.865
01/12 05:32:44 starting evaluation
01/12 05:36:31 test bleu=2.64 loss=59.96 penalty=0.639 ratio=0.691
01/12 05:36:31 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 05:36:31 finished saving model
01/12 05:36:31 new best model
01/12 06:06:46   decaying learning rate to: 0.407
01/12 06:08:54 step 10000 epoch 5 learning rate 0.407 step-time 0.969 loss 45.279
01/12 06:08:54 starting evaluation
01/12 06:13:12 test bleu=3.67 loss=59.60 penalty=1.000 ratio=1.081
01/12 06:13:12 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 06:13:12 finished saving model
01/12 06:13:12 new best model
01/12 06:45:24 step 12000 epoch 5 learning rate 0.407 step-time 0.964 loss 41.826
01/12 06:45:24 starting evaluation
01/12 06:49:42 test bleu=4.31 loss=58.34 penalty=0.938 ratio=0.940
01/12 06:49:42 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 06:49:42 finished saving model
01/12 06:49:42 new best model
01/12 06:55:05   decaying learning rate to: 0.387
01/12 07:21:48 step 14000 epoch 6 learning rate 0.387 step-time 0.961 loss 38.872
01/12 07:21:48 starting evaluation
01/12 07:26:05 test bleu=3.98 loss=58.71 penalty=1.000 ratio=1.061
01/12 07:26:05 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 07:26:05 finished saving model
01/12 07:39:02   decaying learning rate to: 0.368
01/12 07:58:20 step 16000 epoch 7 learning rate 0.368 step-time 0.965 loss 36.502
01/12 07:58:20 starting evaluation
01/12 08:02:30 test bleu=5.23 loss=59.53 penalty=0.856 ratio=0.866
01/12 08:02:30 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 08:02:30 finished saving model
01/12 08:02:30 new best model
01/12 08:23:02   decaying learning rate to: 0.349
01/12 08:34:52 step 18000 epoch 8 learning rate 0.349 step-time 0.968 loss 34.385
01/12 08:34:52 starting evaluation
01/12 08:38:52 test bleu=5.32 loss=60.26 penalty=0.773 ratio=0.795
01/12 08:38:52 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 08:38:52 finished saving model
01/12 08:38:52 new best model
01/12 09:06:48   decaying learning rate to: 0.332
01/12 09:11:02 step 20000 epoch 9 learning rate 0.332 step-time 0.963 loss 32.258
01/12 09:11:02 starting evaluation
01/12 09:15:03 test bleu=6.13 loss=61.62 penalty=0.803 ratio=0.820
01/12 09:15:03 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 09:15:04 finished saving model
01/12 09:15:04 new best model
01/12 09:47:13 step 22000 epoch 9 learning rate 0.332 step-time 0.963 loss 30.215
01/12 09:47:13 starting evaluation
01/12 09:51:10 test bleu=5.71 loss=61.11 penalty=0.752 ratio=0.778
01/12 09:51:10 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 09:51:10 finished saving model
01/12 09:54:26   decaying learning rate to: 0.315
01/12 10:23:32 step 24000 epoch 10 learning rate 0.315 step-time 0.969 loss 28.012
01/12 10:23:32 starting evaluation
01/12 10:27:31 test bleu=5.78 loss=62.95 penalty=0.739 ratio=0.768
01/12 10:27:31 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 10:27:31 finished saving model
01/12 10:38:21   decaying learning rate to: 0.299
01/12 10:59:52 step 26000 epoch 11 learning rate 0.299 step-time 0.968 loss 26.258
01/12 10:59:52 starting evaluation
01/12 11:04:09 test bleu=7.04 loss=63.48 penalty=0.952 ratio=0.953
01/12 11:04:09 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 11:04:09 finished saving model
01/12 11:04:09 new best model
01/12 11:22:21   decaying learning rate to: 0.284
01/12 11:36:21 step 28000 epoch 12 learning rate 0.284 step-time 0.964 loss 24.725
01/12 11:36:21 starting evaluation
01/12 11:40:28 test bleu=7.31 loss=66.26 penalty=0.868 ratio=0.876
01/12 11:40:28 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 11:40:29 finished saving model
01/12 11:40:29 new best model
01/12 12:06:20   decaying learning rate to: 0.27
01/12 12:12:47 step 30000 epoch 13 learning rate 0.27 step-time 0.967 loss 23.275
01/12 12:12:47 starting evaluation
01/12 12:16:51 test bleu=7.26 loss=67.29 penalty=0.864 ratio=0.872
01/12 12:16:51 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 12:16:51 finished saving model
01/12 12:49:10 step 32000 epoch 13 learning rate 0.27 step-time 0.967 loss 21.906
01/12 12:49:10 starting evaluation
01/12 12:53:16 test bleu=7.44 loss=67.73 penalty=0.849 ratio=0.860
01/12 12:53:16 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 12:53:16 finished saving model
01/12 12:53:16 new best model
01/12 12:54:24   decaying learning rate to: 0.257
01/12 13:25:42 step 34000 epoch 14 learning rate 0.257 step-time 0.971 loss 19.750
01/12 13:25:42 starting evaluation
01/12 13:29:51 test bleu=7.11 loss=69.37 penalty=0.921 ratio=0.924
01/12 13:29:51 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 13:29:52 finished saving model
01/12 13:38:27   decaying learning rate to: 0.244
01/12 14:02:06 step 36000 epoch 15 learning rate 0.244 step-time 0.965 loss 18.610
01/12 14:02:06 starting evaluation
01/12 14:06:21 test bleu=8.03 loss=71.33 penalty=0.967 ratio=0.967
01/12 14:06:21 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 14:06:21 finished saving model
01/12 14:06:21 new best model
01/12 14:22:38   decaying learning rate to: 0.232
01/12 14:38:34 step 38000 epoch 16 learning rate 0.232 step-time 0.964 loss 17.443
01/12 14:38:34 starting evaluation
01/12 14:42:54 test bleu=8.01 loss=75.29 penalty=1.000 ratio=1.033
01/12 14:42:54 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 14:42:55 finished saving model
01/12 15:06:45   decaying learning rate to: 0.22
01/12 15:15:08 step 40000 epoch 17 learning rate 0.22 step-time 0.964 loss 16.321
01/12 15:15:08 starting evaluation
01/12 15:19:11 test bleu=8.18 loss=78.29 penalty=0.870 ratio=0.878
01/12 15:19:11 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 15:19:12 finished saving model
01/12 15:19:12 new best model
01/12 15:50:40   decaying learning rate to: 0.209
01/12 15:51:39 step 42000 epoch 18 learning rate 0.209 step-time 0.971 loss 15.345
01/12 15:51:39 starting evaluation
01/12 15:55:52 test bleu=8.27 loss=78.68 penalty=0.940 ratio=0.941
01/12 15:55:52 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 15:55:52 finished saving model
01/12 15:55:52 new best model
01/12 16:28:10 step 44000 epoch 18 learning rate 0.209 step-time 0.966 loss 13.738
01/12 16:28:10 starting evaluation
01/12 16:32:19 test bleu=8.08 loss=78.94 penalty=0.881 ratio=0.887
01/12 16:32:19 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 16:32:19 finished saving model
01/12 16:38:51   decaying learning rate to: 0.199
01/12 17:04:26 step 46000 epoch 19 learning rate 0.199 step-time 0.961 loss 12.690
01/12 17:04:26 starting evaluation
01/12 17:08:40 test bleu=8.42 loss=83.91 penalty=0.966 ratio=0.967
01/12 17:08:40 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 17:08:41 finished saving model
01/12 17:08:41 new best model
01/12 17:22:56   decaying learning rate to: 0.189
01/12 17:41:05 step 48000 epoch 20 learning rate 0.189 step-time 0.970 loss 11.887
01/12 17:41:05 starting evaluation
01/12 17:45:14 test bleu=8.39 loss=87.36 penalty=0.920 ratio=0.923
01/12 17:45:14 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 17:45:15 finished saving model
01/12 18:06:55   decaying learning rate to: 0.179
01/12 18:17:37 step 50000 epoch 21 learning rate 0.179 step-time 0.969 loss 11.099
01/12 18:17:37 starting evaluation
01/12 18:21:48 test bleu=8.57 loss=92.23 penalty=0.956 ratio=0.957
01/12 18:21:48 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 18:21:48 finished saving model
01/12 18:21:48 new best model
01/12 18:50:59   decaying learning rate to: 0.17
01/12 18:54:05 step 52000 epoch 22 learning rate 0.17 step-time 0.966 loss 10.396
01/12 18:54:05 starting evaluation
01/12 18:58:18 test bleu=8.65 loss=92.89 penalty=0.936 ratio=0.938
01/12 18:58:18 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 18:58:18 finished saving model
01/12 18:58:18 new best model
01/12 19:30:29 step 54000 epoch 22 learning rate 0.17 step-time 0.963 loss 9.347
01/12 19:30:29 starting evaluation
01/12 19:34:39 test bleu=8.60 loss=94.21 penalty=0.916 ratio=0.919
01/12 19:34:39 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 19:34:39 finished saving model
01/12 19:39:05   decaying learning rate to: 0.162
01/12 20:06:59 step 56000 epoch 23 learning rate 0.162 step-time 0.968 loss 8.441
01/12 20:06:59 starting evaluation
01/12 20:11:14 test bleu=8.56 loss=99.34 penalty=0.994 ratio=0.994
01/12 20:11:14 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 20:11:14 finished saving model
01/12 20:23:17   decaying learning rate to: 0.154
01/12 20:43:48 step 58000 epoch 24 learning rate 0.154 step-time 0.974 loss 7.866
01/12 20:43:48 starting evaluation
01/12 20:47:57 test bleu=8.44 loss=102.56 penalty=0.925 ratio=0.928
01/12 20:47:57 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 20:47:57 finished saving model
01/12 21:07:22   decaying learning rate to: 0.146
01/12 21:20:12 step 60000 epoch 25 learning rate 0.146 step-time 0.965 loss 7.353
01/12 21:20:12 starting evaluation
01/12 21:24:30 test bleu=8.19 loss=106.50 penalty=1.000 ratio=1.049
01/12 21:24:30 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 21:24:30 finished saving model
01/12 21:51:30   decaying learning rate to: 0.139
01/12 21:56:45 step 62000 epoch 26 learning rate 0.139 step-time 0.965 loss 6.837
01/12 21:56:45 starting evaluation
01/12 22:01:03 test bleu=8.30 loss=110.75 penalty=1.000 ratio=1.049
01/12 22:01:03 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 22:01:03 finished saving model
01/12 22:33:19 step 64000 epoch 26 learning rate 0.139 step-time 0.966 loss 6.257
01/12 22:33:19 starting evaluation
01/12 22:37:36 test bleu=8.77 loss=112.78 penalty=1.000 ratio=1.006
01/12 22:37:36 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 22:37:37 finished saving model
01/12 22:37:37 new best model
01/12 22:39:54   decaying learning rate to: 0.132
01/12 23:10:03 step 66000 epoch 27 learning rate 0.132 step-time 0.971 loss 5.478
01/12 23:10:03 starting evaluation
01/12 23:14:21 test bleu=8.20 loss=118.95 penalty=1.000 ratio=1.067
01/12 23:14:21 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 23:14:21 finished saving model
01/12 23:24:04   decaying learning rate to: 0.125
01/12 23:46:35 step 68000 epoch 28 learning rate 0.125 step-time 0.965 loss 5.132
01/12 23:46:35 starting evaluation
01/12 23:50:54 test bleu=8.05 loss=122.95 penalty=1.000 ratio=1.062
01/12 23:50:54 saving model to models/sperate/hybrid_sbt/checkpoints
01/12 23:50:54 finished saving model
01/13 00:08:22   decaying learning rate to: 0.119
01/13 00:23:17 step 70000 epoch 29 learning rate 0.119 step-time 0.969 loss 4.797
01/13 00:23:17 starting evaluation
01/13 00:27:31 test bleu=8.75 loss=127.80 penalty=1.000 ratio=1.014
01/13 00:27:31 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 00:27:31 finished saving model
01/13 00:52:33   decaying learning rate to: 0.113
01/13 00:59:44 step 72000 epoch 30 learning rate 0.113 step-time 0.964 loss 4.428
01/13 00:59:44 starting evaluation
01/13 01:04:00 test bleu=8.68 loss=129.81 penalty=1.000 ratio=1.045
01/13 01:04:00 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 01:04:00 finished saving model
01/13 01:36:27 step 74000 epoch 31 learning rate 0.113 step-time 0.971 loss 4.128
01/13 01:36:27 starting evaluation
01/13 01:40:44 test bleu=8.39 loss=130.38 penalty=1.000 ratio=1.057
01/13 01:40:44 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 01:40:44 finished saving model
01/13 01:40:54   decaying learning rate to: 0.107
01/13 02:13:00 step 76000 epoch 31 learning rate 0.107 step-time 0.966 loss 3.551
01/13 02:13:00 starting evaluation
01/13 02:17:18 test bleu=8.24 loss=137.07 penalty=1.000 ratio=1.043
01/13 02:17:18 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 02:17:18 finished saving model
01/13 02:25:03   decaying learning rate to: 0.102
01/13 02:49:31 step 78000 epoch 32 learning rate 0.102 step-time 0.964 loss 3.320
01/13 02:49:31 starting evaluation
01/13 02:53:49 test bleu=8.38 loss=140.96 penalty=1.000 ratio=1.060
01/13 02:53:49 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 02:53:49 finished saving model
01/13 03:09:07   decaying learning rate to: 0.0969
01/13 03:26:06 step 80000 epoch 33 learning rate 0.0969 step-time 0.967 loss 3.082
01/13 03:26:06 starting evaluation
01/13 03:30:22 test bleu=8.68 loss=145.08 penalty=1.000 ratio=1.023
01/13 03:30:22 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 03:30:23 finished saving model
01/13 03:53:16   decaying learning rate to: 0.092
01/13 04:02:51 step 82000 epoch 34 learning rate 0.092 step-time 0.972 loss 2.871
01/13 04:02:51 starting evaluation
01/13 04:07:06 test bleu=8.51 loss=148.52 penalty=1.000 ratio=1.030
01/13 04:07:06 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 04:07:06 finished saving model
01/13 04:37:23   decaying learning rate to: 0.0874
01/13 04:39:21 step 84000 epoch 35 learning rate 0.0874 step-time 0.965 loss 2.682
01/13 04:39:21 starting evaluation
01/13 04:43:37 test bleu=8.92 loss=152.53 penalty=1.000 ratio=1.039
01/13 04:43:37 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 04:43:37 finished saving model
01/13 04:43:37 new best model
01/13 05:15:46 step 86000 epoch 35 learning rate 0.0874 step-time 0.962 loss 2.360
01/13 05:15:46 starting evaluation
01/13 05:20:01 test bleu=8.83 loss=154.85 penalty=1.000 ratio=1.012
01/13 05:20:01 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 05:20:01 finished saving model
01/13 05:25:40   decaying learning rate to: 0.083
01/13 05:52:25 step 88000 epoch 36 learning rate 0.083 step-time 0.969 loss 2.169
01/13 05:52:25 starting evaluation
01/13 05:56:39 test bleu=8.66 loss=160.42 penalty=0.987 ratio=0.987
01/13 05:56:39 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 05:56:39 finished saving model
01/13 06:09:50   decaying learning rate to: 0.0789
01/13 06:29:06 step 90000 epoch 37 learning rate 0.0789 step-time 0.971 loss 2.015
01/13 06:29:06 starting evaluation
01/13 06:33:27 test bleu=8.29 loss=163.85 penalty=1.000 ratio=1.067
01/13 06:33:27 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 06:33:27 finished saving model
01/13 06:53:57   decaying learning rate to: 0.0749
01/13 07:05:40 step 92000 epoch 38 learning rate 0.0749 step-time 0.964 loss 1.881
01/13 07:05:40 starting evaluation
01/13 07:09:55 test bleu=8.90 loss=168.12 penalty=1.000 ratio=1.005
01/13 07:09:55 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 07:09:55 finished saving model
01/13 07:38:02   decaying learning rate to: 0.0712
01/13 07:42:10 step 94000 epoch 39 learning rate 0.0712 step-time 0.965 loss 1.746
01/13 07:42:10 starting evaluation
01/13 07:46:29 test bleu=7.92 loss=171.19 penalty=1.000 ratio=1.102
01/13 07:46:29 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 07:46:29 finished saving model
01/13 08:18:41 step 96000 epoch 39 learning rate 0.0712 step-time 0.964 loss 1.589
01/13 08:18:41 starting evaluation
01/13 08:22:56 test bleu=8.68 loss=173.21 penalty=1.000 ratio=1.050
01/13 08:22:57 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 08:22:57 finished saving model
01/13 08:26:24   decaying learning rate to: 0.0676
01/13 08:55:25 step 98000 epoch 40 learning rate 0.0676 step-time 0.972 loss 1.423
01/13 08:55:25 starting evaluation
01/13 08:59:40 test bleu=8.63 loss=178.58 penalty=1.000 ratio=1.061
01/13 08:59:40 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 08:59:40 finished saving model
01/13 09:10:35   decaying learning rate to: 0.0643
01/13 09:31:56 step 100000 epoch 41 learning rate 0.0643 step-time 0.966 loss 1.327
01/13 09:31:56 starting evaluation
01/13 09:36:16 test bleu=8.65 loss=180.85 penalty=1.000 ratio=1.046
01/13 09:36:16 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 09:36:16 finished saving model
01/13 09:54:44   decaying learning rate to: 0.061
01/13 10:08:36 step 102000 epoch 42 learning rate 0.061 step-time 0.968 loss 1.251
01/13 10:08:36 starting evaluation
01/13 10:12:54 test bleu=8.52 loss=186.43 penalty=1.000 ratio=1.054
01/13 10:12:54 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 10:12:55 finished saving model
01/13 10:39:03   decaying learning rate to: 0.058
01/13 10:45:10 step 104000 epoch 43 learning rate 0.058 step-time 0.965 loss 1.179
01/13 10:45:10 starting evaluation
01/13 10:49:28 test bleu=8.46 loss=189.53 penalty=1.000 ratio=1.053
01/13 10:49:28 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 10:49:28 finished saving model
01/13 11:21:53 step 106000 epoch 43 learning rate 0.058 step-time 0.970 loss 1.098
01/13 11:21:53 starting evaluation
01/13 11:26:13 test bleu=8.83 loss=189.32 penalty=1.000 ratio=1.046
01/13 11:26:13 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 11:26:13 finished saving model
01/13 11:27:31   decaying learning rate to: 0.0551
01/13 11:57:49 step 108000 epoch 44 learning rate 0.0551 step-time 0.946 loss 0.965
01/13 11:57:49 starting evaluation
01/13 12:01:53 test bleu=8.30 loss=192.02 penalty=1.000 ratio=1.090
01/13 12:01:53 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 12:01:53 finished saving model
01/13 12:10:20   decaying learning rate to: 0.0523
01/13 12:32:58 step 110000 epoch 45 learning rate 0.0523 step-time 0.930 loss 0.911
01/13 12:32:58 starting evaluation
01/13 12:37:11 test bleu=8.26 loss=195.96 penalty=1.000 ratio=1.076
01/13 12:37:11 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 12:37:11 finished saving model
01/13 12:53:21   decaying learning rate to: 0.0497
01/13 13:08:38 step 112000 epoch 46 learning rate 0.0497 step-time 0.941 loss 0.866
01/13 13:08:38 starting evaluation
01/13 13:12:50 test bleu=8.49 loss=199.45 penalty=1.000 ratio=1.052
01/13 13:12:50 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 13:12:50 finished saving model
01/13 13:36:16   decaying learning rate to: 0.0472
01/13 13:44:28 step 114000 epoch 47 learning rate 0.0472 step-time 0.947 loss 0.810
01/13 13:44:28 starting evaluation
01/13 13:48:38 test bleu=8.45 loss=201.48 penalty=1.000 ratio=1.075
01/13 13:48:38 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 13:48:38 finished saving model
01/13 14:19:20   decaying learning rate to: 0.0449
01/13 14:20:08 step 116000 epoch 48 learning rate 0.0449 step-time 0.943 loss 0.787
01/13 14:20:08 starting evaluation
01/13 14:24:22 test bleu=8.32 loss=204.27 penalty=1.000 ratio=1.072
01/13 14:24:22 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 14:24:23 finished saving model
01/13 14:55:09 step 118000 epoch 48 learning rate 0.0449 step-time 0.921 loss 0.696
01/13 14:55:09 starting evaluation
01/13 14:59:22 test bleu=8.24 loss=207.07 penalty=1.000 ratio=1.089
01/13 14:59:22 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 14:59:22 finished saving model
01/13 15:05:50   decaying learning rate to: 0.0426
01/13 15:30:15 step 120000 epoch 49 learning rate 0.0426 step-time 0.924 loss 0.654
01/13 15:30:15 starting evaluation
01/13 15:34:25 test bleu=8.28 loss=209.07 penalty=1.000 ratio=1.049
01/13 15:34:25 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 15:34:25 finished saving model
01/13 15:48:02   decaying learning rate to: 0.0405
01/13 16:05:12 step 122000 epoch 50 learning rate 0.0405 step-time 0.921 loss 0.626
01/13 16:05:12 starting evaluation
01/13 16:09:24 test bleu=8.65 loss=211.75 penalty=1.000 ratio=1.045
01/13 16:09:24 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 16:09:24 finished saving model
01/13 16:30:09   decaying learning rate to: 0.0385
01/13 16:40:11 step 124000 epoch 51 learning rate 0.0385 step-time 0.922 loss 0.600
01/13 16:40:11 starting evaluation
01/13 16:44:22 test bleu=8.29 loss=212.39 penalty=1.000 ratio=1.074
01/13 16:44:22 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 16:44:22 finished saving model
01/13 17:12:15   decaying learning rate to: 0.0365
01/13 17:15:04 step 126000 epoch 52 learning rate 0.0365 step-time 0.919 loss 0.576
01/13 17:15:04 starting evaluation
01/13 17:19:15 test bleu=8.23 loss=216.56 penalty=1.000 ratio=1.079
01/13 17:19:15 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 17:19:15 finished saving model
01/13 17:50:01 step 128000 epoch 52 learning rate 0.0365 step-time 0.921 loss 0.534
01/13 17:50:01 starting evaluation
01/13 17:54:15 test bleu=8.34 loss=217.88 penalty=1.000 ratio=1.072
01/13 17:54:15 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 17:54:15 finished saving model
01/13 17:58:36   decaying learning rate to: 0.0347
01/13 18:24:57 step 130000 epoch 53 learning rate 0.0347 step-time 0.919 loss 0.496
01/13 18:24:57 starting evaluation
01/13 18:29:08 test bleu=8.30 loss=218.36 penalty=1.000 ratio=1.057
01/13 18:29:08 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 18:29:09 finished saving model
01/13 18:40:38   decaying learning rate to: 0.033
01/13 19:00:04 step 132000 epoch 54 learning rate 0.033 step-time 0.926 loss 0.476
01/13 19:00:04 starting evaluation
01/13 19:04:15 test bleu=8.27 loss=219.47 penalty=1.000 ratio=1.073
01/13 19:04:15 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 19:04:15 finished saving model
01/13 19:22:57   decaying learning rate to: 0.0313
01/13 19:34:58 step 134000 epoch 55 learning rate 0.0313 step-time 0.920 loss 0.455
01/13 19:34:58 starting evaluation
01/13 19:39:08 test bleu=8.47 loss=220.73 penalty=1.000 ratio=1.050
01/13 19:39:08 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 19:39:08 finished saving model
01/13 20:05:05   decaying learning rate to: 0.0298
01/13 20:09:54 step 136000 epoch 56 learning rate 0.0298 step-time 0.921 loss 0.449
01/13 20:09:54 starting evaluation
01/13 20:14:05 test bleu=8.57 loss=222.21 penalty=1.000 ratio=1.058
01/13 20:14:05 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 20:14:05 finished saving model
01/13 20:44:50 step 138000 epoch 56 learning rate 0.0298 step-time 0.920 loss 0.425
01/13 20:44:50 starting evaluation
01/13 20:49:03 test bleu=8.33 loss=223.99 penalty=1.000 ratio=1.060
01/13 20:49:03 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 20:49:03 finished saving model
01/13 20:51:23   decaying learning rate to: 0.0283
01/13 21:19:57 step 140000 epoch 57 learning rate 0.0283 step-time 0.925 loss 0.390
01/13 21:19:57 starting evaluation
01/13 21:24:07 test bleu=8.48 loss=224.82 penalty=1.000 ratio=1.064
01/13 21:24:07 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 21:24:07 finished saving model
01/13 21:33:39   decaying learning rate to: 0.0269
01/13 21:54:53 step 142000 epoch 58 learning rate 0.0269 step-time 0.921 loss 0.375
01/13 21:54:53 starting evaluation
01/13 21:59:05 test bleu=8.46 loss=225.74 penalty=1.000 ratio=1.066
01/13 21:59:05 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 21:59:05 finished saving model
01/13 22:15:50   decaying learning rate to: 0.0255
01/13 22:29:55 step 144000 epoch 59 learning rate 0.0255 step-time 0.923 loss 0.363
01/13 22:29:55 starting evaluation
01/13 22:34:06 test bleu=8.59 loss=227.11 penalty=1.000 ratio=1.044
01/13 22:34:06 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 22:34:06 finished saving model
01/13 22:57:55   decaying learning rate to: 0.0242
01/13 23:04:45 step 146000 epoch 60 learning rate 0.0242 step-time 0.918 loss 0.352
01/13 23:04:45 starting evaluation
01/13 23:08:55 test bleu=8.67 loss=228.29 penalty=1.000 ratio=1.040
01/13 23:08:55 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 23:08:56 finished saving model
01/13 23:39:40 step 148000 epoch 61 learning rate 0.0242 step-time 0.920 loss 0.346
01/13 23:39:40 starting evaluation
01/13 23:43:50 test bleu=8.39 loss=228.27 penalty=1.000 ratio=1.052
01/13 23:43:50 saving model to models/sperate/hybrid_sbt/checkpoints
01/13 23:43:50 finished saving model
01/13 23:44:08   decaying learning rate to: 0.023
01/14 00:14:31 step 150000 epoch 61 learning rate 0.023 step-time 0.918 loss 0.308
01/14 00:14:31 starting evaluation
01/14 00:18:43 test bleu=8.45 loss=229.27 penalty=1.000 ratio=1.047
01/14 00:18:43 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 00:18:43 finished saving model
01/14 00:26:15   decaying learning rate to: 0.0219
01/14 00:49:28 step 152000 epoch 62 learning rate 0.0219 step-time 0.920 loss 0.305
01/14 00:49:28 starting evaluation
01/14 00:53:38 test bleu=8.58 loss=229.47 penalty=1.000 ratio=1.045
01/14 00:53:38 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 00:53:38 finished saving model
01/14 01:08:18   decaying learning rate to: 0.0208
01/14 01:24:20 step 154000 epoch 63 learning rate 0.0208 step-time 0.919 loss 0.299
01/14 01:24:20 starting evaluation
01/14 01:28:31 test bleu=8.60 loss=231.01 penalty=1.000 ratio=1.050
01/14 01:28:31 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 01:28:31 finished saving model
01/14 01:50:14   decaying learning rate to: 0.0197
01/14 01:59:10 step 156000 epoch 64 learning rate 0.0197 step-time 0.917 loss 0.289
01/14 01:59:10 starting evaluation
01/14 02:03:21 test bleu=8.43 loss=231.89 penalty=1.000 ratio=1.040
01/14 02:03:21 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 02:03:22 finished saving model
01/14 02:32:23   decaying learning rate to: 0.0188
01/14 02:34:06 step 158000 epoch 65 learning rate 0.0188 step-time 0.920 loss 0.286
01/14 02:34:06 starting evaluation
01/14 02:38:17 test bleu=8.42 loss=231.56 penalty=1.000 ratio=1.052
01/14 02:38:17 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 02:38:17 finished saving model
01/14 03:09:05 step 160000 epoch 65 learning rate 0.0188 step-time 0.922 loss 0.261
01/14 03:09:05 starting evaluation
01/14 03:13:15 test bleu=8.37 loss=233.04 penalty=1.000 ratio=1.048
01/14 03:13:15 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 03:13:15 finished saving model
01/14 03:18:41   decaying learning rate to: 0.0178
01/14 03:44:07 step 162000 epoch 66 learning rate 0.0178 step-time 0.924 loss 0.252
01/14 03:44:07 starting evaluation
01/14 03:48:17 test bleu=8.78 loss=233.39 penalty=1.000 ratio=1.038
01/14 03:48:17 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 03:48:17 finished saving model
01/14 04:00:53   decaying learning rate to: 0.0169
01/14 04:19:00 step 164000 epoch 67 learning rate 0.0169 step-time 0.920 loss 0.250
01/14 04:19:00 starting evaluation
01/14 04:23:12 test bleu=8.34 loss=234.12 penalty=1.000 ratio=1.068
01/14 04:23:12 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 04:23:12 finished saving model
01/14 04:42:57   decaying learning rate to: 0.0161
01/14 04:53:53 step 166000 epoch 68 learning rate 0.0161 step-time 0.918 loss 0.239
01/14 04:53:53 starting evaluation
01/14 04:58:05 test bleu=8.66 loss=233.94 penalty=1.000 ratio=1.041
01/14 04:58:05 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 04:58:05 finished saving model
01/14 05:25:08   decaying learning rate to: 0.0153
01/14 05:28:53 step 168000 epoch 69 learning rate 0.0153 step-time 0.922 loss 0.242
01/14 05:28:53 starting evaluation
01/14 05:33:04 test bleu=8.48 loss=234.37 penalty=1.000 ratio=1.068
01/14 05:33:04 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 05:33:05 finished saving model
01/14 06:03:54 step 170000 epoch 69 learning rate 0.0153 step-time 0.923 loss 0.226
01/14 06:03:54 starting evaluation
01/14 06:08:04 test bleu=8.53 loss=234.93 penalty=1.000 ratio=1.054
01/14 06:08:04 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 06:08:04 finished saving model
01/14 06:11:28   decaying learning rate to: 0.0145
01/14 06:38:45 step 172000 epoch 70 learning rate 0.0145 step-time 0.919 loss 0.216
01/14 06:38:45 starting evaluation
01/14 06:42:57 test bleu=8.55 loss=235.16 penalty=1.000 ratio=1.047
01/14 06:42:57 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 06:42:57 finished saving model
01/14 06:53:34   decaying learning rate to: 0.0138
01/14 07:13:46 step 174000 epoch 71 learning rate 0.0138 step-time 0.923 loss 0.210
01/14 07:13:46 starting evaluation
01/14 07:17:57 test bleu=8.45 loss=236.03 penalty=1.000 ratio=1.054
01/14 07:17:57 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 07:17:57 finished saving model
01/14 07:35:48   decaying learning rate to: 0.0131
01/14 07:48:45 step 176000 epoch 72 learning rate 0.0131 step-time 0.922 loss 0.208
01/14 07:48:45 starting evaluation
01/14 07:52:55 test bleu=8.68 loss=236.78 penalty=1.000 ratio=1.045
01/14 07:52:55 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 07:52:55 finished saving model
01/14 08:17:49   decaying learning rate to: 0.0124
01/14 08:23:37 step 178000 epoch 73 learning rate 0.0124 step-time 0.919 loss 0.206
01/14 08:23:37 starting evaluation
01/14 08:27:47 test bleu=8.51 loss=237.01 penalty=1.000 ratio=1.038
01/14 08:27:47 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 08:27:47 finished saving model
01/14 08:58:33 step 180000 epoch 73 learning rate 0.0124 step-time 0.921 loss 0.201
01/14 08:58:33 starting evaluation
01/14 09:02:43 test bleu=8.66 loss=236.70 penalty=1.000 ratio=1.048
01/14 09:02:43 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 09:02:43 finished saving model
01/14 09:04:07   decaying learning rate to: 0.0118
01/14 09:33:25 step 182000 epoch 74 learning rate 0.0118 step-time 0.919 loss 0.185
01/14 09:33:25 starting evaluation
01/14 09:37:35 test bleu=8.77 loss=237.69 penalty=1.000 ratio=1.022
01/14 09:37:35 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 09:37:35 finished saving model
01/14 09:46:13   decaying learning rate to: 0.0112
01/14 10:08:25 step 184000 epoch 75 learning rate 0.0112 step-time 0.923 loss 0.184
01/14 10:08:25 starting evaluation
01/14 10:12:37 test bleu=8.45 loss=238.81 penalty=1.000 ratio=1.055
01/14 10:12:37 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 10:12:37 finished saving model
01/14 10:28:24   decaying learning rate to: 0.0107
01/14 10:43:25 step 186000 epoch 76 learning rate 0.0107 step-time 0.922 loss 0.179
01/14 10:43:25 starting evaluation
01/14 10:47:35 test bleu=8.73 loss=239.36 penalty=1.000 ratio=1.039
01/14 10:47:35 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 10:47:35 finished saving model
01/14 11:10:32   decaying learning rate to: 0.0101
01/14 11:18:22 step 188000 epoch 77 learning rate 0.0101 step-time 0.922 loss 0.182
01/14 11:18:22 starting evaluation
01/14 11:22:34 test bleu=8.42 loss=239.86 penalty=1.000 ratio=1.055
01/14 11:22:34 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 11:22:34 finished saving model
01/14 11:52:31   decaying learning rate to: 0.00963
01/14 11:53:08 step 190000 epoch 78 learning rate 0.00963 step-time 0.915 loss 0.178
01/14 11:53:08 starting evaluation
01/14 11:57:14 test bleu=8.55 loss=239.87 penalty=1.000 ratio=1.054
01/14 11:57:14 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 11:57:14 finished saving model
01/14 12:28:39 step 192000 epoch 78 learning rate 0.00963 step-time 0.940 loss 0.164
01/14 12:28:39 starting evaluation
01/14 12:32:57 test bleu=8.57 loss=240.33 penalty=1.000 ratio=1.050
01/14 12:32:57 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 12:32:57 finished saving model
01/14 12:39:47   decaying learning rate to: 0.00915
01/14 13:04:55 step 194000 epoch 79 learning rate 0.00915 step-time 0.957 loss 0.160
01/14 13:04:55 starting evaluation
01/14 13:09:10 test bleu=8.42 loss=239.89 penalty=1.000 ratio=1.043
01/14 13:09:10 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 13:09:10 finished saving model
01/14 13:23:26   decaying learning rate to: 0.00869
01/14 13:41:08 step 196000 epoch 80 learning rate 0.00869 step-time 0.957 loss 0.160
01/14 13:41:08 starting evaluation
01/14 13:45:25 test bleu=8.69 loss=241.17 penalty=1.000 ratio=1.039
01/14 13:45:25 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 13:45:25 finished saving model
01/14 14:07:05   decaying learning rate to: 0.00826
01/14 14:17:19 step 198000 epoch 81 learning rate 0.00826 step-time 0.955 loss 0.159
01/14 14:17:19 starting evaluation
01/14 14:21:35 test bleu=8.60 loss=241.57 penalty=1.000 ratio=1.052
01/14 14:21:35 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 14:21:35 finished saving model
01/14 14:50:44   decaying learning rate to: 0.00784
01/14 14:53:31 step 200000 epoch 82 learning rate 0.00784 step-time 0.956 loss 0.160
01/14 14:53:31 starting evaluation
01/14 14:57:49 test bleu=8.30 loss=241.82 penalty=1.000 ratio=1.064
01/14 14:57:50 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 14:57:50 finished saving model
01/14 15:29:51 step 202000 epoch 82 learning rate 0.00784 step-time 0.959 loss 0.150
01/14 15:29:51 starting evaluation
01/14 15:34:07 test bleu=8.13 loss=241.49 penalty=1.000 ratio=1.079
01/14 15:34:07 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 15:34:07 finished saving model
01/14 15:38:46   decaying learning rate to: 0.00745
01/14 16:06:01 step 204000 epoch 83 learning rate 0.00745 step-time 0.955 loss 0.142
01/14 16:06:01 starting evaluation
01/14 16:10:18 test bleu=8.64 loss=241.89 penalty=1.000 ratio=1.041
01/14 16:10:18 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 16:10:18 finished saving model
01/14 16:22:22   decaying learning rate to: 0.00708
01/14 16:42:08 step 206000 epoch 84 learning rate 0.00708 step-time 0.953 loss 0.145
01/14 16:42:08 starting evaluation
01/14 16:46:25 test bleu=8.51 loss=242.34 penalty=1.000 ratio=1.055
01/14 16:46:25 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 16:46:25 finished saving model
01/14 17:06:05   decaying learning rate to: 0.00673
01/14 17:18:22 step 208000 epoch 85 learning rate 0.00673 step-time 0.956 loss 0.144
01/14 17:18:22 starting evaluation
01/14 17:22:38 test bleu=8.57 loss=242.97 penalty=1.000 ratio=1.052
01/14 17:22:38 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 17:22:38 finished saving model
01/14 17:49:44   decaying learning rate to: 0.00639
01/14 17:54:35 step 210000 epoch 86 learning rate 0.00639 step-time 0.956 loss 0.142
01/14 17:54:35 starting evaluation
01/14 17:58:50 test bleu=8.54 loss=243.43 penalty=1.000 ratio=1.058
01/14 17:58:50 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 17:58:50 finished saving model
01/14 18:32:42 step 212000 epoch 86 learning rate 0.00639 step-time 1.013 loss 0.139
01/14 18:32:42 starting evaluation
01/14 18:37:18 test bleu=8.54 loss=243.39 penalty=1.000 ratio=1.041
01/14 18:37:18 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 18:37:18 finished saving model
01/14 18:40:04   decaying learning rate to: 0.00607
01/14 19:11:06 step 214000 epoch 87 learning rate 0.00607 step-time 1.011 loss 0.133
01/14 19:11:06 starting evaluation
01/14 19:15:44 test bleu=8.45 loss=243.19 penalty=1.000 ratio=1.053
01/14 19:15:44 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 19:15:44 finished saving model
01/14 19:26:28   decaying learning rate to: 0.00577
01/14 19:49:38 step 216000 epoch 88 learning rate 0.00577 step-time 1.014 loss 0.131
01/14 19:49:38 starting evaluation
01/14 19:54:14 test bleu=8.48 loss=244.14 penalty=1.000 ratio=1.049
01/14 19:54:14 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 19:54:14 finished saving model
01/14 20:12:57   decaying learning rate to: 0.00548
01/14 20:27:40 step 218000 epoch 89 learning rate 0.00548 step-time 1.000 loss 0.130
01/14 20:27:40 starting evaluation
01/14 20:31:59 test bleu=8.62 loss=244.02 penalty=1.000 ratio=1.055
01/14 20:31:59 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 20:31:59 finished saving model
01/14 20:57:27   decaying learning rate to: 0.0052
01/14 21:04:36 step 220000 epoch 90 learning rate 0.0052 step-time 0.976 loss 0.132
01/14 21:04:36 starting evaluation
01/14 21:09:00 test bleu=8.36 loss=244.88 penalty=1.000 ratio=1.055
01/14 21:09:00 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 21:09:00 finished saving model
01/14 21:41:18 step 222000 epoch 91 learning rate 0.0052 step-time 0.966 loss 0.129
01/14 21:41:18 starting evaluation
01/14 21:45:42 test bleu=8.40 loss=244.97 penalty=1.000 ratio=1.065
01/14 21:45:42 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 21:45:42 finished saving model
01/14 21:46:10   decaying learning rate to: 0.00494
01/14 22:18:01 step 224000 epoch 91 learning rate 0.00494 step-time 0.967 loss 0.123
01/14 22:18:01 starting evaluation
01/14 22:22:24 test bleu=8.43 loss=244.70 penalty=1.000 ratio=1.055
01/14 22:22:24 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 22:22:24 finished saving model
01/14 22:30:28   decaying learning rate to: 0.0047
01/14 22:54:55 step 226000 epoch 92 learning rate 0.0047 step-time 0.973 loss 0.120
01/14 22:54:55 starting evaluation
01/14 22:59:18 test bleu=8.64 loss=245.01 penalty=1.000 ratio=1.050
01/14 22:59:18 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 22:59:19 finished saving model
01/14 23:14:58   decaying learning rate to: 0.00446
01/14 23:31:47 step 228000 epoch 93 learning rate 0.00446 step-time 0.972 loss 0.121
01/14 23:31:47 starting evaluation
01/14 23:36:14 test bleu=8.69 loss=245.61 penalty=1.000 ratio=1.042
01/14 23:36:14 saving model to models/sperate/hybrid_sbt/checkpoints
01/14 23:36:14 finished saving model
01/14 23:59:28   decaying learning rate to: 0.00424
01/15 00:08:43 step 230000 epoch 94 learning rate 0.00424 step-time 0.972 loss 0.120
01/15 00:08:43 starting evaluation
01/15 00:13:06 test bleu=8.45 loss=245.81 penalty=1.000 ratio=1.060
01/15 00:13:06 saving model to models/sperate/hybrid_sbt/checkpoints
01/15 00:13:06 finished saving model
01/15 00:43:58   decaying learning rate to: 0.00403
01/15 00:45:37 step 232000 epoch 95 learning rate 0.00403 step-time 0.973 loss 0.122
01/15 00:45:37 starting evaluation
01/15 00:50:03 test bleu=8.54 loss=245.95 penalty=1.000 ratio=1.052
01/15 00:50:03 saving model to models/sperate/hybrid_sbt/checkpoints
01/15 00:50:03 finished saving model
01/15 01:22:23 step 234000 epoch 95 learning rate 0.00403 step-time 0.968 loss 0.115
01/15 01:22:23 starting evaluation
01/15 01:26:47 test bleu=8.64 loss=245.95 penalty=1.000 ratio=1.049
01/15 01:26:47 saving model to models/sperate/hybrid_sbt/checkpoints
01/15 01:26:47 finished saving model
01/15 01:32:43   decaying learning rate to: 0.00383
01/15 01:59:15 step 236000 epoch 96 learning rate 0.00383 step-time 0.971 loss 0.113
01/15 01:59:15 starting evaluation
01/15 02:03:38 test bleu=8.58 loss=246.39 penalty=1.000 ratio=1.047
01/15 02:03:38 saving model to models/sperate/hybrid_sbt/checkpoints
01/15 02:03:38 finished saving model
01/15 02:17:05   decaying learning rate to: 0.00363
01/15 02:36:01 step 238000 epoch 97 learning rate 0.00363 step-time 0.969 loss 0.114
01/15 02:36:01 starting evaluation
01/15 02:40:24 test bleu=8.60 loss=246.54 penalty=1.000 ratio=1.039
01/15 02:40:24 saving model to models/sperate/hybrid_sbt/checkpoints
01/15 02:40:25 finished saving model
01/15 03:01:15   decaying learning rate to: 0.00345
01/15 03:12:41 step 240000 epoch 98 learning rate 0.00345 step-time 0.966 loss 0.114
01/15 03:12:41 starting evaluation
01/15 03:17:03 test bleu=8.67 loss=246.88 penalty=1.000 ratio=1.043
01/15 03:17:04 saving model to models/sperate/hybrid_sbt/checkpoints
01/15 03:17:04 finished saving model
01/15 03:45:34   decaying learning rate to: 0.00328
01/15 03:49:20 step 242000 epoch 99 learning rate 0.00328 step-time 0.966 loss 0.113
01/15 03:49:20 starting evaluation
01/15 03:53:44 test bleu=8.53 loss=247.46 penalty=1.000 ratio=1.056
01/15 03:53:44 saving model to models/sperate/hybrid_sbt/checkpoints
01/15 03:53:44 finished saving model
01/15 04:26:22 step 244000 epoch 99 learning rate 0.00328 step-time 0.976 loss 0.112
01/15 04:26:22 starting evaluation
01/15 04:30:45 test bleu=8.56 loss=246.90 penalty=1.000 ratio=1.053
01/15 04:30:45 saving model to models/sperate/hybrid_sbt/checkpoints
01/15 04:30:46 finished saving model
01/15 04:34:24   decaying learning rate to: 0.00312
01/15 05:03:14 step 246000 epoch 100 learning rate 0.00312 step-time 0.972 loss 0.106
01/15 05:03:14 starting evaluation
01/15 05:07:39 test bleu=8.60 loss=247.71 penalty=1.000 ratio=1.046
01/15 05:07:39 saving model to models/sperate/hybrid_sbt/checkpoints
01/15 05:07:39 finished saving model
01/15 05:17:35 finished training
01/15 05:17:35 exiting...
01/15 05:17:35 saving model to models/sperate/hybrid_sbt/checkpoints
01/15 05:17:35 finished saving model
