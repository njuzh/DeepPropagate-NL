nohup: ignoring input
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:107: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:30: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

01/12 03:05:47 label: default
01/12 03:05:47 description:
  default configuration
  next line of description
  last line
01/12 03:05:47 /root/icpc/icpc/translate/__main__.py config/sperate/codenn/config.yaml --train -v
01/12 03:05:47 commit hash 74e0554cb3eb5df835cef993ad570ff8de651f71
01/12 03:05:47 tensorflow version: 1.14.0
01/12 03:05:47 program arguments
01/12 03:05:47   aggregation_method   'sum'
01/12 03:05:47   align_encoder_id     0
01/12 03:05:47   allow_growth         True
01/12 03:05:47   attention_type       'global'
01/12 03:05:47   attn_filter_length   0
01/12 03:05:47   attn_filters         0
01/12 03:05:47   attn_prev_word       False
01/12 03:05:47   attn_size            128
01/12 03:05:47   attn_temperature     1.0
01/12 03:05:47   attn_window_size     0
01/12 03:05:47   average              False
01/12 03:05:47   baseline_activation  None
01/12 03:05:47   baseline_learning_rate 0.001
01/12 03:05:47   baseline_optimizer   'adam'
01/12 03:05:47   baseline_steps       0
01/12 03:05:47   batch_mode           'standard'
01/12 03:05:47   batch_size           64
01/12 03:05:47   beam_size            5
01/12 03:05:47   bidir                True
01/12 03:05:47   bidir_projection     False
01/12 03:05:47   binary               False
01/12 03:05:47   cell_size            256
01/12 03:05:47   cell_type            'GRU'
01/12 03:05:47   character_level      False
01/12 03:05:47   checkpoints          []
01/12 03:05:47   conditional_rnn      False
01/12 03:05:47   config               'config/sperate/codenn/config.yaml'
01/12 03:05:47   convolutions         None
01/12 03:05:47   data_dir             'data/speratedata'
01/12 03:05:47   debug                False
01/12 03:05:47   decay_after_n_epoch  1
01/12 03:05:47   decay_every_n_epoch  1
01/12 03:05:47   decay_if_no_progress None
01/12 03:05:47   decoders             [{'max_len': 40, 'name': 'nl'}]
01/12 03:05:47   description          'default configuration\nnext line of description\nlast line\n'
01/12 03:05:47   dev_prefix           'test'
01/12 03:05:47   early_stopping       True
01/12 03:05:47   embedding_dropout    0.0
01/12 03:05:47   embedding_initializer None
01/12 03:05:47   embedding_size       256
01/12 03:05:47   embedding_weight_scale None
01/12 03:05:47   embeddings_on_cpu    True
01/12 03:05:47   encoders             [{'attention_type': 'global', 'max_len': 200, 'name': 'code'}]
01/12 03:05:47   ensemble             False
01/12 03:05:47   eval_burn_in         0
01/12 03:05:47   feed_previous        0.0
01/12 03:05:47   final_state          'last'
01/12 03:05:47   freeze_variables     []
01/12 03:05:47   generate_first       True
01/12 03:05:47   gpu_id               3
01/12 03:05:47   highway_layers       0
01/12 03:05:47   initial_state_dropout 0.0
01/12 03:05:47   initializer          None
01/12 03:05:47   input_layer_dropout  0.0
01/12 03:05:47   input_layers         None
01/12 03:05:47   keep_best            5
01/12 03:05:47   keep_every_n_hours   0
01/12 03:05:47   label                'default'
01/12 03:05:47   layer_norm           False
01/12 03:05:47   layers               1
01/12 03:05:47   learning_rate        0.5
01/12 03:05:47   learning_rate_decay_factor 0.95
01/12 03:05:47   len_normalization    1.0
01/12 03:05:47   log_file             'log.txt'
01/12 03:05:47   loss_function        'xent'
01/12 03:05:47   max_dev_size         0
01/12 03:05:47   max_epochs           100
01/12 03:05:47   max_gradient_norm    5.0
01/12 03:05:47   max_len              50
01/12 03:05:47   max_steps            600000
01/12 03:05:47   max_test_size        0
01/12 03:05:47   max_to_keep          1
01/12 03:05:47   max_train_size       0
01/12 03:05:47   maxout_stride        None
01/12 03:05:47   mem_fraction         1.0
01/12 03:05:47   min_learning_rate    1e-06
01/12 03:05:47   model_dir            'models/sperate/codenn'
01/12 03:05:47   moving_average       None
01/12 03:05:47   no_gpu               False
01/12 03:05:47   optimizer            'sgd'
01/12 03:05:47   orthogonal_init      False
01/12 03:05:47   output               None
01/12 03:05:47   output_dropout       0.0
01/12 03:05:47   parallel_iterations  16
01/12 03:05:47   pervasive_dropout    False
01/12 03:05:47   pooling_avg          True
01/12 03:05:47   post_process_script  None
01/12 03:05:47   pred_deep_layer      False
01/12 03:05:47   pred_edits           False
01/12 03:05:47   pred_embed_proj      True
01/12 03:05:47   pred_maxout_layer    True
01/12 03:05:47   purge                False
01/12 03:05:47   raw_output           False
01/12 03:05:47   read_ahead           1
01/12 03:05:47   reconstruction_attn_weight 0.05
01/12 03:05:47   reconstruction_decoders False
01/12 03:05:47   reconstruction_weight 1.0
01/12 03:05:47   reinforce_after_n_epoch None
01/12 03:05:47   remove_unk           False
01/12 03:05:47   reverse              False
01/12 03:05:47   reverse_input        True
01/12 03:05:47   reward_function      'sentence_bleu'
01/12 03:05:47   rnn_feed_attn        True
01/12 03:05:47   rnn_input_dropout    0.0
01/12 03:05:47   rnn_output_dropout   0.0
01/12 03:05:47   rnn_state_dropout    0.0
01/12 03:05:47   save                 False
01/12 03:05:47   score_function       'corpus_bleu'
01/12 03:05:47   score_functions      ['bleu', 'loss']
01/12 03:05:47   script_dir           'scripts'
01/12 03:05:47   sgd_after_n_epoch    None
01/12 03:05:47   sgd_learning_rate    1.0
01/12 03:05:47   shuffle              True
01/12 03:05:47   softmax_temperature  1.0
01/12 03:05:47   steps_per_checkpoint 2000
01/12 03:05:47   steps_per_eval       2000
01/12 03:05:47   swap_memory          True
01/12 03:05:47   tie_embeddings       False
01/12 03:05:47   time_pooling         None
01/12 03:05:47   train                True
01/12 03:05:47   train_initial_states True
01/12 03:05:47   train_prefix         'train'
01/12 03:05:47   truncate_lines       True
01/12 03:05:47   update_first         False
01/12 03:05:47   use_baseline         False
01/12 03:05:47   use_dropout          False
01/12 03:05:47   use_lstm_full_state  False
01/12 03:05:47   use_previous_word    True
01/12 03:05:47   verbose              True
01/12 03:05:47   vocab_prefix         'vocab'
01/12 03:05:47   weight_scale         None
01/12 03:05:47   word_dropout         0.0
01/12 03:05:47 python random seed: 4371728930204548329
01/12 03:05:47 tf random seed:     1357571074927237241
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:203: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

01/12 03:05:47 creating model
01/12 03:05:47 using device: /gpu:3
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:230: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

01/12 03:05:47 copying vocab to models/sperate/codenn/data/vocab.code
01/12 03:05:47 copying vocab to models/sperate/codenn/data/vocab.nl
01/12 03:05:47 reading vocabularies
01/12 03:05:47 creating model
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:60: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:111: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:33: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f17cfd30588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f17cfd30588>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:226: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f17cfd30b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f17cfd30b70>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1855781780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1855781780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:838: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1854a9b8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1854a9b8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1854acaf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1854acaf60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:432: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:435: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f185498dda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f185498dda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1852945e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1852945e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f185291fef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f185291fef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f185570be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f185570be10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f185570be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f185570be10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:919: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f185264f2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f185264f2b0>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:131: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:223: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f17fb1c4be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f17fb1c4be0>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f17fb0e4da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f17fb0e4da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f17fb110a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f17fb110a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f17fb110940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f17fb110940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f17fb0c7240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f17fb0c7240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f17fb0c7240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f17fb0c7240>>: AssertionError: Bad argument number for Name: 3, expecting 4
01/12 03:05:51 model parameters (30)
01/12 03:05:51   baseline_step:0 ()
01/12 03:05:51   decoder_nl/attention_code/U_a/kernel:0 (512, 128)
01/12 03:05:51   decoder_nl/attention_code/W_a/bias:0 (128,)
01/12 03:05:51   decoder_nl/attention_code/W_a/kernel:0 (256, 128)
01/12 03:05:51   decoder_nl/attention_code/v_a:0 (128,)
01/12 03:05:51   decoder_nl/code/initial_state_projection/bias:0 (256,)
01/12 03:05:51   decoder_nl/code/initial_state_projection/kernel:0 (256, 256)
01/12 03:05:51   decoder_nl/gru_cell/candidate/bias:0 (256,)
01/12 03:05:51   decoder_nl/gru_cell/candidate/kernel:0 (1024, 256)
01/12 03:05:51   decoder_nl/gru_cell/gates/bias:0 (512,)
01/12 03:05:51   decoder_nl/gru_cell/gates/kernel:0 (1024, 512)
01/12 03:05:51   decoder_nl/maxout/bias:0 (256,)
01/12 03:05:51   decoder_nl/maxout/kernel:0 (1024, 256)
01/12 03:05:51   decoder_nl/softmax0/kernel:0 (128, 256)
01/12 03:05:51   decoder_nl/softmax1/bias:0 (37188,)
01/12 03:05:51   decoder_nl/softmax1/kernel:0 (256, 37188)
01/12 03:05:51   embedding_code:0 (50000, 256)
01/12 03:05:51   embedding_nl:0 (37188, 256)
01/12 03:05:51   encoder_code/initial_state_bw:0 (256,)
01/12 03:05:51   encoder_code/initial_state_fw:0 (256,)
01/12 03:05:51   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
01/12 03:05:51   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
01/12 03:05:51   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
01/12 03:05:51   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
01/12 03:05:51   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
01/12 03:05:51   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
01/12 03:05:51   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
01/12 03:05:51   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
01/12 03:05:51   global_step:0 ()
01/12 03:05:51   learning_rate:0 ()
01/12 03:05:51 number of parameters: 33.91M
WARNING:tensorflow:From /root/icpc/icpc/translate/translation_model.py:666: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

01/12 03:05:52 global step: 0
01/12 03:05:52 baseline step: 0
01/12 03:05:52 reading training data
01/12 03:05:52 total line count: 157832
01/12 03:05:56   lines read: 100000
01/12 03:05:58 files: data/speratedata/train.code data/speratedata/train.nl
01/12 03:05:58 lines reads: 157832
01/12 03:05:58 reading development data
01/12 03:05:59 files: data/speratedata/test.code data/speratedata/test.nl
01/12 03:05:59 lines reads: 16302
01/12 03:05:59 starting training
01/12 03:30:06 step 2000 epoch 1 learning rate 0.5 step-time 0.721 loss 77.624
01/12 03:30:06 starting evaluation
01/12 03:33:55 test bleu=1.00 loss=67.57 penalty=0.550 ratio=0.626
01/12 03:33:55 saving model to models/sperate/codenn/checkpoints
01/12 03:33:55 finished saving model
01/12 03:33:55 new best model
01/12 03:39:14   decaying learning rate to: 0.475
01/12 03:57:43 step 4000 epoch 2 learning rate 0.475 step-time 0.712 loss 58.917
01/12 03:57:43 starting evaluation
01/12 04:01:38 test bleu=1.59 loss=62.73 penalty=0.904 ratio=0.908
01/12 04:01:38 saving model to models/sperate/codenn/checkpoints
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
01/12 04:01:38 finished saving model
01/12 04:01:38 new best model
01/12 04:12:34   decaying learning rate to: 0.451
01/12 04:25:27 step 6000 epoch 3 learning rate 0.451 step-time 0.712 loss 52.136
01/12 04:25:27 starting evaluation
01/12 04:28:56 test bleu=2.33 loss=60.57 penalty=0.773 ratio=0.795
01/12 04:28:56 saving model to models/sperate/codenn/checkpoints
01/12 04:28:56 finished saving model
01/12 04:28:56 new best model
01/12 04:45:29   decaying learning rate to: 0.429
01/12 04:52:43 step 8000 epoch 4 learning rate 0.429 step-time 0.711 loss 47.670
01/12 04:52:43 starting evaluation
01/12 04:56:31 test bleu=3.54 loss=59.37 penalty=0.789 ratio=0.808
01/12 04:56:31 saving model to models/sperate/codenn/checkpoints
01/12 04:56:31 finished saving model
01/12 04:56:31 new best model
01/12 05:18:43   decaying learning rate to: 0.407
01/12 05:20:18 step 10000 epoch 5 learning rate 0.407 step-time 0.711 loss 44.041
01/12 05:20:18 starting evaluation
01/12 05:24:15 test bleu=3.28 loss=59.23 penalty=1.000 ratio=1.223
01/12 05:24:15 saving model to models/sperate/codenn/checkpoints
01/12 05:24:15 finished saving model
01/12 05:48:08 step 12000 epoch 5 learning rate 0.407 step-time 0.714 loss 40.579
01/12 05:48:08 starting evaluation
01/12 05:52:01 test bleu=3.71 loss=58.69 penalty=0.806 ratio=0.823
01/12 05:52:01 saving model to models/sperate/codenn/checkpoints
01/12 05:52:01 finished saving model
01/12 05:52:01 new best model
01/12 05:56:05   decaying learning rate to: 0.387
01/12 06:15:51 step 14000 epoch 6 learning rate 0.387 step-time 0.713 loss 37.519
01/12 06:15:51 starting evaluation
01/12 06:19:28 test bleu=4.48 loss=59.30 penalty=0.826 ratio=0.840
01/12 06:19:28 saving model to models/sperate/codenn/checkpoints
01/12 06:19:28 finished saving model
01/12 06:19:28 new best model
01/12 06:29:08   decaying learning rate to: 0.368
01/12 06:43:43 step 16000 epoch 7 learning rate 0.368 step-time 0.725 loss 35.151
01/12 06:43:43 starting evaluation
01/12 06:47:15 test bleu=5.05 loss=60.28 penalty=0.805 ratio=0.822
01/12 06:47:15 saving model to models/sperate/codenn/checkpoints
01/12 06:47:15 finished saving model
01/12 06:47:15 new best model
01/12 07:02:35   decaying learning rate to: 0.349
01/12 07:11:24 step 18000 epoch 8 learning rate 0.349 step-time 0.723 loss 32.937
01/12 07:11:24 starting evaluation
01/12 07:14:55 test bleu=5.64 loss=61.29 penalty=0.737 ratio=0.767
01/12 07:14:55 saving model to models/sperate/codenn/checkpoints
01/12 07:14:55 finished saving model
01/12 07:14:55 new best model
01/12 07:35:26   decaying learning rate to: 0.332
01/12 07:38:37 step 20000 epoch 9 learning rate 0.332 step-time 0.709 loss 30.925
01/12 07:38:37 starting evaluation
01/12 07:42:15 test bleu=6.26 loss=61.42 penalty=0.828 ratio=0.842
01/12 07:42:15 saving model to models/sperate/codenn/checkpoints
01/12 07:42:15 finished saving model
01/12 07:42:15 new best model
01/12 08:05:58 step 22000 epoch 9 learning rate 0.332 step-time 0.709 loss 28.769
01/12 08:05:58 starting evaluation
01/12 08:09:37 test bleu=6.54 loss=62.03 penalty=0.809 ratio=0.825
01/12 08:09:37 saving model to models/sperate/codenn/checkpoints
01/12 08:09:37 finished saving model
01/12 08:09:37 new best model
01/12 08:12:04   decaying learning rate to: 0.315
01/12 08:33:13 step 24000 epoch 10 learning rate 0.315 step-time 0.706 loss 26.385
01/12 08:33:13 starting evaluation
01/12 08:37:07 test bleu=6.95 loss=63.34 penalty=0.993 ratio=0.993
01/12 08:37:07 saving model to models/sperate/codenn/checkpoints
01/12 08:37:07 finished saving model
01/12 08:37:07 new best model
01/12 08:45:15   decaying learning rate to: 0.299
01/12 09:00:52 step 26000 epoch 11 learning rate 0.299 step-time 0.710 loss 24.756
01/12 09:00:52 starting evaluation
01/12 09:04:38 test bleu=6.95 loss=65.68 penalty=0.904 ratio=0.909
01/12 09:04:38 saving model to models/sperate/codenn/checkpoints
01/12 09:04:38 finished saving model
01/12 09:04:38 new best model
01/12 09:18:23   decaying learning rate to: 0.284
01/12 09:28:21 step 28000 epoch 12 learning rate 0.284 step-time 0.710 loss 23.290
01/12 09:28:21 starting evaluation
01/12 09:32:01 test bleu=7.21 loss=68.07 penalty=0.845 ratio=0.856
01/12 09:32:01 saving model to models/sperate/codenn/checkpoints
01/12 09:32:01 finished saving model
01/12 09:32:01 new best model
01/12 09:51:27   decaying learning rate to: 0.27
01/12 09:55:55 step 30000 epoch 13 learning rate 0.27 step-time 0.715 loss 21.807
01/12 09:55:55 starting evaluation
01/12 09:59:56 test bleu=7.39 loss=70.01 penalty=0.977 ratio=0.977
01/12 09:59:56 saving model to models/sperate/codenn/checkpoints
01/12 09:59:56 finished saving model
01/12 09:59:56 new best model
01/12 10:24:09 step 32000 epoch 13 learning rate 0.27 step-time 0.724 loss 20.354
01/12 10:24:09 starting evaluation
01/12 10:27:45 test bleu=7.51 loss=69.07 penalty=0.984 ratio=0.984
01/12 10:27:45 saving model to models/sperate/codenn/checkpoints
01/12 10:27:45 finished saving model
01/12 10:27:45 new best model
01/12 10:28:34   decaying learning rate to: 0.257
01/12 10:51:49 step 34000 epoch 14 learning rate 0.257 step-time 0.720 loss 18.344
01/12 10:51:49 starting evaluation
01/12 10:55:29 test bleu=7.28 loss=72.32 penalty=0.828 ratio=0.841
01/12 10:55:29 saving model to models/sperate/codenn/checkpoints
01/12 10:55:29 finished saving model
01/12 11:01:31   decaying learning rate to: 0.244
01/12 11:19:14 step 36000 epoch 15 learning rate 0.244 step-time 0.710 loss 17.099
01/12 11:19:14 starting evaluation
01/12 11:22:55 test bleu=7.61 loss=74.87 penalty=0.859 ratio=0.868
01/12 11:22:55 saving model to models/sperate/codenn/checkpoints
01/12 11:22:55 finished saving model
01/12 11:22:55 new best model
01/12 11:34:41   decaying learning rate to: 0.232
01/12 11:46:44 step 38000 epoch 16 learning rate 0.232 step-time 0.712 loss 15.946
01/12 11:46:44 starting evaluation
01/12 11:50:36 test bleu=7.80 loss=79.91 penalty=0.988 ratio=0.988
01/12 11:50:36 saving model to models/sperate/codenn/checkpoints
01/12 11:50:36 finished saving model
01/12 11:50:36 new best model
01/12 12:07:52   decaying learning rate to: 0.22
01/12 12:14:15 step 40000 epoch 17 learning rate 0.22 step-time 0.707 loss 15.014
01/12 12:14:15 starting evaluation
01/12 12:18:03 test bleu=7.74 loss=81.99 penalty=0.937 ratio=0.939
01/12 12:18:03 saving model to models/sperate/codenn/checkpoints
01/12 12:18:04 finished saving model
01/12 12:41:09   decaying learning rate to: 0.209
01/12 12:41:53 step 42000 epoch 18 learning rate 0.209 step-time 0.713 loss 14.048
01/12 12:41:53 starting evaluation
01/12 12:45:47 test bleu=7.98 loss=82.67 penalty=1.000 ratio=1.021
01/12 12:45:47 saving model to models/sperate/codenn/checkpoints
01/12 12:45:47 finished saving model
01/12 12:45:47 new best model
01/12 13:09:31 step 44000 epoch 18 learning rate 0.209 step-time 0.709 loss 12.403
01/12 13:09:31 starting evaluation
01/12 13:13:22 test bleu=8.15 loss=84.24 penalty=0.956 ratio=0.957
01/12 13:13:22 saving model to models/sperate/codenn/checkpoints
01/12 13:13:22 finished saving model
01/12 13:13:22 new best model
01/12 13:18:20   decaying learning rate to: 0.199
01/12 13:37:14 step 46000 epoch 19 learning rate 0.199 step-time 0.714 loss 11.465
01/12 13:37:14 starting evaluation
01/12 13:41:13 test bleu=8.06 loss=89.03 penalty=1.000 ratio=1.046
01/12 13:41:13 saving model to models/sperate/codenn/checkpoints
01/12 13:41:13 finished saving model
01/12 13:51:48   decaying learning rate to: 0.189
01/12 14:05:22 step 48000 epoch 20 learning rate 0.189 step-time 0.723 loss 10.684
01/12 14:05:22 starting evaluation
01/12 14:09:01 test bleu=8.17 loss=91.63 penalty=1.000 ratio=1.020
01/12 14:09:01 saving model to models/sperate/codenn/checkpoints
01/12 14:09:01 finished saving model
01/12 14:09:01 new best model
01/12 14:25:12   decaying learning rate to: 0.179
01/12 14:33:08 step 50000 epoch 21 learning rate 0.179 step-time 0.721 loss 9.992
01/12 14:33:08 starting evaluation
01/12 14:36:58 test bleu=8.20 loss=95.21 penalty=0.964 ratio=0.965
01/12 14:36:58 saving model to models/sperate/codenn/checkpoints
01/12 14:36:58 finished saving model
01/12 14:36:58 new best model
01/12 14:58:22   decaying learning rate to: 0.17
01/12 15:00:41 step 52000 epoch 22 learning rate 0.17 step-time 0.709 loss 9.245
01/12 15:00:41 starting evaluation
01/12 15:04:33 test bleu=8.19 loss=99.33 penalty=1.000 ratio=1.009
01/12 15:04:33 saving model to models/sperate/codenn/checkpoints
01/12 15:04:33 finished saving model
01/12 15:28:16 step 54000 epoch 22 learning rate 0.17 step-time 0.709 loss 8.318
01/12 15:28:16 starting evaluation
01/12 15:32:11 test bleu=7.63 loss=100.27 penalty=1.000 ratio=1.098
01/12 15:32:11 saving model to models/sperate/codenn/checkpoints
01/12 15:32:11 finished saving model
01/12 15:35:29   decaying learning rate to: 0.162
01/12 15:55:58 step 56000 epoch 23 learning rate 0.162 step-time 0.711 loss 7.455
01/12 15:55:58 starting evaluation
01/12 15:59:45 test bleu=8.26 loss=105.18 penalty=0.935 ratio=0.937
01/12 15:59:45 saving model to models/sperate/codenn/checkpoints
01/12 15:59:45 finished saving model
01/12 15:59:45 new best model
01/12 16:08:43   decaying learning rate to: 0.154
01/12 16:23:26 step 58000 epoch 24 learning rate 0.154 step-time 0.709 loss 6.986
01/12 16:23:26 starting evaluation
01/12 16:27:21 test bleu=7.81 loss=109.70 penalty=1.000 ratio=1.075
01/12 16:27:21 saving model to models/sperate/codenn/checkpoints
01/12 16:27:21 finished saving model
01/12 16:41:57   decaying learning rate to: 0.146
01/12 16:51:06 step 60000 epoch 25 learning rate 0.146 step-time 0.711 loss 6.470
01/12 16:51:06 starting evaluation
01/12 16:54:55 test bleu=8.56 loss=114.41 penalty=0.989 ratio=0.989
01/12 16:54:55 saving model to models/sperate/codenn/checkpoints
01/12 16:54:55 finished saving model
01/12 16:54:55 new best model
01/12 17:15:12   decaying learning rate to: 0.139
01/12 17:18:51 step 62000 epoch 26 learning rate 0.139 step-time 0.716 loss 6.031
01/12 17:18:51 starting evaluation
01/12 17:22:54 test bleu=8.21 loss=118.89 penalty=1.000 ratio=1.060
01/12 17:22:54 saving model to models/sperate/codenn/checkpoints
01/12 17:22:54 finished saving model
01/12 17:47:07 step 64000 epoch 26 learning rate 0.139 step-time 0.724 loss 5.482
01/12 17:47:07 starting evaluation
01/12 17:50:49 test bleu=8.23 loss=119.01 penalty=1.000 ratio=1.070
01/12 17:50:49 saving model to models/sperate/codenn/checkpoints
01/12 17:50:49 finished saving model
01/12 17:52:27   decaying learning rate to: 0.132
01/12 18:14:59 step 66000 epoch 27 learning rate 0.132 step-time 0.723 loss 4.824
01/12 18:14:59 starting evaluation
01/12 18:18:52 test bleu=8.30 loss=124.59 penalty=1.000 ratio=1.060
01/12 18:18:52 saving model to models/sperate/codenn/checkpoints
01/12 18:18:52 finished saving model
01/12 18:25:46   decaying learning rate to: 0.125
01/12 18:42:35 step 68000 epoch 28 learning rate 0.125 step-time 0.709 loss 4.500
01/12 18:42:35 starting evaluation
01/12 18:46:30 test bleu=7.85 loss=130.03 penalty=1.000 ratio=1.079
01/12 18:46:30 saving model to models/sperate/codenn/checkpoints
01/12 18:46:30 finished saving model
01/12 18:59:03   decaying learning rate to: 0.119
01/12 19:10:14 step 70000 epoch 29 learning rate 0.119 step-time 0.710 loss 4.186
01/12 19:10:14 starting evaluation
01/12 19:14:08 test bleu=8.41 loss=133.31 penalty=1.000 ratio=1.027
01/12 19:14:08 saving model to models/sperate/codenn/checkpoints
01/12 19:14:08 finished saving model
01/12 19:32:20   decaying learning rate to: 0.113
01/12 19:37:51 step 72000 epoch 30 learning rate 0.113 step-time 0.710 loss 3.884
01/12 19:37:51 starting evaluation
01/12 19:41:46 test bleu=7.91 loss=137.06 penalty=1.000 ratio=1.076
01/12 19:41:46 saving model to models/sperate/codenn/checkpoints
01/12 19:41:47 finished saving model
01/12 20:05:38 step 74000 epoch 31 learning rate 0.113 step-time 0.713 loss 3.604
01/12 20:05:38 starting evaluation
01/12 20:09:26 test bleu=8.30 loss=138.48 penalty=1.000 ratio=1.039
01/12 20:09:26 saving model to models/sperate/codenn/checkpoints
01/12 20:09:26 finished saving model
01/12 20:09:33   decaying learning rate to: 0.107
01/12 20:33:18 step 76000 epoch 31 learning rate 0.107 step-time 0.714 loss 3.095
01/12 20:33:18 starting evaluation
01/12 20:37:12 test bleu=8.41 loss=145.28 penalty=1.000 ratio=1.037
01/12 20:37:12 saving model to models/sperate/codenn/checkpoints
01/12 20:37:12 finished saving model
01/12 20:42:57   decaying learning rate to: 0.102
01/12 21:01:03 step 78000 epoch 32 learning rate 0.102 step-time 0.713 loss 2.900
01/12 21:01:03 starting evaluation
01/12 21:05:05 test bleu=7.83 loss=149.41 penalty=1.000 ratio=1.110
01/12 21:05:05 saving model to models/sperate/codenn/checkpoints
01/12 21:05:05 finished saving model
01/12 21:16:25   decaying learning rate to: 0.0969
01/12 21:29:10 step 80000 epoch 33 learning rate 0.0969 step-time 0.720 loss 2.699
01/12 21:29:10 starting evaluation
01/12 21:32:52 test bleu=8.27 loss=154.35 penalty=1.000 ratio=1.067
01/12 21:32:52 saving model to models/sperate/codenn/checkpoints
01/12 21:32:52 finished saving model
01/12 21:49:48   decaying learning rate to: 0.092
01/12 21:56:58 step 82000 epoch 34 learning rate 0.092 step-time 0.721 loss 2.502
01/12 21:56:58 starting evaluation
01/12 22:00:51 test bleu=8.14 loss=157.66 penalty=1.000 ratio=1.070
01/12 22:00:51 saving model to models/sperate/codenn/checkpoints
01/12 22:00:51 finished saving model
01/12 22:23:06   decaying learning rate to: 0.0874
01/12 22:24:34 step 84000 epoch 35 learning rate 0.0874 step-time 0.709 loss 2.342
01/12 22:24:34 starting evaluation
01/12 22:28:29 test bleu=8.18 loss=159.80 penalty=1.000 ratio=1.088
01/12 22:28:29 saving model to models/sperate/codenn/checkpoints
01/12 22:28:29 finished saving model
01/12 22:52:12 step 86000 epoch 35 learning rate 0.0874 step-time 0.710 loss 2.058
01/12 22:52:12 starting evaluation
01/12 22:56:06 test bleu=8.15 loss=162.55 penalty=1.000 ratio=1.057
01/12 22:56:06 saving model to models/sperate/codenn/checkpoints
01/12 22:56:06 finished saving model
01/12 23:00:17   decaying learning rate to: 0.083
01/12 23:19:51 step 88000 epoch 36 learning rate 0.083 step-time 0.710 loss 1.880
01/12 23:19:51 starting evaluation
01/12 23:23:44 test bleu=8.16 loss=168.43 penalty=1.000 ratio=1.085
01/12 23:23:44 saving model to models/sperate/codenn/checkpoints
01/12 23:23:44 finished saving model
01/12 23:33:36   decaying learning rate to: 0.0789
01/12 23:47:36 step 90000 epoch 37 learning rate 0.0789 step-time 0.714 loss 1.769
01/12 23:47:36 starting evaluation
01/12 23:51:29 test bleu=8.27 loss=171.99 penalty=1.000 ratio=1.066
01/12 23:51:29 saving model to models/sperate/codenn/checkpoints
01/12 23:51:29 finished saving model
01/13 00:06:59   decaying learning rate to: 0.0749
01/13 00:15:11 step 92000 epoch 38 learning rate 0.0749 step-time 0.709 loss 1.652
01/13 00:15:11 starting evaluation
01/13 00:19:05 test bleu=8.35 loss=177.64 penalty=1.000 ratio=1.053
01/13 00:19:05 saving model to models/sperate/codenn/checkpoints
01/13 00:19:05 finished saving model
01/13 00:40:08   decaying learning rate to: 0.0712
01/13 00:42:53 step 94000 epoch 39 learning rate 0.0712 step-time 0.712 loss 1.547
01/13 00:42:53 starting evaluation
01/13 00:46:55 test bleu=8.00 loss=179.30 penalty=1.000 ratio=1.094
01/13 00:46:55 saving model to models/sperate/codenn/checkpoints
01/13 00:46:55 finished saving model
01/13 01:11:02 step 96000 epoch 39 learning rate 0.0712 step-time 0.722 loss 1.407
01/13 01:11:02 starting evaluation
01/13 01:14:45 test bleu=8.25 loss=182.10 penalty=1.000 ratio=1.052
01/13 01:14:45 saving model to models/sperate/codenn/checkpoints
01/13 01:14:45 finished saving model
01/13 01:17:17   decaying learning rate to: 0.0676
01/13 01:38:55 step 98000 epoch 40 learning rate 0.0676 step-time 0.723 loss 1.256
01/13 01:38:55 starting evaluation
01/13 01:42:49 test bleu=8.21 loss=185.89 penalty=1.000 ratio=1.071
01/13 01:42:49 saving model to models/sperate/codenn/checkpoints
01/13 01:42:49 finished saving model
01/13 01:50:32   decaying learning rate to: 0.0643
01/13 02:06:34 step 100000 epoch 41 learning rate 0.0643 step-time 0.710 loss 1.180
01/13 02:06:34 starting evaluation
01/13 02:10:29 test bleu=8.42 loss=188.30 penalty=1.000 ratio=1.066
01/13 02:10:29 saving model to models/sperate/codenn/checkpoints
01/13 02:10:30 finished saving model
01/13 02:23:57   decaying learning rate to: 0.061
01/13 02:34:16 step 102000 epoch 42 learning rate 0.061 step-time 0.711 loss 1.117
01/13 02:34:16 starting evaluation
01/13 02:38:08 test bleu=8.40 loss=193.48 penalty=1.000 ratio=1.074
01/13 02:38:08 saving model to models/sperate/codenn/checkpoints
01/13 02:38:09 finished saving model
01/13 02:57:15   decaying learning rate to: 0.058
01/13 03:01:56 step 104000 epoch 43 learning rate 0.058 step-time 0.712 loss 1.052
01/13 03:01:56 starting evaluation
01/13 03:05:51 test bleu=8.28 loss=194.83 penalty=1.000 ratio=1.063
01/13 03:05:51 saving model to models/sperate/codenn/checkpoints
01/13 03:05:51 finished saving model
01/13 03:29:36 step 106000 epoch 43 learning rate 0.058 step-time 0.710 loss 0.979
01/13 03:29:36 starting evaluation
01/13 03:33:31 test bleu=8.35 loss=198.74 penalty=1.000 ratio=1.071
01/13 03:33:31 saving model to models/sperate/codenn/checkpoints
01/13 03:33:31 finished saving model
01/13 03:34:30   decaying learning rate to: 0.0551
01/13 03:57:28 step 108000 epoch 44 learning rate 0.0551 step-time 0.716 loss 0.868
01/13 03:57:28 starting evaluation
01/13 04:01:23 test bleu=8.58 loss=201.32 penalty=1.000 ratio=1.060
01/13 04:01:23 saving model to models/sperate/codenn/checkpoints
01/13 04:01:23 finished saving model
01/13 04:01:23 new best model
01/13 04:07:59   decaying learning rate to: 0.0523
01/13 04:25:14 step 110000 epoch 45 learning rate 0.0523 step-time 0.713 loss 0.825
01/13 04:25:14 starting evaluation
01/13 04:29:12 test bleu=8.24 loss=203.87 penalty=1.000 ratio=1.058
01/13 04:29:12 saving model to models/sperate/codenn/checkpoints
01/13 04:29:12 finished saving model
01/13 04:41:28   decaying learning rate to: 0.0497
01/13 04:53:19 step 112000 epoch 46 learning rate 0.0497 step-time 0.721 loss 0.789
01/13 04:53:19 starting evaluation
01/13 04:57:06 test bleu=8.16 loss=206.83 penalty=1.000 ratio=1.082
01/13 04:57:06 saving model to models/sperate/codenn/checkpoints
01/13 04:57:06 finished saving model
01/13 05:15:00   decaying learning rate to: 0.0472
01/13 05:21:14 step 114000 epoch 47 learning rate 0.0472 step-time 0.722 loss 0.755
01/13 05:21:14 starting evaluation
01/13 05:25:07 test bleu=8.17 loss=207.60 penalty=1.000 ratio=1.096
01/13 05:25:07 saving model to models/sperate/codenn/checkpoints
01/13 05:25:07 finished saving model
01/13 05:48:21   decaying learning rate to: 0.0449
01/13 05:48:58 step 116000 epoch 48 learning rate 0.0449 step-time 0.713 loss 0.711
01/13 05:48:58 starting evaluation
01/13 05:52:49 test bleu=8.50 loss=211.97 penalty=1.000 ratio=1.052
01/13 05:52:49 saving model to models/sperate/codenn/checkpoints
01/13 05:52:49 finished saving model
01/13 06:16:35 step 118000 epoch 48 learning rate 0.0449 step-time 0.711 loss 0.640
01/13 06:16:35 starting evaluation
01/13 06:20:30 test bleu=8.18 loss=212.99 penalty=1.000 ratio=1.072
01/13 06:20:30 saving model to models/sperate/codenn/checkpoints
01/13 06:20:30 finished saving model
01/13 06:25:32   decaying learning rate to: 0.0426
01/13 06:44:12 step 120000 epoch 49 learning rate 0.0426 step-time 0.709 loss 0.607
01/13 06:44:12 starting evaluation
01/13 06:48:08 test bleu=8.22 loss=215.85 penalty=1.000 ratio=1.062
01/13 06:48:08 saving model to models/sperate/codenn/checkpoints
01/13 06:48:08 finished saving model
01/13 06:58:48   decaying learning rate to: 0.0405
01/13 07:11:55 step 122000 epoch 50 learning rate 0.0405 step-time 0.711 loss 0.585
01/13 07:11:55 starting evaluation
01/13 07:15:49 test bleu=8.07 loss=218.08 penalty=1.000 ratio=1.092
01/13 07:15:49 saving model to models/sperate/codenn/checkpoints
01/13 07:15:49 finished saving model
01/13 07:32:07   decaying learning rate to: 0.0385
01/13 07:39:39 step 124000 epoch 51 learning rate 0.0385 step-time 0.713 loss 0.559
01/13 07:39:39 starting evaluation
01/13 07:43:32 test bleu=8.44 loss=219.04 penalty=1.000 ratio=1.048
01/13 07:43:32 saving model to models/sperate/codenn/checkpoints
01/13 07:43:32 finished saving model
01/13 08:05:16   decaying learning rate to: 0.0365
01/13 08:07:25 step 126000 epoch 52 learning rate 0.0365 step-time 0.714 loss 0.543
01/13 08:07:25 starting evaluation
01/13 08:11:22 test bleu=8.00 loss=221.02 penalty=1.000 ratio=1.103
01/13 08:11:22 saving model to models/sperate/codenn/checkpoints
01/13 08:11:22 finished saving model
01/13 08:35:26 step 128000 epoch 52 learning rate 0.0365 step-time 0.720 loss 0.503
01/13 08:35:26 starting evaluation
01/13 08:39:17 test bleu=8.60 loss=221.67 penalty=1.000 ratio=1.049
01/13 08:39:17 saving model to models/sperate/codenn/checkpoints
01/13 08:39:17 finished saving model
01/13 08:39:17 new best model
01/13 08:42:43   decaying learning rate to: 0.0347
01/13 09:03:31 step 130000 epoch 53 learning rate 0.0347 step-time 0.725 loss 0.459
01/13 09:03:31 starting evaluation
01/13 09:07:16 test bleu=8.13 loss=221.95 penalty=1.000 ratio=1.088
01/13 09:07:16 saving model to models/sperate/codenn/checkpoints
01/13 09:07:17 finished saving model
01/13 09:16:06   decaying learning rate to: 0.033
01/13 09:31:14 step 132000 epoch 54 learning rate 0.033 step-time 0.717 loss 0.448
01/13 09:31:14 starting evaluation
01/13 09:35:07 test bleu=8.11 loss=224.69 penalty=1.000 ratio=1.095
01/13 09:35:07 saving model to models/sperate/codenn/checkpoints
01/13 09:35:07 finished saving model
01/13 09:49:29   decaying learning rate to: 0.0313
01/13 09:58:56 step 134000 epoch 55 learning rate 0.0313 step-time 0.712 loss 0.439
01/13 09:58:56 starting evaluation
01/13 10:02:53 test bleu=7.98 loss=227.56 penalty=1.000 ratio=1.114
01/13 10:02:53 saving model to models/sperate/codenn/checkpoints
01/13 10:02:53 finished saving model
01/13 10:22:50   decaying learning rate to: 0.0298
01/13 10:26:39 step 136000 epoch 56 learning rate 0.0298 step-time 0.711 loss 0.419
01/13 10:26:39 starting evaluation
01/13 10:30:34 test bleu=8.32 loss=227.35 penalty=1.000 ratio=1.073
01/13 10:30:34 saving model to models/sperate/codenn/checkpoints
01/13 10:30:34 finished saving model
01/13 10:54:17 step 138000 epoch 56 learning rate 0.0298 step-time 0.709 loss 0.403
01/13 10:54:17 starting evaluation
01/13 10:58:12 test bleu=7.76 loss=226.75 penalty=1.000 ratio=1.148
01/13 10:58:12 saving model to models/sperate/codenn/checkpoints
01/13 10:58:12 finished saving model
01/13 11:00:03   decaying learning rate to: 0.0283
01/13 11:22:04 step 140000 epoch 57 learning rate 0.0283 step-time 0.714 loss 0.368
01/13 11:22:04 starting evaluation
01/13 11:25:57 test bleu=8.37 loss=228.06 penalty=1.000 ratio=1.077
01/13 11:25:57 saving model to models/sperate/codenn/checkpoints
01/13 11:25:57 finished saving model
01/13 11:33:22   decaying learning rate to: 0.0269
01/13 11:49:40 step 142000 epoch 58 learning rate 0.0269 step-time 0.709 loss 0.356
01/13 11:49:40 starting evaluation
01/13 11:53:37 test bleu=8.11 loss=228.76 penalty=1.000 ratio=1.112
01/13 11:53:37 saving model to models/sperate/codenn/checkpoints
01/13 11:53:38 finished saving model
01/13 12:06:55   decaying learning rate to: 0.0255
01/13 12:17:39 step 144000 epoch 59 learning rate 0.0255 step-time 0.719 loss 0.345
01/13 12:17:39 starting evaluation
01/13 12:21:43 test bleu=8.36 loss=229.79 penalty=1.000 ratio=1.073
01/13 12:21:43 saving model to models/sperate/codenn/checkpoints
01/13 12:21:44 finished saving model
01/13 12:40:41   decaying learning rate to: 0.0242
01/13 12:46:10 step 146000 epoch 60 learning rate 0.0242 step-time 0.731 loss 0.338
01/13 12:46:10 starting evaluation
01/13 12:49:57 test bleu=8.15 loss=230.39 penalty=1.000 ratio=1.085
01/13 12:49:57 saving model to models/sperate/codenn/checkpoints
01/13 12:49:57 finished saving model
01/13 13:14:19 step 148000 epoch 61 learning rate 0.0242 step-time 0.729 loss 0.331
01/13 13:14:19 starting evaluation
01/13 13:18:11 test bleu=8.44 loss=230.78 penalty=1.000 ratio=1.059
01/13 13:18:11 saving model to models/sperate/codenn/checkpoints
01/13 13:18:11 finished saving model
01/13 13:18:23   decaying learning rate to: 0.023
01/13 13:42:18 step 150000 epoch 61 learning rate 0.023 step-time 0.721 loss 0.301
01/13 13:42:18 starting evaluation
01/13 13:46:14 test bleu=8.31 loss=231.76 penalty=1.000 ratio=1.062
01/13 13:46:14 saving model to models/sperate/codenn/checkpoints
01/13 13:46:15 finished saving model
01/13 13:51:44   decaying learning rate to: 0.0219
01/13 14:10:03 step 152000 epoch 62 learning rate 0.0219 step-time 0.712 loss 0.291
01/13 14:10:03 starting evaluation
01/13 14:13:56 test bleu=8.27 loss=232.29 penalty=1.000 ratio=1.082
01/13 14:13:56 saving model to models/sperate/codenn/checkpoints
01/13 14:13:56 finished saving model
01/13 14:24:53   decaying learning rate to: 0.0208
01/13 14:37:37 step 154000 epoch 63 learning rate 0.0208 step-time 0.709 loss 0.285
01/13 14:37:37 starting evaluation
01/13 14:41:30 test bleu=8.37 loss=232.99 penalty=1.000 ratio=1.081
01/13 14:41:30 saving model to models/sperate/codenn/checkpoints
01/13 14:41:31 finished saving model
01/13 14:58:13   decaying learning rate to: 0.0197
01/13 15:05:15 step 156000 epoch 64 learning rate 0.0197 step-time 0.710 loss 0.276
01/13 15:05:15 starting evaluation
01/13 15:09:10 test bleu=8.11 loss=233.23 penalty=1.000 ratio=1.083
01/13 15:09:10 saving model to models/sperate/codenn/checkpoints
01/13 15:09:11 finished saving model
01/13 15:31:33   decaying learning rate to: 0.0188
01/13 15:32:55 step 158000 epoch 65 learning rate 0.0188 step-time 0.710 loss 0.280
01/13 15:32:55 starting evaluation
01/13 15:36:50 test bleu=8.58 loss=234.47 penalty=1.000 ratio=1.059
01/13 15:36:50 saving model to models/sperate/codenn/checkpoints
01/13 15:36:50 finished saving model
01/13 16:00:39 step 160000 epoch 65 learning rate 0.0188 step-time 0.713 loss 0.255
01/13 16:00:39 starting evaluation
01/13 16:04:34 test bleu=8.30 loss=234.77 penalty=1.000 ratio=1.076
01/13 16:04:34 saving model to models/sperate/codenn/checkpoints
01/13 16:04:34 finished saving model
01/13 16:08:52   decaying learning rate to: 0.0178
01/13 16:28:20 step 162000 epoch 66 learning rate 0.0178 step-time 0.711 loss 0.241
01/13 16:28:20 starting evaluation
01/13 16:32:15 test bleu=8.44 loss=236.15 penalty=1.000 ratio=1.062
01/13 16:32:15 saving model to models/sperate/codenn/checkpoints
01/13 16:32:15 finished saving model
01/13 16:42:13   decaying learning rate to: 0.0169
01/13 16:55:59 step 164000 epoch 67 learning rate 0.0169 step-time 0.710 loss 0.242
01/13 16:55:59 starting evaluation
01/13 16:59:55 test bleu=8.49 loss=236.17 penalty=1.000 ratio=1.062
01/13 16:59:55 saving model to models/sperate/codenn/checkpoints
01/13 16:59:55 finished saving model
01/13 17:15:37   decaying learning rate to: 0.0161
01/13 17:23:49 step 166000 epoch 68 learning rate 0.0161 step-time 0.715 loss 0.237
01/13 17:23:49 starting evaluation
01/13 17:27:49 test bleu=8.39 loss=237.16 penalty=1.000 ratio=1.057
01/13 17:27:49 saving model to models/sperate/codenn/checkpoints
01/13 17:27:49 finished saving model
01/13 17:49:08   decaying learning rate to: 0.0153
01/13 17:51:57 step 168000 epoch 69 learning rate 0.0153 step-time 0.722 loss 0.229
01/13 17:51:57 starting evaluation
01/13 17:55:47 test bleu=8.41 loss=237.38 penalty=1.000 ratio=1.064
01/13 17:55:47 saving model to models/sperate/codenn/checkpoints
01/13 17:55:48 finished saving model
01/13 18:19:59 step 170000 epoch 69 learning rate 0.0153 step-time 0.724 loss 0.224
01/13 18:19:59 starting evaluation
01/13 18:23:36 test bleu=8.17 loss=237.07 penalty=1.000 ratio=1.075
01/13 18:23:36 saving model to models/sperate/codenn/checkpoints
01/13 18:23:37 finished saving model
01/13 18:26:08   decaying learning rate to: 0.0145
01/13 18:47:38 step 172000 epoch 70 learning rate 0.0145 step-time 0.719 loss 0.208
01/13 18:47:38 starting evaluation
01/13 18:51:30 test bleu=8.41 loss=236.83 penalty=1.000 ratio=1.080
01/13 18:51:30 saving model to models/sperate/codenn/checkpoints
01/13 18:51:30 finished saving model
01/13 18:59:19   decaying learning rate to: 0.0138
01/13 19:15:17 step 174000 epoch 71 learning rate 0.0138 step-time 0.712 loss 0.205
01/13 19:15:17 starting evaluation
01/13 19:19:10 test bleu=8.31 loss=239.08 penalty=1.000 ratio=1.086
01/13 19:19:10 saving model to models/sperate/codenn/checkpoints
01/13 19:19:11 finished saving model
01/13 19:32:43   decaying learning rate to: 0.0131
01/13 19:42:59 step 176000 epoch 72 learning rate 0.0131 step-time 0.712 loss 0.203
01/13 19:42:59 starting evaluation
01/13 19:46:53 test bleu=8.42 loss=238.55 penalty=1.000 ratio=1.067
01/13 19:46:53 saving model to models/sperate/codenn/checkpoints
01/13 19:46:53 finished saving model
01/13 20:06:07   decaying learning rate to: 0.0124
01/13 20:10:43 step 178000 epoch 73 learning rate 0.0124 step-time 0.713 loss 0.200
01/13 20:10:43 starting evaluation
01/13 20:14:38 test bleu=8.45 loss=239.41 penalty=1.000 ratio=1.059
01/13 20:14:38 saving model to models/sperate/codenn/checkpoints
01/13 20:14:38 finished saving model
01/13 20:38:23 step 180000 epoch 73 learning rate 0.0124 step-time 0.711 loss 0.197
01/13 20:38:23 starting evaluation
01/13 20:42:19 test bleu=8.53 loss=239.46 penalty=1.000 ratio=1.051
01/13 20:42:19 saving model to models/sperate/codenn/checkpoints
01/13 20:42:19 finished saving model
01/13 20:43:25   decaying learning rate to: 0.0118
01/13 21:06:07 step 182000 epoch 74 learning rate 0.0118 step-time 0.712 loss 0.179
01/13 21:06:07 starting evaluation
01/13 21:10:02 test bleu=8.40 loss=238.63 penalty=1.000 ratio=1.053
01/13 21:10:02 saving model to models/sperate/codenn/checkpoints
01/13 21:10:02 finished saving model
01/13 21:16:49   decaying learning rate to: 0.0112
01/13 21:33:42 step 184000 epoch 75 learning rate 0.0112 step-time 0.708 loss 0.181
01/13 21:33:42 starting evaluation
01/13 21:37:37 test bleu=8.27 loss=241.20 penalty=1.000 ratio=1.082
01/13 21:37:37 saving model to models/sperate/codenn/checkpoints
01/13 21:37:37 finished saving model
01/13 21:50:06   decaying learning rate to: 0.0107
01/13 22:01:22 step 186000 epoch 76 learning rate 0.0107 step-time 0.710 loss 0.177
01/13 22:01:22 starting evaluation
01/13 22:05:17 test bleu=8.77 loss=241.52 penalty=1.000 ratio=1.040
01/13 22:05:17 saving model to models/sperate/codenn/checkpoints
01/13 22:05:17 finished saving model
01/13 22:05:17 new best model
01/13 22:23:27   decaying learning rate to: 0.0101
01/13 22:29:04 step 188000 epoch 77 learning rate 0.0101 step-time 0.711 loss 0.174
01/13 22:29:04 starting evaluation
01/13 22:33:00 test bleu=8.32 loss=241.30 penalty=1.000 ratio=1.068
01/13 22:33:00 saving model to models/sperate/codenn/checkpoints
01/13 22:33:00 finished saving model
01/13 22:56:24   decaying learning rate to: 0.00963
01/13 22:56:52 step 190000 epoch 78 learning rate 0.00963 step-time 0.714 loss 0.174
01/13 22:56:52 starting evaluation
01/13 23:00:52 test bleu=8.41 loss=241.74 penalty=1.000 ratio=1.072
01/13 23:00:52 saving model to models/sperate/codenn/checkpoints
01/13 23:00:52 finished saving model
01/13 23:24:55 step 192000 epoch 78 learning rate 0.00963 step-time 0.719 loss 0.162
01/13 23:24:55 starting evaluation
01/13 23:28:54 test bleu=8.18 loss=241.97 penalty=1.000 ratio=1.091
01/13 23:28:54 saving model to models/sperate/codenn/checkpoints
01/13 23:28:54 finished saving model
01/13 23:34:05   decaying learning rate to: 0.00915
01/13 23:53:18 step 194000 epoch 79 learning rate 0.00915 step-time 0.730 loss 0.159
01/13 23:53:18 starting evaluation
01/13 23:57:00 test bleu=8.44 loss=243.30 penalty=1.000 ratio=1.059
01/13 23:57:00 saving model to models/sperate/codenn/checkpoints
01/13 23:57:00 finished saving model
01/14 00:07:51   decaying learning rate to: 0.00869
01/14 00:21:22 step 196000 epoch 80 learning rate 0.00869 step-time 0.729 loss 0.157
01/14 00:21:22 starting evaluation
01/14 00:25:02 test bleu=8.25 loss=242.75 penalty=1.000 ratio=1.081
01/14 00:25:02 saving model to models/sperate/codenn/checkpoints
01/14 00:25:02 finished saving model
01/14 00:41:21   decaying learning rate to: 0.00826
01/14 00:49:08 step 198000 epoch 81 learning rate 0.00826 step-time 0.721 loss 0.158
01/14 00:49:08 starting evaluation
01/14 00:53:04 test bleu=8.22 loss=243.68 penalty=1.000 ratio=1.097
01/14 00:53:04 saving model to models/sperate/codenn/checkpoints
01/14 00:53:04 finished saving model
01/14 01:14:45   decaying learning rate to: 0.00784
01/14 01:16:52 step 200000 epoch 82 learning rate 0.00784 step-time 0.712 loss 0.156
01/14 01:16:52 starting evaluation
01/14 01:20:47 test bleu=8.29 loss=244.11 penalty=1.000 ratio=1.076
01/14 01:20:47 saving model to models/sperate/codenn/checkpoints
01/14 01:20:47 finished saving model
01/14 01:44:30 step 202000 epoch 82 learning rate 0.00784 step-time 0.710 loss 0.147
01/14 01:44:30 starting evaluation
01/14 01:48:26 test bleu=8.21 loss=244.13 penalty=1.000 ratio=1.088
01/14 01:48:26 saving model to models/sperate/codenn/checkpoints
01/14 01:48:26 finished saving model
01/14 01:52:02   decaying learning rate to: 0.00745
01/14 02:12:18 step 204000 epoch 83 learning rate 0.00745 step-time 0.714 loss 0.145
01/14 02:12:18 starting evaluation
01/14 02:16:12 test bleu=8.14 loss=244.38 penalty=1.000 ratio=1.076
01/14 02:16:12 saving model to models/sperate/codenn/checkpoints
01/14 02:16:13 finished saving model
01/14 02:25:13   decaying learning rate to: 0.00708
01/14 02:39:58 step 206000 epoch 84 learning rate 0.00708 step-time 0.711 loss 0.140
01/14 02:39:58 starting evaluation
01/14 02:43:54 test bleu=8.49 loss=244.54 penalty=1.000 ratio=1.062
01/14 02:43:54 saving model to models/sperate/codenn/checkpoints
01/14 02:43:54 finished saving model
01/14 02:58:21   decaying learning rate to: 0.00673
01/14 03:07:43 step 208000 epoch 85 learning rate 0.00673 step-time 0.713 loss 0.144
01/14 03:07:43 starting evaluation
01/14 03:11:39 test bleu=8.19 loss=245.20 penalty=1.000 ratio=1.087
01/14 03:11:39 saving model to models/sperate/codenn/checkpoints
01/14 03:11:39 finished saving model
01/14 03:31:43   decaying learning rate to: 0.00639
01/14 03:35:28 step 210000 epoch 86 learning rate 0.00639 step-time 0.713 loss 0.139
01/14 03:35:28 starting evaluation
01/14 03:39:25 test bleu=8.30 loss=245.75 penalty=1.000 ratio=1.065
01/14 03:39:25 saving model to models/sperate/codenn/checkpoints
01/14 03:39:25 finished saving model
01/14 04:03:13 step 212000 epoch 86 learning rate 0.00639 step-time 0.712 loss 0.139
01/14 04:03:13 starting evaluation
01/14 04:07:09 test bleu=8.19 loss=245.78 penalty=1.000 ratio=1.075
01/14 04:07:09 saving model to models/sperate/codenn/checkpoints
01/14 04:07:09 finished saving model
01/14 04:09:08   decaying learning rate to: 0.00607
01/14 04:30:56 step 214000 epoch 87 learning rate 0.00607 step-time 0.712 loss 0.131
01/14 04:30:56 starting evaluation
01/14 04:34:50 test bleu=8.57 loss=245.81 penalty=1.000 ratio=1.056
01/14 04:34:50 saving model to models/sperate/codenn/checkpoints
01/14 04:34:51 finished saving model
01/14 04:42:30   decaying learning rate to: 0.00577
01/14 04:58:47 step 216000 epoch 88 learning rate 0.00577 step-time 0.716 loss 0.131
01/14 04:58:47 starting evaluation
01/14 05:02:44 test bleu=8.48 loss=246.37 penalty=1.000 ratio=1.066
01/14 05:02:44 saving model to models/sperate/codenn/checkpoints
01/14 05:02:45 finished saving model
01/14 05:16:06   decaying learning rate to: 0.00548
01/14 05:26:47 step 218000 epoch 89 learning rate 0.00548 step-time 0.719 loss 0.129
01/14 05:26:47 starting evaluation
01/14 05:30:47 test bleu=8.42 loss=247.28 penalty=1.000 ratio=1.067
01/14 05:30:47 saving model to models/sperate/codenn/checkpoints
01/14 05:30:47 finished saving model
01/14 05:49:47   decaying learning rate to: 0.0052
01/14 05:55:06 step 220000 epoch 90 learning rate 0.0052 step-time 0.728 loss 0.131
01/14 05:55:06 starting evaluation
01/14 05:58:49 test bleu=8.38 loss=247.22 penalty=1.000 ratio=1.068
01/14 05:58:49 saving model to models/sperate/codenn/checkpoints
01/14 05:58:49 finished saving model
01/14 06:23:03 step 222000 epoch 91 learning rate 0.0052 step-time 0.725 loss 0.130
01/14 06:23:03 starting evaluation
01/14 06:26:45 test bleu=8.19 loss=246.89 penalty=1.000 ratio=1.070
01/14 06:26:45 saving model to models/sperate/codenn/checkpoints
01/14 06:26:46 finished saving model
01/14 06:27:03   decaying learning rate to: 0.00494
01/14 06:50:49 step 224000 epoch 91 learning rate 0.00494 step-time 0.720 loss 0.121
01/14 06:50:49 starting evaluation
01/14 06:54:44 test bleu=8.48 loss=247.27 penalty=1.000 ratio=1.069
01/14 06:54:44 saving model to models/sperate/codenn/checkpoints
01/14 06:54:44 finished saving model
01/14 07:00:13   decaying learning rate to: 0.0047
01/14 07:18:32 step 226000 epoch 92 learning rate 0.0047 step-time 0.712 loss 0.121
01/14 07:18:32 starting evaluation
01/14 07:22:27 test bleu=8.40 loss=248.26 penalty=1.000 ratio=1.082
01/14 07:22:27 saving model to models/sperate/codenn/checkpoints
01/14 07:22:28 finished saving model
01/14 07:33:37   decaying learning rate to: 0.00446
01/14 07:46:13 step 228000 epoch 93 learning rate 0.00446 step-time 0.711 loss 0.121
01/14 07:46:13 starting evaluation
01/14 07:50:09 test bleu=8.54 loss=248.17 penalty=1.000 ratio=1.057
01/14 07:50:09 saving model to models/sperate/codenn/checkpoints
01/14 07:50:09 finished saving model
01/14 08:07:01   decaying learning rate to: 0.00424
01/14 08:13:59 step 230000 epoch 94 learning rate 0.00424 step-time 0.713 loss 0.122
01/14 08:13:59 starting evaluation
01/14 08:17:54 test bleu=8.51 loss=248.22 penalty=1.000 ratio=1.065
01/14 08:17:54 saving model to models/sperate/codenn/checkpoints
01/14 08:17:54 finished saving model
01/14 08:40:20   decaying learning rate to: 0.00403
01/14 08:41:33 step 232000 epoch 95 learning rate 0.00403 step-time 0.708 loss 0.121
01/14 08:41:33 starting evaluation
01/14 08:45:29 test bleu=8.45 loss=248.78 penalty=1.000 ratio=1.067
01/14 08:45:29 saving model to models/sperate/codenn/checkpoints
01/14 08:45:29 finished saving model
01/14 09:09:14 step 234000 epoch 95 learning rate 0.00403 step-time 0.711 loss 0.116
01/14 09:09:14 starting evaluation
01/14 09:13:09 test bleu=8.16 loss=248.74 penalty=1.000 ratio=1.077
01/14 09:13:09 saving model to models/sperate/codenn/checkpoints
01/14 09:13:09 finished saving model
01/14 09:17:36   decaying learning rate to: 0.00383
01/14 09:36:57 step 236000 epoch 96 learning rate 0.00383 step-time 0.712 loss 0.114
01/14 09:36:57 starting evaluation
01/14 09:40:53 test bleu=8.42 loss=249.17 penalty=1.000 ratio=1.076
01/14 09:40:53 saving model to models/sperate/codenn/checkpoints
01/14 09:40:53 finished saving model
01/14 09:50:57   decaying learning rate to: 0.00363
01/14 10:04:31 step 238000 epoch 97 learning rate 0.00363 step-time 0.707 loss 0.114
01/14 10:04:31 starting evaluation
01/14 10:08:26 test bleu=8.47 loss=249.53 penalty=1.000 ratio=1.070
01/14 10:08:26 saving model to models/sperate/codenn/checkpoints
01/14 10:08:26 finished saving model
01/14 10:24:16   decaying learning rate to: 0.00345
01/14 10:32:13 step 240000 epoch 98 learning rate 0.00345 step-time 0.711 loss 0.113
01/14 10:32:13 starting evaluation
01/14 10:36:09 test bleu=8.49 loss=249.84 penalty=1.000 ratio=1.070
01/14 10:36:09 saving model to models/sperate/codenn/checkpoints
01/14 10:36:09 finished saving model
01/14 10:57:40   decaying learning rate to: 0.00328
01/14 11:00:07 step 242000 epoch 99 learning rate 0.00328 step-time 0.717 loss 0.115
01/14 11:00:07 starting evaluation
01/14 11:04:07 test bleu=8.42 loss=249.95 penalty=1.000 ratio=1.073
01/14 11:04:07 saving model to models/sperate/codenn/checkpoints
01/14 11:04:07 finished saving model
01/14 11:28:25 step 244000 epoch 99 learning rate 0.00328 step-time 0.727 loss 0.113
01/14 11:28:25 starting evaluation
01/14 11:32:18 test bleu=8.47 loss=249.69 penalty=1.000 ratio=1.074
01/14 11:32:18 saving model to models/sperate/codenn/checkpoints
01/14 11:32:18 finished saving model
01/14 11:35:09   decaying learning rate to: 0.00312
01/14 11:56:43 step 246000 epoch 100 learning rate 0.00312 step-time 0.731 loss 0.108
01/14 11:56:43 starting evaluation
01/14 12:00:26 test bleu=8.36 loss=250.71 penalty=1.000 ratio=1.079
01/14 12:00:26 saving model to models/sperate/codenn/checkpoints
01/14 12:00:26 finished saving model
01/14 12:07:42 finished training
01/14 12:07:42 exiting...
01/14 12:07:42 saving model to models/sperate/codenn/checkpoints
01/14 12:07:42 finished saving model
