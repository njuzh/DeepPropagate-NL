nohup: ignoring input
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:107: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:30: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

12/29 10:01:03 label: default
12/29 10:01:03 description:
  default configuration
  next line of description
  last line
12/29 10:01:03 /root/icpc/icpc/translate/__main__.py config/10-folds/8_fold/hybrid_pnl/config.yaml --train -v
12/29 10:01:03 commit hash 74e0554cb3eb5df835cef993ad570ff8de651f71
12/29 10:01:03 tensorflow version: 1.14.0
12/29 10:01:03 program arguments
12/29 10:01:03   aggregation_method   'sum'
12/29 10:01:03   align_encoder_id     0
12/29 10:01:03   allow_growth         True
12/29 10:01:03   attention_type       'global'
12/29 10:01:03   attn_filter_length   0
12/29 10:01:03   attn_filters         0
12/29 10:01:03   attn_prev_word       False
12/29 10:01:03   attn_size            128
12/29 10:01:03   attn_temperature     1.0
12/29 10:01:03   attn_window_size     0
12/29 10:01:03   average              False
12/29 10:01:03   baseline_activation  None
12/29 10:01:03   baseline_learning_rate 0.001
12/29 10:01:03   baseline_optimizer   'adam'
12/29 10:01:03   baseline_steps       0
12/29 10:01:03   batch_mode           'standard'
12/29 10:01:03   batch_size           64
12/29 10:01:03   beam_size            5
12/29 10:01:03   bidir                True
12/29 10:01:03   bidir_projection     False
12/29 10:01:03   binary               False
12/29 10:01:03   cell_size            256
12/29 10:01:03   cell_type            'GRU'
12/29 10:01:03   character_level      False
12/29 10:01:03   checkpoints          []
12/29 10:01:03   conditional_rnn      False
12/29 10:01:03   config               'config/10-folds/8_fold/hybrid_pnl/config.yaml'
12/29 10:01:03   convolutions         None
12/29 10:01:03   data_dir             'data/gooddata/8_fold'
12/29 10:01:03   debug                False
12/29 10:01:03   decay_after_n_epoch  1
12/29 10:01:03   decay_every_n_epoch  1
12/29 10:01:03   decay_if_no_progress None
12/29 10:01:03   decoders             [{'max_len': 40, 'name': 'nl'}]
12/29 10:01:03   description          'default configuration\nnext line of description\nlast line\n'
12/29 10:01:03   dev_prefix           'test'
12/29 10:01:03   early_stopping       True
12/29 10:01:03   embedding_dropout    0.0
12/29 10:01:03   embedding_initializer None
12/29 10:01:03   embedding_size       256
12/29 10:01:03   embedding_weight_scale None
12/29 10:01:03   embeddings_on_cpu    True
12/29 10:01:03   encoders             [{'attention_type': 'global', 'max_len': 200, 'name': 'code'},
 {'attention_type': 'global', 'max_len': 80, 'name': 'pnl'}]
12/29 10:01:03   ensemble             False
12/29 10:01:03   eval_burn_in         0
12/29 10:01:03   feed_previous        0.0
12/29 10:01:03   final_state          'last'
12/29 10:01:03   freeze_variables     []
12/29 10:01:03   generate_first       True
12/29 10:01:03   gpu_id               0
12/29 10:01:03   highway_layers       0
12/29 10:01:03   initial_state_dropout 0.0
12/29 10:01:03   initializer          None
12/29 10:01:03   input_layer_dropout  0.0
12/29 10:01:03   input_layers         None
12/29 10:01:03   keep_best            5
12/29 10:01:03   keep_every_n_hours   0
12/29 10:01:03   label                'default'
12/29 10:01:03   layer_norm           False
12/29 10:01:03   layers               1
12/29 10:01:03   learning_rate        0.5
12/29 10:01:03   learning_rate_decay_factor 0.95
12/29 10:01:03   len_normalization    1.0
12/29 10:01:03   log_file             'log.txt'
12/29 10:01:03   loss_function        'xent'
12/29 10:01:03   max_dev_size         0
12/29 10:01:03   max_epochs           100
12/29 10:01:03   max_gradient_norm    5.0
12/29 10:01:03   max_len              50
12/29 10:01:03   max_steps            600000
12/29 10:01:03   max_test_size        0
12/29 10:01:03   max_to_keep          1
12/29 10:01:03   max_train_size       0
12/29 10:01:03   maxout_stride        None
12/29 10:01:03   mem_fraction         1.0
12/29 10:01:03   min_learning_rate    1e-06
12/29 10:01:03   model_dir            'models/8_fold_hybrid_pnl'
12/29 10:01:03   moving_average       None
12/29 10:01:03   no_gpu               False
12/29 10:01:03   optimizer            'sgd'
12/29 10:01:03   orthogonal_init      False
12/29 10:01:03   output               None
12/29 10:01:03   output_dropout       0.0
12/29 10:01:03   parallel_iterations  16
12/29 10:01:03   pervasive_dropout    False
12/29 10:01:03   pooling_avg          True
12/29 10:01:03   post_process_script  None
12/29 10:01:03   pred_deep_layer      False
12/29 10:01:03   pred_edits           False
12/29 10:01:03   pred_embed_proj      True
12/29 10:01:03   pred_maxout_layer    True
12/29 10:01:03   purge                False
12/29 10:01:03   raw_output           False
12/29 10:01:03   read_ahead           1
12/29 10:01:03   reconstruction_attn_weight 0.05
12/29 10:01:03   reconstruction_decoders False
12/29 10:01:03   reconstruction_weight 1.0
12/29 10:01:03   reinforce_after_n_epoch None
12/29 10:01:03   remove_unk           False
12/29 10:01:03   reverse              False
12/29 10:01:03   reverse_input        True
12/29 10:01:03   reward_function      'sentence_bleu'
12/29 10:01:03   rnn_feed_attn        True
12/29 10:01:03   rnn_input_dropout    0.0
12/29 10:01:03   rnn_output_dropout   0.0
12/29 10:01:03   rnn_state_dropout    0.0
12/29 10:01:03   save                 False
12/29 10:01:03   score_function       'corpus_bleu'
12/29 10:01:03   score_functions      ['bleu', 'loss']
12/29 10:01:03   script_dir           'scripts'
12/29 10:01:03   sgd_after_n_epoch    None
12/29 10:01:03   sgd_learning_rate    1.0
12/29 10:01:03   shuffle              True
12/29 10:01:03   softmax_temperature  1.0
12/29 10:01:03   steps_per_checkpoint 2000
12/29 10:01:03   steps_per_eval       2000
12/29 10:01:03   swap_memory          True
12/29 10:01:03   tie_embeddings       False
12/29 10:01:03   time_pooling         None
12/29 10:01:03   train                True
12/29 10:01:03   train_initial_states True
12/29 10:01:03   train_prefix         'train'
12/29 10:01:03   truncate_lines       True
12/29 10:01:03   update_first         False
12/29 10:01:03   use_baseline         False
12/29 10:01:03   use_dropout          False
12/29 10:01:03   use_lstm_full_state  False
12/29 10:01:03   use_previous_word    True
12/29 10:01:03   verbose              True
12/29 10:01:03   vocab_prefix         'vocab'
12/29 10:01:03   weight_scale         None
12/29 10:01:03   word_dropout         0.0
12/29 10:01:03 python random seed: 2269877544987853598
12/29 10:01:03 tf random seed:     7392542449536001868
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:203: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

12/29 10:01:03 creating model
12/29 10:01:03 using device: /gpu:0
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:230: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

12/29 10:01:03 copying vocab to models/8_fold_hybrid_pnl/data/vocab.code
12/29 10:01:03 copying vocab to models/8_fold_hybrid_pnl/data/vocab.pnl
12/29 10:01:03 copying vocab to models/8_fold_hybrid_pnl/data/vocab.nl
12/29 10:01:03 reading vocabularies
12/29 10:01:03 creating model
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:60: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:111: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:33: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f437d171828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f437d171828>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:226: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f437d171fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f437d171fd0>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f4401be9240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f4401be9240>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f4401be9710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f4401be9710>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4401be96d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4401be96d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:838: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f440187bac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f440187bac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f440189fb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f440189fb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:432: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:435: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4401813a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4401813a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f44017aeb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f44017aeb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4401611ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4401611ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4401611e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4401611e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43fd349cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43fd349cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43fd349c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43fd349c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43fd338080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43fd338080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43fd300ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43fd300ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f44017f6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f44017f6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:919: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f43fd1ab2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f43fd1ab2b0>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:131: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:223: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f43a2e906d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f43a2e906d8>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43a2dcc3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43a2dcc3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43a2e90a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43a2e90a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43a2e33cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43a2e33cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43a2d122b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43a2d122b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43a2d125f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43a2d125f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43a2c66ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43a2c66ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43a2c66ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f43a2c66ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
12/29 10:01:10 model parameters (45)
12/29 10:01:10   baseline_step:0 ()
12/29 10:01:10   decoder_nl/attention_code/U_a/kernel:0 (512, 128)
12/29 10:01:10   decoder_nl/attention_code/W_a/bias:0 (128,)
12/29 10:01:10   decoder_nl/attention_code/W_a/kernel:0 (256, 128)
12/29 10:01:10   decoder_nl/attention_code/v_a:0 (128,)
12/29 10:01:10   decoder_nl/attention_pnl/U_a/kernel:0 (512, 128)
12/29 10:01:10   decoder_nl/attention_pnl/W_a/bias:0 (128,)
12/29 10:01:10   decoder_nl/attention_pnl/W_a/kernel:0 (256, 128)
12/29 10:01:10   decoder_nl/attention_pnl/v_a:0 (128,)
12/29 10:01:10   decoder_nl/code_pnl/initial_state_projection/bias:0 (256,)
12/29 10:01:10   decoder_nl/code_pnl/initial_state_projection/kernel:0 (512, 256)
12/29 10:01:10   decoder_nl/gru_cell/candidate/bias:0 (256,)
12/29 10:01:10   decoder_nl/gru_cell/candidate/kernel:0 (1024, 256)
12/29 10:01:10   decoder_nl/gru_cell/gates/bias:0 (512,)
12/29 10:01:10   decoder_nl/gru_cell/gates/kernel:0 (1024, 512)
12/29 10:01:10   decoder_nl/maxout/bias:0 (256,)
12/29 10:01:10   decoder_nl/maxout/kernel:0 (1024, 256)
12/29 10:01:10   decoder_nl/softmax0/kernel:0 (128, 256)
12/29 10:01:10   decoder_nl/softmax1/bias:0 (38032,)
12/29 10:01:10   decoder_nl/softmax1/kernel:0 (256, 38032)
12/29 10:01:10   embedding_code:0 (50000, 256)
12/29 10:01:10   embedding_nl:0 (38032, 256)
12/29 10:01:10   embedding_pnl:0 (37478, 256)
12/29 10:01:10   encoder_code/initial_state_bw:0 (256,)
12/29 10:01:10   encoder_code/initial_state_fw:0 (256,)
12/29 10:01:10   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/29 10:01:10   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/29 10:01:10   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/29 10:01:10   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/29 10:01:10   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/29 10:01:10   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/29 10:01:10   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/29 10:01:10   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/29 10:01:10   encoder_pnl/initial_state_bw:0 (256,)
12/29 10:01:10   encoder_pnl/initial_state_fw:0 (256,)
12/29 10:01:10   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/29 10:01:10   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/29 10:01:10   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/29 10:01:10   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/29 10:01:10   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/29 10:01:10   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/29 10:01:10   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/29 10:01:10   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/29 10:01:10   global_step:0 ()
12/29 10:01:10   learning_rate:0 ()
12/29 10:01:10 number of parameters: 44.89M
WARNING:tensorflow:From /root/icpc/icpc/translate/translation_model.py:666: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

12/29 10:01:12 global step: 0
12/29 10:01:12 baseline step: 0
12/29 10:01:12 reading training data
12/29 10:01:12 total line count: 156721
12/29 10:01:17   lines read: 100000
12/29 10:01:20 files: data/gooddata/8_fold/train.code data/gooddata/8_fold/train.pnl data/gooddata/8_fold/train.nl
12/29 10:01:20 lines reads: 156721
12/29 10:01:20 reading development data
12/29 10:01:21 files: data/gooddata/8_fold/test.code data/gooddata/8_fold/test.pnl data/gooddata/8_fold/test.nl
12/29 10:01:21 lines reads: 17413
12/29 10:01:22 starting training
12/29 10:27:59 step 2000 epoch 1 learning rate 0.5 step-time 0.797 loss 80.335
12/29 10:27:59 starting evaluation
12/29 10:32:55 test bleu=0.79 loss=65.03 penalty=0.768 ratio=0.791
12/29 10:32:55 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 10:32:56 finished saving model
12/29 10:32:56 new best model
12/29 10:38:56   decaying learning rate to: 0.475
12/29 11:00:05 step 4000 epoch 2 learning rate 0.475 step-time 0.813 loss 59.807
12/29 11:00:05 starting evaluation
12/29 11:05:09 test bleu=3.66 loss=56.26 penalty=1.000 ratio=1.478
12/29 11:05:09 saving model to models/8_fold_hybrid_pnl/checkpoints
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
12/29 11:05:09 finished saving model
12/29 11:05:09 new best model
12/29 11:17:16   decaying learning rate to: 0.451
12/29 11:32:24 step 6000 epoch 3 learning rate 0.451 step-time 0.815 loss 52.265
12/29 11:32:24 starting evaluation
12/29 11:37:26 test bleu=6.24 loss=50.39 penalty=1.000 ratio=1.510
12/29 11:37:26 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 11:37:26 finished saving model
12/29 11:37:26 new best model
12/29 11:55:42   decaying learning rate to: 0.429
12/29 12:04:36 step 8000 epoch 4 learning rate 0.429 step-time 0.813 loss 46.657
12/29 12:04:36 starting evaluation
12/29 12:09:40 test bleu=11.13 loss=46.09 penalty=1.000 ratio=1.243
12/29 12:09:40 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 12:09:40 finished saving model
12/29 12:09:40 new best model
12/29 12:34:06   decaying learning rate to: 0.407
12/29 12:36:53 step 10000 epoch 5 learning rate 0.407 step-time 0.814 loss 42.640
12/29 12:36:53 starting evaluation
12/29 12:41:53 test bleu=16.89 loss=42.69 penalty=0.888 ratio=0.894
12/29 12:41:53 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 12:41:53 finished saving model
12/29 12:41:53 new best model
12/29 13:09:06 step 12000 epoch 5 learning rate 0.407 step-time 0.814 loss 39.114
12/29 13:09:06 starting evaluation
12/29 13:14:03 test bleu=18.53 loss=40.48 penalty=0.832 ratio=0.845
12/29 13:14:03 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 13:14:03 finished saving model
12/29 13:14:03 new best model
12/29 13:17:22   decaying learning rate to: 0.387
12/29 13:41:14 step 14000 epoch 6 learning rate 0.387 step-time 0.814 loss 35.882
12/29 13:41:14 starting evaluation
12/29 13:46:12 test bleu=20.49 loss=39.34 penalty=0.870 ratio=0.878
12/29 13:46:12 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 13:46:12 finished saving model
12/29 13:46:12 new best model
12/29 13:55:39   decaying learning rate to: 0.368
12/29 14:13:24 step 16000 epoch 7 learning rate 0.368 step-time 0.814 loss 33.640
12/29 14:13:24 starting evaluation
12/29 14:18:02 test bleu=23.24 loss=37.94 penalty=0.855 ratio=0.864
12/29 14:18:02 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 14:18:02 finished saving model
12/29 14:18:02 new best model
12/29 14:33:36   decaying learning rate to: 0.349
12/29 14:45:13 step 18000 epoch 8 learning rate 0.349 step-time 0.813 loss 31.254
12/29 14:45:13 starting evaluation
12/29 14:50:15 test bleu=23.95 loss=37.76 penalty=1.000 ratio=1.021
12/29 14:50:15 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 14:50:15 finished saving model
12/29 14:50:15 new best model
12/29 15:11:51   decaying learning rate to: 0.332
12/29 15:17:29 step 20000 epoch 9 learning rate 0.332 step-time 0.815 loss 29.371
12/29 15:17:29 starting evaluation
12/29 15:22:28 test bleu=25.68 loss=37.08 penalty=0.912 ratio=0.915
12/29 15:22:28 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 15:22:28 finished saving model
12/29 15:22:28 new best model
12/29 15:49:44 step 22000 epoch 9 learning rate 0.332 step-time 0.816 loss 27.472
12/29 15:49:44 starting evaluation
12/29 15:54:39 test bleu=25.91 loss=36.36 penalty=0.861 ratio=0.870
12/29 15:54:39 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 15:54:39 finished saving model
12/29 15:54:39 new best model
12/29 15:55:13   decaying learning rate to: 0.315
12/29 16:21:53 step 24000 epoch 10 learning rate 0.315 step-time 0.815 loss 24.843
12/29 16:21:53 starting evaluation
12/29 16:26:50 test bleu=27.59 loss=36.51 penalty=0.881 ratio=0.888
12/29 16:26:50 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 16:26:50 finished saving model
12/29 16:26:50 new best model
12/29 16:33:28   decaying learning rate to: 0.299
12/29 16:54:04 step 26000 epoch 11 learning rate 0.299 step-time 0.815 loss 23.015
12/29 16:54:04 starting evaluation
12/29 16:59:05 test bleu=28.34 loss=36.97 penalty=0.901 ratio=0.905
12/29 16:59:05 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 16:59:05 finished saving model
12/29 16:59:05 new best model
12/29 17:11:55   decaying learning rate to: 0.284
12/29 17:26:16 step 28000 epoch 12 learning rate 0.284 step-time 0.813 loss 21.353
12/29 17:26:16 starting evaluation
12/29 17:31:14 test bleu=29.24 loss=37.55 penalty=0.892 ratio=0.897
12/29 17:31:14 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 17:31:14 finished saving model
12/29 17:31:14 new best model
12/29 17:50:11   decaying learning rate to: 0.27
12/29 17:58:31 step 30000 epoch 13 learning rate 0.27 step-time 0.816 loss 19.811
12/29 17:58:31 starting evaluation
12/29 18:03:26 test bleu=30.03 loss=38.79 penalty=0.895 ratio=0.900
12/29 18:03:26 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 18:03:26 finished saving model
12/29 18:03:26 new best model
12/29 18:28:28   decaying learning rate to: 0.257
12/29 18:30:39 step 32000 epoch 14 learning rate 0.257 step-time 0.814 loss 18.402
12/29 18:30:39 starting evaluation
12/29 18:35:31 test bleu=31.22 loss=39.10 penalty=0.987 ratio=0.987
12/29 18:35:31 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 18:35:31 finished saving model
12/29 18:35:31 new best model
12/29 19:02:36 step 34000 epoch 14 learning rate 0.257 step-time 0.810 loss 16.506
12/29 19:02:36 starting evaluation
12/29 19:07:36 test bleu=30.12 loss=38.70 penalty=0.846 ratio=0.857
12/29 19:07:36 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 19:07:36 finished saving model
12/29 19:11:30   decaying learning rate to: 0.244
12/29 19:34:51 step 36000 epoch 15 learning rate 0.244 step-time 0.816 loss 14.831
12/29 19:34:51 starting evaluation
12/29 19:39:45 test bleu=31.86 loss=40.29 penalty=0.897 ratio=0.902
12/29 19:39:45 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 19:39:45 finished saving model
12/29 19:39:45 new best model
12/29 19:49:45   decaying learning rate to: 0.232
12/29 20:06:59 step 38000 epoch 16 learning rate 0.232 step-time 0.815 loss 13.607
12/29 20:06:59 starting evaluation
12/29 20:11:56 test bleu=32.15 loss=42.40 penalty=0.919 ratio=0.922
12/29 20:11:56 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 20:11:57 finished saving model
12/29 20:11:57 new best model
12/29 20:28:00   decaying learning rate to: 0.22
12/29 20:39:11 step 40000 epoch 17 learning rate 0.22 step-time 0.815 loss 12.516
12/29 20:39:11 starting evaluation
12/29 20:44:08 test bleu=32.97 loss=43.79 penalty=0.939 ratio=0.941
12/29 20:44:08 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 20:44:08 finished saving model
12/29 20:44:08 new best model
12/29 21:06:22   decaying learning rate to: 0.209
12/29 21:11:23 step 42000 epoch 18 learning rate 0.209 step-time 0.815 loss 11.517
12/29 21:11:23 starting evaluation
12/29 21:16:22 test bleu=33.21 loss=45.42 penalty=0.967 ratio=0.967
12/29 21:16:22 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 21:16:23 finished saving model
12/29 21:16:23 new best model
12/29 21:43:39 step 44000 epoch 18 learning rate 0.209 step-time 0.816 loss 10.354
12/29 21:43:39 starting evaluation
12/29 21:48:31 test bleu=33.35 loss=44.28 penalty=0.907 ratio=0.911
12/29 21:48:31 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 21:48:31 finished saving model
12/29 21:48:31 new best model
12/29 21:49:39   decaying learning rate to: 0.199
12/29 22:15:49 step 46000 epoch 19 learning rate 0.199 step-time 0.817 loss 8.880
12/29 22:15:49 starting evaluation
12/29 22:20:39 test bleu=33.60 loss=47.00 penalty=0.935 ratio=0.937
12/29 22:20:39 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 22:20:39 finished saving model
12/29 22:20:39 new best model
12/29 22:27:58   decaying learning rate to: 0.189
12/29 22:47:57 step 48000 epoch 20 learning rate 0.189 step-time 0.817 loss 8.085
12/29 22:47:57 starting evaluation
12/29 22:52:49 test bleu=34.05 loss=49.49 penalty=0.968 ratio=0.968
12/29 22:52:49 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 22:52:49 finished saving model
12/29 22:52:49 new best model
12/29 23:06:07   decaying learning rate to: 0.179
12/29 23:20:03 step 50000 epoch 21 learning rate 0.179 step-time 0.815 loss 7.353
12/29 23:20:03 starting evaluation
12/29 23:25:06 test bleu=34.65 loss=52.25 penalty=0.987 ratio=0.987
12/29 23:25:06 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 23:25:07 finished saving model
12/29 23:25:07 new best model
12/29 23:44:32   decaying learning rate to: 0.17
12/29 23:52:16 step 52000 epoch 22 learning rate 0.17 step-time 0.813 loss 6.636
12/29 23:52:16 starting evaluation
12/29 23:57:12 test bleu=34.07 loss=54.21 penalty=1.000 ratio=1.004
12/29 23:57:12 saving model to models/8_fold_hybrid_pnl/checkpoints
12/29 23:57:12 finished saving model
12/30 00:22:11   decaying learning rate to: 0.162
12/30 00:23:48 step 54000 epoch 23 learning rate 0.162 step-time 0.796 loss 6.055
12/30 00:23:48 starting evaluation
12/30 00:28:40 test bleu=34.75 loss=55.69 penalty=0.973 ratio=0.974
12/30 00:28:40 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 00:28:41 finished saving model
12/30 00:28:41 new best model
12/30 00:55:13 step 56000 epoch 23 learning rate 0.162 step-time 0.794 loss 5.144
12/30 00:55:13 starting evaluation
12/30 01:00:05 test bleu=35.34 loss=56.65 penalty=0.959 ratio=0.960
12/30 01:00:05 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 01:00:06 finished saving model
12/30 01:00:06 new best model
12/30 01:04:23   decaying learning rate to: 0.154
12/30 01:26:40 step 58000 epoch 24 learning rate 0.154 step-time 0.795 loss 4.521
12/30 01:26:40 starting evaluation
12/30 01:31:31 test bleu=34.82 loss=60.55 penalty=0.944 ratio=0.945
12/30 01:31:31 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 01:31:31 finished saving model
12/30 01:41:56   decaying learning rate to: 0.146
12/30 01:58:11 step 60000 epoch 25 learning rate 0.146 step-time 0.798 loss 4.116
12/30 01:58:11 starting evaluation
12/30 02:03:05 test bleu=34.90 loss=62.65 penalty=0.961 ratio=0.962
12/30 02:03:05 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 02:03:05 finished saving model
12/30 02:19:30   decaying learning rate to: 0.139
12/30 02:29:39 step 62000 epoch 26 learning rate 0.139 step-time 0.795 loss 3.720
12/30 02:29:39 starting evaluation
12/30 02:34:32 test bleu=35.64 loss=64.93 penalty=0.964 ratio=0.964
12/30 02:34:32 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 02:34:32 finished saving model
12/30 02:34:32 new best model
12/30 02:56:42   decaying learning rate to: 0.132
12/30 03:01:00 step 64000 epoch 27 learning rate 0.132 step-time 0.792 loss 3.363
12/30 03:01:00 starting evaluation
12/30 03:05:43 test bleu=35.96 loss=67.22 penalty=0.988 ratio=0.988
12/30 03:05:43 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 03:05:43 finished saving model
12/30 03:05:43 new best model
12/30 03:32:45 step 66000 epoch 27 learning rate 0.132 step-time 0.809 loss 2.955
12/30 03:32:45 starting evaluation
12/30 03:37:39 test bleu=35.24 loss=68.75 penalty=1.000 ratio=1.015
12/30 03:37:39 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 03:37:39 finished saving model
12/30 03:39:18   decaying learning rate to: 0.125
12/30 04:04:51 step 68000 epoch 28 learning rate 0.125 step-time 0.814 loss 2.504
12/30 04:04:51 starting evaluation
12/30 04:09:53 test bleu=35.93 loss=71.43 penalty=0.976 ratio=0.977
12/30 04:09:53 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 04:09:53 finished saving model
12/30 04:17:33   decaying learning rate to: 0.119
12/30 04:37:01 step 70000 epoch 29 learning rate 0.119 step-time 0.812 loss 2.286
12/30 04:37:01 starting evaluation
12/30 04:42:01 test bleu=35.88 loss=73.54 penalty=1.000 ratio=1.014
12/30 04:42:01 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 04:42:01 finished saving model
12/30 04:55:42   decaying learning rate to: 0.113
12/30 05:09:01 step 72000 epoch 30 learning rate 0.113 step-time 0.808 loss 2.081
12/30 05:09:01 starting evaluation
12/30 05:14:00 test bleu=36.04 loss=75.13 penalty=1.000 ratio=1.007
12/30 05:14:00 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 05:14:00 finished saving model
12/30 05:14:00 new best model
12/30 05:33:58   decaying learning rate to: 0.107
12/30 05:41:12 step 74000 epoch 31 learning rate 0.107 step-time 0.814 loss 1.902
12/30 05:41:12 starting evaluation
12/30 05:46:11 test bleu=35.66 loss=78.39 penalty=1.000 ratio=1.021
12/30 05:46:11 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 05:46:11 finished saving model
12/30 06:12:16   decaying learning rate to: 0.102
12/30 06:13:20 step 76000 epoch 32 learning rate 0.102 step-time 0.812 loss 1.712
12/30 06:13:20 starting evaluation
12/30 06:18:21 test bleu=35.78 loss=79.30 penalty=1.000 ratio=1.018
12/30 06:18:21 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 06:18:21 finished saving model
12/30 06:45:25 step 78000 epoch 32 learning rate 0.102 step-time 0.810 loss 1.449
12/30 06:45:25 starting evaluation
12/30 06:50:25 test bleu=36.53 loss=81.53 penalty=1.000 ratio=1.002
12/30 06:50:25 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 06:50:25 finished saving model
12/30 06:50:25 new best model
12/30 06:55:23   decaying learning rate to: 0.0969
12/30 07:17:47 step 80000 epoch 33 learning rate 0.0969 step-time 0.819 loss 1.338
12/30 07:17:47 starting evaluation
12/30 07:22:34 test bleu=36.63 loss=82.66 penalty=0.981 ratio=0.981
12/30 07:22:34 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 07:22:35 finished saving model
12/30 07:22:35 new best model
12/30 07:34:37   decaying learning rate to: 0.092
12/30 07:52:54 step 82000 epoch 34 learning rate 0.092 step-time 0.907 loss 1.215
12/30 07:52:54 starting evaluation
12/30 07:58:20 test bleu=35.87 loss=85.22 penalty=1.000 ratio=1.027
12/30 07:58:20 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 07:58:20 finished saving model
12/30 08:18:07   decaying learning rate to: 0.0874
12/30 08:29:43 step 84000 epoch 35 learning rate 0.0874 step-time 0.939 loss 1.110
12/30 08:29:43 starting evaluation
12/30 08:35:02 test bleu=36.01 loss=86.61 penalty=1.000 ratio=1.023
12/30 08:35:02 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 08:35:02 finished saving model
12/30 09:00:58   decaying learning rate to: 0.083
12/30 09:04:59 step 86000 epoch 36 learning rate 0.083 step-time 0.896 loss 1.044
12/30 09:04:59 starting evaluation
12/30 09:09:59 test bleu=35.93 loss=88.86 penalty=1.000 ratio=1.029
12/30 09:09:59 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 09:10:00 finished saving model
12/30 09:37:12 step 88000 epoch 36 learning rate 0.083 step-time 0.814 loss 0.927
12/30 09:37:12 starting evaluation
12/30 09:42:11 test bleu=36.33 loss=89.98 penalty=1.000 ratio=1.021
12/30 09:42:11 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 09:42:12 finished saving model
12/30 09:44:24   decaying learning rate to: 0.0789
12/30 10:09:23 step 90000 epoch 37 learning rate 0.0789 step-time 0.813 loss 0.820
12/30 10:09:23 starting evaluation
12/30 10:14:23 test bleu=35.46 loss=91.29 penalty=1.000 ratio=1.052
12/30 10:14:23 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 10:14:23 finished saving model
12/30 10:22:49   decaying learning rate to: 0.0749
12/30 10:41:41 step 92000 epoch 38 learning rate 0.0749 step-time 0.817 loss 0.769
12/30 10:41:41 starting evaluation
12/30 10:46:38 test bleu=37.08 loss=92.39 penalty=1.000 ratio=1.007
12/30 10:46:38 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 10:46:39 finished saving model
12/30 10:46:39 new best model
12/30 11:01:13   decaying learning rate to: 0.0712
12/30 11:13:57 step 94000 epoch 39 learning rate 0.0712 step-time 0.817 loss 0.717
12/30 11:13:57 starting evaluation
12/30 11:18:46 test bleu=37.36 loss=94.01 penalty=0.999 ratio=0.999
12/30 11:18:46 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 11:18:46 finished saving model
12/30 11:18:46 new best model
12/30 11:39:27   decaying learning rate to: 0.0676
12/30 11:46:06 step 96000 epoch 40 learning rate 0.0676 step-time 0.818 loss 0.675
12/30 11:46:06 starting evaluation
12/30 11:51:09 test bleu=37.29 loss=95.35 penalty=1.000 ratio=1.001
12/30 11:51:09 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 11:51:09 finished saving model
12/30 12:17:56   decaying learning rate to: 0.0643
12/30 12:18:30 step 98000 epoch 41 learning rate 0.0643 step-time 0.818 loss 0.637
12/30 12:18:30 starting evaluation
12/30 12:23:28 test bleu=37.22 loss=95.92 penalty=1.000 ratio=1.009
12/30 12:23:28 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 12:23:28 finished saving model
12/30 12:50:45 step 100000 epoch 41 learning rate 0.0643 step-time 0.816 loss 0.565
12/30 12:50:45 starting evaluation
12/30 12:55:43 test bleu=37.24 loss=96.32 penalty=1.000 ratio=1.008
12/30 12:55:43 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 12:55:44 finished saving model
12/30 13:01:18   decaying learning rate to: 0.061
12/30 13:23:05 step 102000 epoch 42 learning rate 0.061 step-time 0.818 loss 0.533
12/30 13:23:05 starting evaluation
12/30 13:28:06 test bleu=36.59 loss=97.71 penalty=1.000 ratio=1.023
12/30 13:28:06 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 13:28:07 finished saving model
12/30 13:39:38   decaying learning rate to: 0.058
12/30 13:55:22 step 104000 epoch 43 learning rate 0.058 step-time 0.816 loss 0.500
12/30 13:55:22 starting evaluation
12/30 14:00:21 test bleu=37.04 loss=98.40 penalty=1.000 ratio=1.017
12/30 14:00:21 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 14:00:21 finished saving model
12/30 14:18:08   decaying learning rate to: 0.0551
12/30 14:27:34 step 106000 epoch 44 learning rate 0.0551 step-time 0.814 loss 0.481
12/30 14:27:34 starting evaluation
12/30 14:32:31 test bleu=37.06 loss=98.82 penalty=1.000 ratio=1.013
12/30 14:32:31 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 14:32:31 finished saving model
12/30 14:56:28   decaying learning rate to: 0.0523
12/30 14:59:43 step 108000 epoch 45 learning rate 0.0523 step-time 0.814 loss 0.462
12/30 14:59:43 starting evaluation
12/30 15:04:42 test bleu=36.58 loss=99.47 penalty=1.000 ratio=1.031
12/30 15:04:42 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 15:04:42 finished saving model
12/30 15:31:56 step 110000 epoch 45 learning rate 0.0523 step-time 0.815 loss 0.429
12/30 15:31:56 starting evaluation
12/30 15:36:47 test bleu=36.63 loss=100.40 penalty=1.000 ratio=1.028
12/30 15:36:47 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 15:36:47 finished saving model
12/30 15:39:40   decaying learning rate to: 0.0497
12/30 16:03:57 step 112000 epoch 46 learning rate 0.0497 step-time 0.813 loss 0.390
12/30 16:03:57 starting evaluation
12/30 16:09:00 test bleu=37.65 loss=100.10 penalty=1.000 ratio=1.003
12/30 16:09:00 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 16:09:01 finished saving model
12/30 16:09:01 new best model
12/30 16:17:51   decaying learning rate to: 0.0472
12/30 16:36:15 step 114000 epoch 47 learning rate 0.0472 step-time 0.815 loss 0.378
12/30 16:36:15 starting evaluation
12/30 16:41:13 test bleu=37.68 loss=100.60 penalty=1.000 ratio=1.003
12/30 16:41:13 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 16:41:13 finished saving model
12/30 16:41:13 new best model
12/30 16:56:04   decaying learning rate to: 0.0449
12/30 17:08:21 step 116000 epoch 48 learning rate 0.0449 step-time 0.812 loss 0.354
12/30 17:08:21 starting evaluation
12/30 17:13:20 test bleu=37.61 loss=101.42 penalty=1.000 ratio=1.011
12/30 17:13:20 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 17:13:20 finished saving model
12/30 17:34:28   decaying learning rate to: 0.0426
12/30 17:40:33 step 118000 epoch 49 learning rate 0.0426 step-time 0.815 loss 0.347
12/30 17:40:33 starting evaluation
12/30 17:45:34 test bleu=37.20 loss=100.88 penalty=1.000 ratio=1.018
12/30 17:45:34 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 17:45:34 finished saving model
12/30 18:12:48 step 120000 epoch 50 learning rate 0.0426 step-time 0.815 loss 0.333
12/30 18:12:48 starting evaluation
12/30 18:17:46 test bleu=36.92 loss=101.45 penalty=1.000 ratio=1.030
12/30 18:17:46 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 18:17:46 finished saving model
12/30 18:17:47   decaying learning rate to: 0.0405
12/30 18:45:01 step 122000 epoch 50 learning rate 0.0405 step-time 0.816 loss 0.288
12/30 18:45:01 starting evaluation
12/30 18:50:00 test bleu=37.85 loss=101.74 penalty=1.000 ratio=1.006
12/30 18:50:00 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 18:50:00 finished saving model
12/30 18:50:00 new best model
12/30 18:56:05   decaying learning rate to: 0.0385
12/30 19:17:17 step 124000 epoch 51 learning rate 0.0385 step-time 0.816 loss 0.274
12/30 19:17:17 starting evaluation
12/30 19:22:17 test bleu=37.33 loss=101.76 penalty=1.000 ratio=1.022
12/30 19:22:17 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 19:22:17 finished saving model
12/30 19:34:39   decaying learning rate to: 0.0365
12/30 19:49:31 step 126000 epoch 52 learning rate 0.0365 step-time 0.815 loss 0.268
12/30 19:49:31 starting evaluation
12/30 19:54:20 test bleu=37.87 loss=101.40 penalty=1.000 ratio=1.007
12/30 19:54:20 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 19:54:20 finished saving model
12/30 19:54:20 new best model
12/30 20:12:35   decaying learning rate to: 0.0347
12/30 20:21:25 step 128000 epoch 53 learning rate 0.0347 step-time 0.811 loss 0.251
12/30 20:21:25 starting evaluation
12/30 20:26:26 test bleu=37.72 loss=101.43 penalty=1.000 ratio=1.013
12/30 20:26:26 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 20:26:26 finished saving model
12/30 20:50:43   decaying learning rate to: 0.033
12/30 20:53:40 step 130000 epoch 54 learning rate 0.033 step-time 0.815 loss 0.247
12/30 20:53:40 starting evaluation
12/30 20:58:40 test bleu=37.64 loss=101.72 penalty=1.000 ratio=1.014
12/30 20:58:40 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 20:58:40 finished saving model
12/30 21:25:57 step 132000 epoch 54 learning rate 0.033 step-time 0.816 loss 0.227
12/30 21:25:57 starting evaluation
12/30 21:30:57 test bleu=37.36 loss=101.13 penalty=1.000 ratio=1.024
12/30 21:30:57 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 21:30:57 finished saving model
12/30 21:34:15   decaying learning rate to: 0.0313
12/30 21:58:13 step 134000 epoch 55 learning rate 0.0313 step-time 0.816 loss 0.211
12/30 21:58:13 starting evaluation
12/30 22:03:10 test bleu=38.21 loss=101.72 penalty=1.000 ratio=1.001
12/30 22:03:10 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 22:03:11 finished saving model
12/30 22:03:11 new best model
12/30 22:12:33   decaying learning rate to: 0.0298
12/30 22:30:26 step 136000 epoch 56 learning rate 0.0298 step-time 0.816 loss 0.203
12/30 22:30:26 starting evaluation
12/30 22:35:24 test bleu=38.02 loss=101.70 penalty=1.000 ratio=1.007
12/30 22:35:24 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 22:35:24 finished saving model
12/30 22:51:05   decaying learning rate to: 0.0283
12/30 23:02:36 step 138000 epoch 57 learning rate 0.0283 step-time 0.814 loss 0.192
12/30 23:02:36 starting evaluation
12/30 23:07:36 test bleu=38.16 loss=102.04 penalty=1.000 ratio=1.002
12/30 23:07:36 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 23:07:36 finished saving model
12/30 23:29:22   decaying learning rate to: 0.0269
12/30 23:34:51 step 140000 epoch 58 learning rate 0.0269 step-time 0.815 loss 0.186
12/30 23:34:51 starting evaluation
12/30 23:39:49 test bleu=37.72 loss=101.75 penalty=1.000 ratio=1.017
12/30 23:39:49 saving model to models/8_fold_hybrid_pnl/checkpoints
12/30 23:39:49 finished saving model
12/31 00:07:01 step 142000 epoch 58 learning rate 0.0269 step-time 0.814 loss 0.184
12/31 00:07:01 starting evaluation
12/31 00:11:50 test bleu=37.25 loss=101.59 penalty=1.000 ratio=1.031
12/31 00:11:50 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 00:11:50 finished saving model
12/31 00:12:26   decaying learning rate to: 0.0255
12/31 00:38:11 step 144000 epoch 59 learning rate 0.0255 step-time 0.789 loss 0.162
12/31 00:38:11 starting evaluation
12/31 00:43:05 test bleu=37.90 loss=101.80 penalty=1.000 ratio=1.011
12/31 00:43:05 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 00:43:05 finished saving model
12/31 00:49:34   decaying learning rate to: 0.0242
12/31 01:09:33 step 146000 epoch 60 learning rate 0.0242 step-time 0.792 loss 0.163
12/31 01:09:33 starting evaluation
12/31 01:14:35 test bleu=38.11 loss=101.97 penalty=1.000 ratio=1.005
12/31 01:14:35 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 01:14:35 finished saving model
12/31 01:26:58   decaying learning rate to: 0.023
12/31 01:41:09 step 148000 epoch 61 learning rate 0.023 step-time 0.795 loss 0.154
12/31 01:41:09 starting evaluation
12/31 01:46:06 test bleu=37.41 loss=102.29 penalty=1.000 ratio=1.023
12/31 01:46:06 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 01:46:06 finished saving model
12/31 02:04:23   decaying learning rate to: 0.0219
12/31 02:12:40 step 150000 epoch 62 learning rate 0.0219 step-time 0.795 loss 0.154
12/31 02:12:40 starting evaluation
12/31 02:17:34 test bleu=38.32 loss=102.36 penalty=1.000 ratio=1.001
12/31 02:17:34 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 02:17:34 finished saving model
12/31 02:17:34 new best model
12/31 02:42:02   decaying learning rate to: 0.0208
12/31 02:44:10 step 152000 epoch 63 learning rate 0.0208 step-time 0.796 loss 0.146
12/31 02:44:10 starting evaluation
12/31 02:49:07 test bleu=38.05 loss=102.70 penalty=1.000 ratio=1.009
12/31 02:49:07 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 02:49:07 finished saving model
12/31 03:15:37 step 154000 epoch 63 learning rate 0.0208 step-time 0.793 loss 0.138
12/31 03:15:37 starting evaluation
12/31 03:20:33 test bleu=37.47 loss=102.43 penalty=1.000 ratio=1.022
12/31 03:20:33 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 03:20:34 finished saving model
12/31 03:24:22   decaying learning rate to: 0.0197
12/31 03:47:10 step 156000 epoch 64 learning rate 0.0197 step-time 0.796 loss 0.130
12/31 03:47:10 starting evaluation
12/31 03:52:07 test bleu=38.09 loss=102.61 penalty=1.000 ratio=1.007
12/31 03:52:07 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 03:52:07 finished saving model
12/31 04:02:05   decaying learning rate to: 0.0188
12/31 04:18:47 step 158000 epoch 65 learning rate 0.0188 step-time 0.798 loss 0.132
12/31 04:18:47 starting evaluation
12/31 04:23:44 test bleu=37.34 loss=102.65 penalty=1.000 ratio=1.027
12/31 04:23:44 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 04:23:44 finished saving model
12/31 04:39:38   decaying learning rate to: 0.0178
12/31 04:50:23 step 160000 epoch 66 learning rate 0.0178 step-time 0.798 loss 0.125
12/31 04:50:23 starting evaluation
12/31 04:55:09 test bleu=37.81 loss=103.02 penalty=1.000 ratio=1.014
12/31 04:55:09 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 04:55:10 finished saving model
12/31 05:16:57   decaying learning rate to: 0.0169
12/31 05:21:45 step 162000 epoch 67 learning rate 0.0169 step-time 0.796 loss 0.123
12/31 05:21:45 starting evaluation
12/31 05:26:36 test bleu=37.96 loss=103.02 penalty=1.000 ratio=1.012
12/31 05:26:36 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 05:26:36 finished saving model
12/31 05:53:04 step 164000 epoch 67 learning rate 0.0169 step-time 0.792 loss 0.121
12/31 05:53:04 starting evaluation
12/31 05:58:08 test bleu=37.96 loss=102.89 penalty=1.000 ratio=1.009
12/31 05:58:08 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 05:58:08 finished saving model
12/31 05:59:14   decaying learning rate to: 0.0161
12/31 06:24:48 step 166000 epoch 68 learning rate 0.0161 step-time 0.798 loss 0.111
12/31 06:24:48 starting evaluation
12/31 06:29:44 test bleu=38.18 loss=103.23 penalty=1.000 ratio=1.004
12/31 06:29:44 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 06:29:44 finished saving model
12/31 06:36:48   decaying learning rate to: 0.0153
12/31 06:56:32 step 168000 epoch 69 learning rate 0.0153 step-time 0.802 loss 0.109
12/31 06:56:32 starting evaluation
12/31 07:01:27 test bleu=38.45 loss=103.25 penalty=0.998 ratio=0.998
12/31 07:01:27 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 07:01:28 finished saving model
12/31 07:01:28 new best model
12/31 07:14:27   decaying learning rate to: 0.0145
12/31 07:28:11 step 170000 epoch 70 learning rate 0.0145 step-time 0.800 loss 0.109
12/31 07:28:11 starting evaluation
12/31 07:33:08 test bleu=37.81 loss=103.64 penalty=1.000 ratio=1.015
12/31 07:33:08 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 07:33:08 finished saving model
12/31 07:52:14   decaying learning rate to: 0.0138
12/31 07:59:49 step 172000 epoch 71 learning rate 0.0138 step-time 0.799 loss 0.109
12/31 07:59:49 starting evaluation
12/31 08:04:45 test bleu=37.48 loss=104.04 penalty=1.000 ratio=1.021
12/31 08:04:45 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 08:04:45 finished saving model
12/31 08:29:52   decaying learning rate to: 0.0131
12/31 08:31:27 step 174000 epoch 72 learning rate 0.0131 step-time 0.799 loss 0.105
12/31 08:31:27 starting evaluation
12/31 08:36:22 test bleu=38.22 loss=103.95 penalty=1.000 ratio=1.005
12/31 08:36:22 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 08:36:22 finished saving model
12/31 09:03:08 step 176000 epoch 72 learning rate 0.0131 step-time 0.801 loss 0.099
12/31 09:03:08 starting evaluation
12/31 09:08:05 test bleu=37.60 loss=104.03 penalty=1.000 ratio=1.022
12/31 09:08:05 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 09:08:05 finished saving model
12/31 09:12:28   decaying learning rate to: 0.0124
12/31 09:34:43 step 178000 epoch 73 learning rate 0.0124 step-time 0.797 loss 0.095
12/31 09:34:43 starting evaluation
12/31 09:39:37 test bleu=37.73 loss=103.89 penalty=1.000 ratio=1.017
12/31 09:39:37 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 09:39:37 finished saving model
12/31 09:50:05   decaying learning rate to: 0.0118
12/31 10:06:16 step 180000 epoch 74 learning rate 0.0118 step-time 0.798 loss 0.095
12/31 10:06:16 starting evaluation
12/31 10:11:03 test bleu=38.03 loss=104.64 penalty=1.000 ratio=1.011
12/31 10:11:03 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 10:11:03 finished saving model
12/31 10:27:14   decaying learning rate to: 0.0112
12/31 10:37:31 step 182000 epoch 75 learning rate 0.0112 step-time 0.792 loss 0.093
12/31 10:37:31 starting evaluation
12/31 10:42:26 test bleu=37.64 loss=104.39 penalty=1.000 ratio=1.020
12/31 10:42:26 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 10:42:27 finished saving model
12/31 11:04:31   decaying learning rate to: 0.0107
12/31 11:08:58 step 184000 epoch 76 learning rate 0.0107 step-time 0.794 loss 0.094
12/31 11:08:58 starting evaluation
12/31 11:13:58 test bleu=37.64 loss=104.59 penalty=1.000 ratio=1.018
12/31 11:13:58 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 11:13:58 finished saving model
12/31 11:40:32 step 186000 epoch 76 learning rate 0.0107 step-time 0.795 loss 0.089
12/31 11:40:32 starting evaluation
12/31 11:45:30 test bleu=37.73 loss=104.46 penalty=1.000 ratio=1.017
12/31 11:45:30 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 11:45:31 finished saving model
12/31 11:47:09   decaying learning rate to: 0.0101
12/31 12:12:09 step 188000 epoch 77 learning rate 0.0101 step-time 0.797 loss 0.087
12/31 12:12:09 starting evaluation
12/31 12:17:05 test bleu=37.91 loss=104.83 penalty=1.000 ratio=1.012
12/31 12:17:05 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 12:17:05 finished saving model
12/31 12:24:36   decaying learning rate to: 0.00963
12/31 12:43:43 step 190000 epoch 78 learning rate 0.00963 step-time 0.797 loss 0.084
12/31 12:43:43 starting evaluation
12/31 12:48:38 test bleu=38.25 loss=104.97 penalty=1.000 ratio=1.004
12/31 12:48:38 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 12:48:38 finished saving model
12/31 13:02:14   decaying learning rate to: 0.00915
12/31 13:15:15 step 192000 epoch 79 learning rate 0.00915 step-time 0.796 loss 0.085
12/31 13:15:15 starting evaluation
12/31 13:20:13 test bleu=37.76 loss=104.98 penalty=1.000 ratio=1.016
12/31 13:20:13 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 13:20:13 finished saving model
12/31 13:39:51   decaying learning rate to: 0.00869
12/31 13:46:50 step 194000 epoch 80 learning rate 0.00869 step-time 0.796 loss 0.083
12/31 13:46:50 starting evaluation
12/31 13:51:46 test bleu=37.61 loss=105.07 penalty=1.000 ratio=1.023
12/31 13:51:46 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 13:51:47 finished saving model
12/31 14:16:49   decaying learning rate to: 0.00826
12/31 14:17:51 step 196000 epoch 81 learning rate 0.00826 step-time 0.780 loss 0.085
12/31 14:17:51 starting evaluation
12/31 14:22:45 test bleu=37.65 loss=105.22 penalty=1.000 ratio=1.018
12/31 14:22:45 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 14:22:45 finished saving model
12/31 14:48:54 step 198000 epoch 81 learning rate 0.00826 step-time 0.783 loss 0.078
12/31 14:48:54 starting evaluation
12/31 14:53:42 test bleu=38.02 loss=105.69 penalty=1.000 ratio=1.010
12/31 14:53:42 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 14:53:42 finished saving model
12/31 14:58:42   decaying learning rate to: 0.00784
12/31 15:19:46 step 200000 epoch 82 learning rate 0.00784 step-time 0.780 loss 0.076
12/31 15:19:46 starting evaluation
12/31 15:24:29 test bleu=37.91 loss=105.46 penalty=1.000 ratio=1.012
12/31 15:24:29 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 15:24:29 finished saving model
12/31 15:35:10   decaying learning rate to: 0.00745
12/31 15:50:26 step 202000 epoch 83 learning rate 0.00745 step-time 0.777 loss 0.077
12/31 15:50:26 starting evaluation
12/31 15:55:14 test bleu=38.11 loss=105.53 penalty=1.000 ratio=1.006
12/31 15:55:14 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 15:55:14 finished saving model
12/31 16:11:41   decaying learning rate to: 0.00708
12/31 16:21:14 step 204000 epoch 84 learning rate 0.00708 step-time 0.778 loss 0.077
12/31 16:21:14 starting evaluation
12/31 16:26:15 test bleu=38.05 loss=105.71 penalty=1.000 ratio=1.010
12/31 16:26:15 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 16:26:15 finished saving model
12/31 16:48:25   decaying learning rate to: 0.00673
12/31 16:52:22 step 206000 epoch 85 learning rate 0.00673 step-time 0.781 loss 0.077
12/31 16:52:22 starting evaluation
12/31 16:57:16 test bleu=37.98 loss=105.83 penalty=1.000 ratio=1.011
12/31 16:57:16 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 16:57:16 finished saving model
12/31 17:23:19 step 208000 epoch 85 learning rate 0.00673 step-time 0.780 loss 0.074
12/31 17:23:19 starting evaluation
12/31 17:28:11 test bleu=38.37 loss=105.85 penalty=0.999 ratio=0.999
12/31 17:28:11 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 17:28:11 finished saving model
12/31 17:30:19   decaying learning rate to: 0.00639
12/31 17:54:15 step 210000 epoch 86 learning rate 0.00639 step-time 0.780 loss 0.072
12/31 17:54:15 starting evaluation
12/31 17:59:08 test bleu=38.16 loss=106.15 penalty=1.000 ratio=1.007
12/31 17:59:08 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 17:59:08 finished saving model
12/31 18:07:06   decaying learning rate to: 0.00607
12/31 18:25:13 step 212000 epoch 87 learning rate 0.00607 step-time 0.781 loss 0.071
12/31 18:25:13 starting evaluation
12/31 18:30:06 test bleu=37.90 loss=106.21 penalty=1.000 ratio=1.014
12/31 18:30:06 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 18:30:06 finished saving model
12/31 18:43:55   decaying learning rate to: 0.00577
12/31 18:56:04 step 214000 epoch 88 learning rate 0.00577 step-time 0.777 loss 0.071
12/31 18:56:04 starting evaluation
12/31 19:00:57 test bleu=37.58 loss=106.16 penalty=1.000 ratio=1.023
12/31 19:00:57 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 19:00:57 finished saving model
12/31 19:20:41   decaying learning rate to: 0.00548
12/31 19:26:57 step 216000 epoch 89 learning rate 0.00548 step-time 0.778 loss 0.072
12/31 19:26:57 starting evaluation
12/31 19:31:49 test bleu=37.98 loss=106.30 penalty=1.000 ratio=1.012
12/31 19:31:49 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 19:31:49 finished saving model
12/31 19:57:15   decaying learning rate to: 0.0052
12/31 19:57:44 step 218000 epoch 90 learning rate 0.0052 step-time 0.776 loss 0.072
12/31 19:57:44 starting evaluation
12/31 20:02:38 test bleu=37.47 loss=106.29 penalty=1.000 ratio=1.025
12/31 20:02:38 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 20:02:38 finished saving model
12/31 20:27:56 step 220000 epoch 90 learning rate 0.0052 step-time 0.757 loss 0.067
12/31 20:27:56 starting evaluation
12/31 20:31:29 test bleu=37.83 loss=106.72 penalty=1.000 ratio=1.014
12/31 20:31:29 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 20:31:29 finished saving model
12/31 20:36:26   decaying learning rate to: 0.00494
12/31 20:55:18 step 222000 epoch 91 learning rate 0.00494 step-time 0.712 loss 0.068
12/31 20:55:18 starting evaluation
12/31 20:58:53 test bleu=37.77 loss=106.72 penalty=1.000 ratio=1.017
12/31 20:58:53 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 20:58:53 finished saving model
12/31 21:09:12   decaying learning rate to: 0.0047
12/31 21:22:52 step 224000 epoch 92 learning rate 0.0047 step-time 0.718 loss 0.068
12/31 21:22:52 starting evaluation
12/31 21:26:42 test bleu=38.05 loss=106.77 penalty=1.000 ratio=1.012
12/31 21:26:42 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 21:26:42 finished saving model
12/31 21:44:01   decaying learning rate to: 0.00446
12/31 21:53:05 step 226000 epoch 93 learning rate 0.00446 step-time 0.789 loss 0.068
12/31 21:53:05 starting evaluation
12/31 21:56:59 test bleu=37.78 loss=106.84 penalty=1.000 ratio=1.017
12/31 21:56:59 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 21:56:59 finished saving model
12/31 22:20:08   decaying learning rate to: 0.00424
12/31 22:23:24 step 228000 epoch 94 learning rate 0.00424 step-time 0.790 loss 0.067
12/31 22:23:24 starting evaluation
12/31 22:27:21 test bleu=37.81 loss=106.79 penalty=1.000 ratio=1.015
12/31 22:27:21 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 22:27:22 finished saving model
12/31 22:53:41 step 230000 epoch 94 learning rate 0.00424 step-time 0.787 loss 0.066
12/31 22:53:41 starting evaluation
12/31 22:57:39 test bleu=37.58 loss=106.93 penalty=1.000 ratio=1.021
12/31 22:57:39 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 22:57:39 finished saving model
12/31 23:00:19   decaying learning rate to: 0.00403
12/31 23:23:59 step 232000 epoch 95 learning rate 0.00403 step-time 0.787 loss 0.064
12/31 23:23:59 starting evaluation
12/31 23:27:55 test bleu=37.91 loss=107.12 penalty=1.000 ratio=1.012
12/31 23:27:55 saving model to models/8_fold_hybrid_pnl/checkpoints
12/31 23:27:55 finished saving model
12/31 23:37:08   decaying learning rate to: 0.00383
12/31 23:56:06 step 234000 epoch 96 learning rate 0.00383 step-time 0.842 loss 0.065
12/31 23:56:06 starting evaluation
01/01 00:00:30 test bleu=38.02 loss=107.13 penalty=1.000 ratio=1.011
01/01 00:00:30 saving model to models/8_fold_hybrid_pnl/checkpoints
01/01 00:00:30 finished saving model
01/01 00:16:07   decaying learning rate to: 0.00363
01/01 00:28:45 step 236000 epoch 97 learning rate 0.00363 step-time 0.845 loss 0.064
01/01 00:28:45 starting evaluation
01/01 00:33:11 test bleu=37.80 loss=107.15 penalty=1.000 ratio=1.015
01/01 00:33:11 saving model to models/8_fold_hybrid_pnl/checkpoints
01/01 00:33:11 finished saving model
01/01 00:55:06   decaying learning rate to: 0.00345
01/01 01:01:34 step 238000 epoch 98 learning rate 0.00345 step-time 0.848 loss 0.064
01/01 01:01:34 starting evaluation
01/01 01:05:58 test bleu=37.83 loss=107.20 penalty=1.000 ratio=1.015
01/01 01:05:58 saving model to models/8_fold_hybrid_pnl/checkpoints
01/01 01:05:58 finished saving model
01/01 01:34:19 step 240000 epoch 99 learning rate 0.00345 step-time 0.847 loss 0.065
01/01 01:34:19 starting evaluation
01/01 01:38:43 test bleu=37.83 loss=107.29 penalty=1.000 ratio=1.015
01/01 01:38:43 saving model to models/8_fold_hybrid_pnl/checkpoints
01/01 01:38:44 finished saving model
01/01 01:38:46   decaying learning rate to: 0.00328
01/01 02:06:57 step 242000 epoch 99 learning rate 0.00328 step-time 0.844 loss 0.062
01/01 02:06:57 starting evaluation
01/01 02:11:22 test bleu=37.88 loss=107.38 penalty=1.000 ratio=1.014
01/01 02:11:22 saving model to models/8_fold_hybrid_pnl/checkpoints
01/01 02:11:22 finished saving model
01/01 02:17:44   decaying learning rate to: 0.00312
01/01 02:39:41 step 244000 epoch 100 learning rate 0.00312 step-time 0.846 loss 0.062
01/01 02:39:41 starting evaluation
01/01 02:44:04 test bleu=37.81 loss=107.35 penalty=1.000 ratio=1.017
01/01 02:44:04 saving model to models/8_fold_hybrid_pnl/checkpoints
01/01 02:44:04 finished saving model
01/01 02:56:28 finished training
01/01 02:56:28 exiting...
01/01 02:56:28 saving model to models/8_fold_hybrid_pnl/checkpoints
01/01 02:56:29 finished saving model
