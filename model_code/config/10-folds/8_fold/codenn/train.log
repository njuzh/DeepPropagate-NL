nohup: ignoring input
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:107: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:30: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

12/29 04:28:41 label: default
12/29 04:28:41 description:
  default configuration
  next line of description
  last line
12/29 04:28:41 /root/icpc/icpc/translate/__main__.py config/10-folds/8_fold/codenn/config.yaml --train -v
12/29 04:28:41 commit hash 74e0554cb3eb5df835cef993ad570ff8de651f71
12/29 04:28:41 tensorflow version: 1.14.0
12/29 04:28:41 program arguments
12/29 04:28:41   aggregation_method   'sum'
12/29 04:28:41   align_encoder_id     0
12/29 04:28:41   allow_growth         True
12/29 04:28:41   attention_type       'global'
12/29 04:28:41   attn_filter_length   0
12/29 04:28:41   attn_filters         0
12/29 04:28:41   attn_prev_word       False
12/29 04:28:41   attn_size            128
12/29 04:28:41   attn_temperature     1.0
12/29 04:28:41   attn_window_size     0
12/29 04:28:41   average              False
12/29 04:28:41   baseline_activation  None
12/29 04:28:41   baseline_learning_rate 0.001
12/29 04:28:41   baseline_optimizer   'adam'
12/29 04:28:41   baseline_steps       0
12/29 04:28:41   batch_mode           'standard'
12/29 04:28:41   batch_size           64
12/29 04:28:41   beam_size            5
12/29 04:28:41   bidir                True
12/29 04:28:41   bidir_projection     False
12/29 04:28:41   binary               False
12/29 04:28:41   cell_size            256
12/29 04:28:41   cell_type            'GRU'
12/29 04:28:41   character_level      False
12/29 04:28:41   checkpoints          []
12/29 04:28:41   conditional_rnn      False
12/29 04:28:41   config               'config/10-folds/8_fold/codenn/config.yaml'
12/29 04:28:41   convolutions         None
12/29 04:28:41   data_dir             'data/gooddata/8_fold'
12/29 04:28:41   debug                False
12/29 04:28:41   decay_after_n_epoch  1
12/29 04:28:41   decay_every_n_epoch  1
12/29 04:28:41   decay_if_no_progress None
12/29 04:28:41   decoders             [{'max_len': 40, 'name': 'nl'}]
12/29 04:28:41   description          'default configuration\nnext line of description\nlast line\n'
12/29 04:28:41   dev_prefix           'test'
12/29 04:28:41   early_stopping       True
12/29 04:28:41   embedding_dropout    0.0
12/29 04:28:41   embedding_initializer None
12/29 04:28:41   embedding_size       256
12/29 04:28:41   embedding_weight_scale None
12/29 04:28:41   embeddings_on_cpu    True
12/29 04:28:41   encoders             [{'attention_type': 'global', 'max_len': 200, 'name': 'code'}]
12/29 04:28:41   ensemble             False
12/29 04:28:41   eval_burn_in         0
12/29 04:28:41   feed_previous        0.0
12/29 04:28:41   final_state          'last'
12/29 04:28:41   freeze_variables     []
12/29 04:28:41   generate_first       True
12/29 04:28:41   gpu_id               1
12/29 04:28:41   highway_layers       0
12/29 04:28:41   initial_state_dropout 0.0
12/29 04:28:41   initializer          None
12/29 04:28:41   input_layer_dropout  0.0
12/29 04:28:41   input_layers         None
12/29 04:28:41   keep_best            5
12/29 04:28:41   keep_every_n_hours   0
12/29 04:28:41   label                'default'
12/29 04:28:41   layer_norm           False
12/29 04:28:41   layers               1
12/29 04:28:41   learning_rate        0.5
12/29 04:28:41   learning_rate_decay_factor 0.95
12/29 04:28:41   len_normalization    1.0
12/29 04:28:41   log_file             'log.txt'
12/29 04:28:41   loss_function        'xent'
12/29 04:28:41   max_dev_size         0
12/29 04:28:41   max_epochs           100
12/29 04:28:41   max_gradient_norm    5.0
12/29 04:28:41   max_len              50
12/29 04:28:41   max_steps            600000
12/29 04:28:41   max_test_size        0
12/29 04:28:41   max_to_keep          1
12/29 04:28:41   max_train_size       0
12/29 04:28:41   maxout_stride        None
12/29 04:28:41   mem_fraction         1.0
12/29 04:28:41   min_learning_rate    1e-06
12/29 04:28:41   model_dir            'models/8_fold_codenn'
12/29 04:28:41   moving_average       None
12/29 04:28:41   no_gpu               False
12/29 04:28:41   optimizer            'sgd'
12/29 04:28:41   orthogonal_init      False
12/29 04:28:41   output               None
12/29 04:28:41   output_dropout       0.0
12/29 04:28:41   parallel_iterations  16
12/29 04:28:41   pervasive_dropout    False
12/29 04:28:41   pooling_avg          True
12/29 04:28:41   post_process_script  None
12/29 04:28:41   pred_deep_layer      False
12/29 04:28:41   pred_edits           False
12/29 04:28:41   pred_embed_proj      True
12/29 04:28:41   pred_maxout_layer    True
12/29 04:28:41   purge                False
12/29 04:28:41   raw_output           False
12/29 04:28:41   read_ahead           1
12/29 04:28:41   reconstruction_attn_weight 0.05
12/29 04:28:41   reconstruction_decoders False
12/29 04:28:41   reconstruction_weight 1.0
12/29 04:28:41   reinforce_after_n_epoch None
12/29 04:28:41   remove_unk           False
12/29 04:28:41   reverse              False
12/29 04:28:41   reverse_input        True
12/29 04:28:41   reward_function      'sentence_bleu'
12/29 04:28:41   rnn_feed_attn        True
12/29 04:28:41   rnn_input_dropout    0.0
12/29 04:28:41   rnn_output_dropout   0.0
12/29 04:28:41   rnn_state_dropout    0.0
12/29 04:28:41   save                 False
12/29 04:28:41   score_function       'corpus_bleu'
12/29 04:28:41   score_functions      ['bleu', 'loss']
12/29 04:28:41   script_dir           'scripts'
12/29 04:28:41   sgd_after_n_epoch    None
12/29 04:28:41   sgd_learning_rate    1.0
12/29 04:28:41   shuffle              True
12/29 04:28:41   softmax_temperature  1.0
12/29 04:28:41   steps_per_checkpoint 2000
12/29 04:28:41   steps_per_eval       2000
12/29 04:28:41   swap_memory          True
12/29 04:28:41   tie_embeddings       False
12/29 04:28:41   time_pooling         None
12/29 04:28:41   train                True
12/29 04:28:41   train_initial_states True
12/29 04:28:41   train_prefix         'train'
12/29 04:28:41   truncate_lines       True
12/29 04:28:41   update_first         False
12/29 04:28:41   use_baseline         False
12/29 04:28:41   use_dropout          False
12/29 04:28:41   use_lstm_full_state  False
12/29 04:28:41   use_previous_word    True
12/29 04:28:41   verbose              True
12/29 04:28:41   vocab_prefix         'vocab'
12/29 04:28:41   weight_scale         None
12/29 04:28:41   word_dropout         0.0
12/29 04:28:41 python random seed: 9100251511556400381
12/29 04:28:41 tf random seed:     2081757524355935794
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:203: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

12/29 04:28:41 creating model
12/29 04:28:41 using device: /gpu:1
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:230: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

12/29 04:28:41 copying vocab to models/8_fold_codenn/data/vocab.code
12/29 04:28:41 copying vocab to models/8_fold_codenn/data/vocab.nl
12/29 04:28:41 reading vocabularies
12/29 04:28:41 creating model
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:60: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:111: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:33: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f860a6da6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f860a6da6d8>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:226: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f860a6da978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f860a6da978>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f860a6cff60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f860a6cff60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:838: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86914bc588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86914bc588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8691489fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8691489fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:432: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:435: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8691336ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8691336ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86912eaf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86912eaf98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8691346d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8691346d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86916c8240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86916c8240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86916c8240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86916c8240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:919: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f8690ff5160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f8690ff5160>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:131: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:223: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f8637b07a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f8637b07a90>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8637aa2c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8637aa2c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8637a51be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8637a51be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8637a51940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8637a51940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8637a51940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8637a51940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8637a51940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8637a51940>>: AssertionError: Bad argument number for Name: 3, expecting 4
12/29 04:28:45 model parameters (30)
12/29 04:28:45   baseline_step:0 ()
12/29 04:28:45   decoder_nl/attention_code/U_a/kernel:0 (512, 128)
12/29 04:28:45   decoder_nl/attention_code/W_a/bias:0 (128,)
12/29 04:28:45   decoder_nl/attention_code/W_a/kernel:0 (256, 128)
12/29 04:28:45   decoder_nl/attention_code/v_a:0 (128,)
12/29 04:28:45   decoder_nl/code/initial_state_projection/bias:0 (256,)
12/29 04:28:45   decoder_nl/code/initial_state_projection/kernel:0 (256, 256)
12/29 04:28:45   decoder_nl/gru_cell/candidate/bias:0 (256,)
12/29 04:28:45   decoder_nl/gru_cell/candidate/kernel:0 (1024, 256)
12/29 04:28:45   decoder_nl/gru_cell/gates/bias:0 (512,)
12/29 04:28:45   decoder_nl/gru_cell/gates/kernel:0 (1024, 512)
12/29 04:28:45   decoder_nl/maxout/bias:0 (256,)
12/29 04:28:45   decoder_nl/maxout/kernel:0 (1024, 256)
12/29 04:28:45   decoder_nl/softmax0/kernel:0 (128, 256)
12/29 04:28:45   decoder_nl/softmax1/bias:0 (38032,)
12/29 04:28:45   decoder_nl/softmax1/kernel:0 (256, 38032)
12/29 04:28:45   embedding_code:0 (50000, 256)
12/29 04:28:45   embedding_nl:0 (38032, 256)
12/29 04:28:45   encoder_code/initial_state_bw:0 (256,)
12/29 04:28:45   encoder_code/initial_state_fw:0 (256,)
12/29 04:28:45   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/29 04:28:45   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/29 04:28:45   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/29 04:28:45   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/29 04:28:45   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/29 04:28:45   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/29 04:28:45   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/29 04:28:45   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/29 04:28:45   global_step:0 ()
12/29 04:28:45   learning_rate:0 ()
12/29 04:28:45 number of parameters: 34.35M
WARNING:tensorflow:From /root/icpc/icpc/translate/translation_model.py:666: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

12/29 04:28:46 global step: 0
12/29 04:28:46 baseline step: 0
12/29 04:28:46 reading training data
12/29 04:28:46 total line count: 156721
12/29 04:28:50   lines read: 100000
12/29 04:28:53 files: data/gooddata/8_fold/train.code data/gooddata/8_fold/train.nl
12/29 04:28:53 lines reads: 156721
12/29 04:28:53 reading development data
12/29 04:28:54 files: data/gooddata/8_fold/test.code data/gooddata/8_fold/test.nl
12/29 04:28:54 lines reads: 17413
12/29 04:28:54 starting training
12/29 04:53:13 step 2000 epoch 1 learning rate 0.5 step-time 0.727 loss 77.969
12/29 04:53:13 starting evaluation
12/29 04:57:12 test bleu=0.66 loss=64.43 penalty=1.000 ratio=2.378
12/29 04:57:12 saving model to models/8_fold_codenn/checkpoints
12/29 04:57:12 finished saving model
12/29 04:57:12 new best model
12/29 05:02:18   decaying learning rate to: 0.475
12/29 05:21:09 step 4000 epoch 2 learning rate 0.475 step-time 0.716 loss 58.809
12/29 05:21:09 starting evaluation
12/29 05:25:24 test bleu=4.25 loss=54.72 penalty=0.802 ratio=0.820
12/29 05:25:24 saving model to models/8_fold_codenn/checkpoints
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
12/29 05:25:24 finished saving model
12/29 05:25:24 new best model
12/29 05:35:54   decaying learning rate to: 0.451
12/29 05:49:19 step 6000 epoch 3 learning rate 0.451 step-time 0.716 loss 52.259
12/29 05:49:19 starting evaluation
12/29 05:53:39 test bleu=5.57 loss=50.80 penalty=1.000 ratio=1.476
12/29 05:53:39 saving model to models/8_fold_codenn/checkpoints
12/29 05:53:40 finished saving model
12/29 05:53:40 new best model
12/29 06:09:18   decaying learning rate to: 0.429
12/29 06:17:15 step 8000 epoch 4 learning rate 0.429 step-time 0.705 loss 47.606
12/29 06:17:15 starting evaluation
12/29 06:21:34 test bleu=10.40 loss=47.03 penalty=0.966 ratio=0.966
12/29 06:21:34 saving model to models/8_fold_codenn/checkpoints
12/29 06:21:34 finished saving model
12/29 06:21:34 new best model
12/29 06:42:58   decaying learning rate to: 0.407
12/29 06:45:26 step 10000 epoch 5 learning rate 0.407 step-time 0.714 loss 44.031
12/29 06:45:26 starting evaluation
12/29 06:49:45 test bleu=12.11 loss=45.15 penalty=0.953 ratio=0.954
12/29 06:49:45 saving model to models/8_fold_codenn/checkpoints
12/29 06:49:45 finished saving model
12/29 06:49:45 new best model
12/29 07:13:29 step 12000 epoch 5 learning rate 0.407 step-time 0.710 loss 40.806
12/29 07:13:29 starting evaluation
12/29 07:17:32 test bleu=13.14 loss=43.17 penalty=0.751 ratio=0.777
12/29 07:17:32 saving model to models/8_fold_codenn/checkpoints
12/29 07:17:32 finished saving model
12/29 07:17:32 new best model
12/29 07:20:30   decaying learning rate to: 0.387
12/29 07:41:12 step 14000 epoch 6 learning rate 0.387 step-time 0.708 loss 37.763
12/29 07:41:12 starting evaluation
12/29 07:45:10 test bleu=14.83 loss=41.93 penalty=0.782 ratio=0.803
12/29 07:45:10 saving model to models/8_fold_codenn/checkpoints
12/29 07:45:11 finished saving model
12/29 07:45:11 new best model
12/29 07:53:37   decaying learning rate to: 0.368
12/29 08:09:09 step 16000 epoch 7 learning rate 0.368 step-time 0.717 loss 35.372
12/29 08:09:09 starting evaluation
12/29 08:13:06 test bleu=17.01 loss=40.75 penalty=0.780 ratio=0.801
12/29 08:13:06 saving model to models/8_fold_codenn/checkpoints
12/29 08:13:07 finished saving model
12/29 08:13:07 new best model
12/29 08:26:46   decaying learning rate to: 0.349
12/29 08:37:01 step 18000 epoch 8 learning rate 0.349 step-time 0.715 loss 33.139
12/29 08:37:01 starting evaluation
12/29 08:40:54 test bleu=18.36 loss=40.54 penalty=0.978 ratio=0.979
12/29 08:40:54 saving model to models/8_fold_codenn/checkpoints
12/29 08:40:54 finished saving model
12/29 08:40:54 new best model
12/29 08:59:46   decaying learning rate to: 0.332
12/29 09:04:40 step 20000 epoch 9 learning rate 0.332 step-time 0.711 loss 31.207
12/29 09:04:40 starting evaluation
12/29 09:08:49 test bleu=20.87 loss=39.80 penalty=0.902 ratio=0.906
12/29 09:08:49 saving model to models/8_fold_codenn/checkpoints
12/29 09:08:50 finished saving model
12/29 09:08:50 new best model
12/29 09:32:38 step 22000 epoch 9 learning rate 0.332 step-time 0.712 loss 29.403
12/29 09:32:38 starting evaluation
12/29 09:36:43 test bleu=21.41 loss=38.93 penalty=0.873 ratio=0.881
12/29 09:36:43 saving model to models/8_fold_codenn/checkpoints
12/29 09:36:43 finished saving model
12/29 09:36:43 new best model
12/29 09:37:13   decaying learning rate to: 0.315
12/29 10:00:20 step 24000 epoch 10 learning rate 0.315 step-time 0.706 loss 26.854
12/29 10:00:20 starting evaluation
12/29 10:04:31 test bleu=22.76 loss=38.92 penalty=0.942 ratio=0.943
12/29 10:04:31 saving model to models/8_fold_codenn/checkpoints
12/29 10:04:31 finished saving model
12/29 10:04:31 new best model
12/29 10:10:26   decaying learning rate to: 0.299
12/29 10:28:23 step 26000 epoch 11 learning rate 0.299 step-time 0.714 loss 25.143
12/29 10:28:23 starting evaluation
12/29 10:32:26 test bleu=23.22 loss=39.61 penalty=0.809 ratio=0.826
12/29 10:32:26 saving model to models/8_fold_codenn/checkpoints
12/29 10:32:26 finished saving model
12/29 10:32:26 new best model
12/29 10:43:49   decaying learning rate to: 0.284
12/29 10:56:16 step 28000 epoch 12 learning rate 0.284 step-time 0.713 loss 23.628
12/29 10:56:16 starting evaluation
12/29 11:00:24 test bleu=24.45 loss=40.34 penalty=0.883 ratio=0.889
12/29 11:00:24 saving model to models/8_fold_codenn/checkpoints
12/29 11:00:24 finished saving model
12/29 11:00:24 new best model
12/29 11:17:15   decaying learning rate to: 0.27
12/29 11:24:22 step 30000 epoch 13 learning rate 0.27 step-time 0.717 loss 22.227
12/29 11:24:22 starting evaluation
12/29 11:28:42 test bleu=25.34 loss=40.45 penalty=0.957 ratio=0.958
12/29 11:28:42 saving model to models/8_fold_codenn/checkpoints
12/29 11:28:42 finished saving model
12/29 11:28:42 new best model
12/29 11:50:57   decaying learning rate to: 0.257
12/29 11:52:57 step 32000 epoch 14 learning rate 0.257 step-time 0.726 loss 20.849
12/29 11:52:57 starting evaluation
12/29 11:56:53 test bleu=26.53 loss=41.39 penalty=0.900 ratio=0.905
12/29 11:56:53 saving model to models/8_fold_codenn/checkpoints
12/29 11:56:53 finished saving model
12/29 11:56:53 new best model
12/29 12:21:00 step 34000 epoch 14 learning rate 0.257 step-time 0.721 loss 19.061
12/29 12:21:00 starting evaluation
12/29 12:25:07 test bleu=26.61 loss=40.68 penalty=0.875 ratio=0.882
12/29 12:25:07 saving model to models/8_fold_codenn/checkpoints
12/29 12:25:08 finished saving model
12/29 12:25:08 new best model
12/29 12:28:22   decaying learning rate to: 0.244
12/29 12:48:53 step 36000 epoch 15 learning rate 0.244 step-time 0.710 loss 17.581
12/29 12:48:53 starting evaluation
12/29 12:53:04 test bleu=27.53 loss=41.64 penalty=0.935 ratio=0.937
12/29 12:53:04 saving model to models/8_fold_codenn/checkpoints
12/29 12:53:05 finished saving model
12/29 12:53:05 new best model
12/29 13:01:37   decaying learning rate to: 0.232
12/29 13:16:45 step 38000 epoch 16 learning rate 0.232 step-time 0.708 loss 16.363
12/29 13:16:45 starting evaluation
12/29 13:21:02 test bleu=27.81 loss=43.45 penalty=1.000 ratio=1.014
12/29 13:21:02 saving model to models/8_fold_codenn/checkpoints
12/29 13:21:03 finished saving model
12/29 13:21:03 new best model
12/29 13:35:01   decaying learning rate to: 0.22
12/29 13:44:45 step 40000 epoch 17 learning rate 0.22 step-time 0.709 loss 15.259
12/29 13:44:45 starting evaluation
12/29 13:48:56 test bleu=28.48 loss=44.25 penalty=0.934 ratio=0.936
12/29 13:48:56 saving model to models/8_fold_codenn/checkpoints
12/29 13:48:56 finished saving model
12/29 13:48:56 new best model
12/29 14:08:22   decaying learning rate to: 0.209
12/29 14:12:47 step 42000 epoch 18 learning rate 0.209 step-time 0.714 loss 14.293
12/29 14:12:47 starting evaluation
12/29 14:16:56 test bleu=29.07 loss=46.14 penalty=0.936 ratio=0.938
12/29 14:16:56 saving model to models/8_fold_codenn/checkpoints
12/29 14:16:56 finished saving model
12/29 14:16:56 new best model
12/29 14:40:54 step 44000 epoch 18 learning rate 0.209 step-time 0.717 loss 13.296
12/29 14:40:54 starting evaluation
12/29 14:45:06 test bleu=29.55 loss=45.70 penalty=0.911 ratio=0.915
12/29 14:45:06 saving model to models/8_fold_codenn/checkpoints
12/29 14:45:06 finished saving model
12/29 14:45:06 new best model
12/29 14:46:06   decaying learning rate to: 0.199
12/29 15:09:25 step 46000 epoch 19 learning rate 0.199 step-time 0.727 loss 11.736
12/29 15:09:25 starting evaluation
12/29 15:13:30 test bleu=30.19 loss=46.92 penalty=0.988 ratio=0.988
12/29 15:13:30 saving model to models/8_fold_codenn/checkpoints
12/29 15:13:30 finished saving model
12/29 15:13:30 new best model
12/29 15:19:52   decaying learning rate to: 0.189
12/29 15:37:40 step 48000 epoch 20 learning rate 0.189 step-time 0.723 loss 10.932
12/29 15:37:40 starting evaluation
12/29 15:41:52 test bleu=30.12 loss=49.28 penalty=0.949 ratio=0.951
12/29 15:41:52 saving model to models/8_fold_codenn/checkpoints
12/29 15:41:52 finished saving model
12/29 15:53:18   decaying learning rate to: 0.179
12/29 16:05:40 step 50000 epoch 21 learning rate 0.179 step-time 0.712 loss 10.175
12/29 16:05:40 starting evaluation
12/29 16:09:56 test bleu=30.56 loss=51.00 penalty=0.980 ratio=0.980
12/29 16:09:56 saving model to models/8_fold_codenn/checkpoints
12/29 16:09:56 finished saving model
12/29 16:09:56 new best model
12/29 16:26:45   decaying learning rate to: 0.17
12/29 16:33:41 step 52000 epoch 22 learning rate 0.17 step-time 0.710 loss 9.535
12/29 16:33:41 starting evaluation
12/29 16:37:58 test bleu=30.53 loss=53.15 penalty=1.000 ratio=1.011
12/29 16:37:58 saving model to models/8_fold_codenn/checkpoints
12/29 16:37:58 finished saving model
12/29 17:00:20   decaying learning rate to: 0.162
12/29 17:01:48 step 54000 epoch 23 learning rate 0.162 step-time 0.713 loss 8.806
12/29 17:01:48 starting evaluation
12/29 17:06:04 test bleu=31.39 loss=54.64 penalty=0.985 ratio=0.985
12/29 17:06:04 saving model to models/8_fold_codenn/checkpoints
12/29 17:06:04 finished saving model
12/29 17:06:04 new best model
12/29 17:29:56 step 56000 epoch 23 learning rate 0.162 step-time 0.714 loss 7.817
12/29 17:29:56 starting evaluation
12/29 17:34:07 test bleu=31.82 loss=54.82 penalty=0.967 ratio=0.967
12/29 17:34:07 saving model to models/8_fold_codenn/checkpoints
12/29 17:34:07 finished saving model
12/29 17:34:07 new best model
12/29 17:38:05   decaying learning rate to: 0.154
12/29 17:57:58 step 58000 epoch 24 learning rate 0.154 step-time 0.714 loss 7.122
12/29 17:57:58 starting evaluation
12/29 18:02:12 test bleu=31.81 loss=57.14 penalty=0.981 ratio=0.981
12/29 18:02:12 saving model to models/8_fold_codenn/checkpoints
12/29 18:02:12 finished saving model
12/29 18:11:37   decaying learning rate to: 0.146
12/29 18:26:19 step 60000 epoch 25 learning rate 0.146 step-time 0.722 loss 6.586
12/29 18:26:19 starting evaluation
12/29 18:30:34 test bleu=31.84 loss=59.40 penalty=0.991 ratio=0.991
12/29 18:30:34 saving model to models/8_fold_codenn/checkpoints
12/29 18:30:34 finished saving model
12/29 18:30:34 new best model
12/29 18:45:30   decaying learning rate to: 0.139
12/29 18:54:58 step 62000 epoch 26 learning rate 0.139 step-time 0.730 loss 6.209
12/29 18:54:58 starting evaluation
12/29 18:59:05 test bleu=31.43 loss=61.87 penalty=1.000 ratio=1.021
12/29 18:59:05 saving model to models/8_fold_codenn/checkpoints
12/29 18:59:05 finished saving model
12/29 19:19:03   decaying learning rate to: 0.132
12/29 19:23:00 step 64000 epoch 27 learning rate 0.132 step-time 0.716 loss 5.675
12/29 19:23:00 starting evaluation
12/29 19:27:16 test bleu=32.26 loss=64.07 penalty=1.000 ratio=1.000
12/29 19:27:16 saving model to models/8_fold_codenn/checkpoints
12/29 19:27:16 finished saving model
12/29 19:27:16 new best model
12/29 19:51:03 step 66000 epoch 27 learning rate 0.132 step-time 0.711 loss 5.235
12/29 19:51:03 starting evaluation
12/29 19:55:20 test bleu=31.93 loss=64.53 penalty=1.000 ratio=1.017
12/29 19:55:20 saving model to models/8_fold_codenn/checkpoints
12/29 19:55:20 finished saving model
12/29 19:56:50   decaying learning rate to: 0.125
12/29 20:19:13 step 68000 epoch 28 learning rate 0.125 step-time 0.715 loss 4.583
12/29 20:19:13 starting evaluation
12/29 20:23:29 test bleu=32.62 loss=67.36 penalty=0.990 ratio=0.990
12/29 20:23:29 saving model to models/8_fold_codenn/checkpoints
12/29 20:23:29 finished saving model
12/29 20:23:29 new best model
12/29 20:30:26   decaying learning rate to: 0.119
12/29 20:47:22 step 70000 epoch 29 learning rate 0.119 step-time 0.714 loss 4.248
12/29 20:47:22 starting evaluation
12/29 20:51:36 test bleu=32.42 loss=70.01 penalty=1.000 ratio=1.008
12/29 20:51:36 saving model to models/8_fold_codenn/checkpoints
12/29 20:51:36 finished saving model
12/29 21:03:58   decaying learning rate to: 0.113
12/29 21:15:26 step 72000 epoch 30 learning rate 0.113 step-time 0.713 loss 3.952
12/29 21:15:26 starting evaluation
12/29 21:19:44 test bleu=31.52 loss=71.66 penalty=1.000 ratio=1.045
12/29 21:19:44 saving model to models/8_fold_codenn/checkpoints
12/29 21:19:44 finished saving model
12/29 21:37:35   decaying learning rate to: 0.107
12/29 21:43:41 step 74000 epoch 31 learning rate 0.107 step-time 0.717 loss 3.667
12/29 21:43:41 starting evaluation
12/29 21:48:01 test bleu=32.54 loss=74.30 penalty=1.000 ratio=1.022
12/29 21:48:01 saving model to models/8_fold_codenn/checkpoints
12/29 21:48:02 finished saving model
12/29 22:11:23   decaying learning rate to: 0.102
12/29 22:12:22 step 76000 epoch 32 learning rate 0.102 step-time 0.728 loss 3.434
12/29 22:12:22 starting evaluation
12/29 22:16:22 test bleu=33.09 loss=75.74 penalty=1.000 ratio=1.011
12/29 22:16:22 saving model to models/8_fold_codenn/checkpoints
12/29 22:16:22 finished saving model
12/29 22:16:22 new best model
12/29 22:40:35 step 78000 epoch 32 learning rate 0.102 step-time 0.725 loss 2.982
12/29 22:40:35 starting evaluation
12/29 22:44:51 test bleu=33.28 loss=77.39 penalty=1.000 ratio=1.011
12/29 22:44:51 saving model to models/8_fold_codenn/checkpoints
12/29 22:44:51 finished saving model
12/29 22:44:51 new best model
12/29 22:49:01   decaying learning rate to: 0.0969
12/29 23:08:41 step 80000 epoch 33 learning rate 0.0969 step-time 0.713 loss 2.746
12/29 23:08:41 starting evaluation
12/29 23:12:57 test bleu=33.28 loss=79.78 penalty=1.000 ratio=1.005
12/29 23:12:57 saving model to models/8_fold_codenn/checkpoints
12/29 23:12:58 finished saving model
12/29 23:22:29   decaying learning rate to: 0.092
12/29 23:36:45 step 82000 epoch 34 learning rate 0.092 step-time 0.711 loss 2.571
12/29 23:36:45 starting evaluation
12/29 23:41:01 test bleu=32.87 loss=81.91 penalty=1.000 ratio=1.026
12/29 23:41:01 saving model to models/8_fold_codenn/checkpoints
12/29 23:41:01 finished saving model
12/29 23:55:54   decaying learning rate to: 0.0874
12/30 00:04:48 step 84000 epoch 35 learning rate 0.0874 step-time 0.711 loss 2.391
12/30 00:04:48 starting evaluation
12/30 00:09:03 test bleu=31.95 loss=83.74 penalty=1.000 ratio=1.050
12/30 00:09:03 saving model to models/8_fold_codenn/checkpoints
12/30 00:09:03 finished saving model
12/30 00:29:17   decaying learning rate to: 0.083
12/30 00:32:44 step 86000 epoch 36 learning rate 0.083 step-time 0.709 loss 2.215
12/30 00:32:44 starting evaluation
12/30 00:36:59 test bleu=33.17 loss=86.54 penalty=1.000 ratio=1.022
12/30 00:36:59 saving model to models/8_fold_codenn/checkpoints
12/30 00:36:59 finished saving model
12/30 01:00:36 step 88000 epoch 36 learning rate 0.083 step-time 0.706 loss 2.027
12/30 01:00:36 starting evaluation
12/30 01:04:50 test bleu=32.46 loss=87.59 penalty=1.000 ratio=1.040
12/30 01:04:50 saving model to models/8_fold_codenn/checkpoints
12/30 01:04:51 finished saving model
12/30 01:06:49   decaying learning rate to: 0.0789
12/30 01:28:41 step 90000 epoch 37 learning rate 0.0789 step-time 0.713 loss 1.793
12/30 01:28:41 starting evaluation
12/30 01:32:56 test bleu=32.62 loss=89.45 penalty=1.000 ratio=1.033
12/30 01:32:56 saving model to models/8_fold_codenn/checkpoints
12/30 01:32:56 finished saving model
12/30 01:40:20   decaying learning rate to: 0.0749
12/30 01:57:01 step 92000 epoch 38 learning rate 0.0749 step-time 0.721 loss 1.693
12/30 01:57:01 starting evaluation
12/30 02:00:53 test bleu=33.97 loss=91.45 penalty=0.990 ratio=0.991
12/30 02:00:53 saving model to models/8_fold_codenn/checkpoints
12/30 02:00:53 finished saving model
12/30 02:00:53 new best model
12/30 02:13:33   decaying learning rate to: 0.0712
12/30 02:24:52 step 94000 epoch 39 learning rate 0.0712 step-time 0.717 loss 1.573
12/30 02:24:52 starting evaluation
12/30 02:29:04 test bleu=33.87 loss=93.24 penalty=0.999 ratio=0.999
12/30 02:29:04 saving model to models/8_fold_codenn/checkpoints
12/30 02:29:04 finished saving model
12/30 02:46:45   decaying learning rate to: 0.0676
12/30 02:52:40 step 96000 epoch 40 learning rate 0.0676 step-time 0.706 loss 1.479
12/30 02:52:40 starting evaluation
12/30 02:56:54 test bleu=32.30 loss=94.99 penalty=1.000 ratio=1.056
12/30 02:56:55 saving model to models/8_fold_codenn/checkpoints
12/30 02:56:55 finished saving model
12/30 03:20:04   decaying learning rate to: 0.0643
12/30 03:20:33 step 98000 epoch 41 learning rate 0.0643 step-time 0.707 loss 1.392
12/30 03:20:33 starting evaluation
12/30 03:24:50 test bleu=32.81 loss=96.30 penalty=1.000 ratio=1.042
12/30 03:24:50 saving model to models/8_fold_codenn/checkpoints
12/30 03:24:51 finished saving model
12/30 03:48:38 step 100000 epoch 41 learning rate 0.0643 step-time 0.712 loss 1.215
12/30 03:48:38 starting evaluation
12/30 03:52:56 test bleu=32.80 loss=98.08 penalty=1.000 ratio=1.043
12/30 03:52:56 saving model to models/8_fold_codenn/checkpoints
12/30 03:52:56 finished saving model
12/30 03:57:56   decaying learning rate to: 0.061
12/30 04:16:47 step 102000 epoch 42 learning rate 0.061 step-time 0.714 loss 1.148
12/30 04:16:47 starting evaluation
12/30 04:21:04 test bleu=32.70 loss=99.90 penalty=1.000 ratio=1.041
12/30 04:21:04 saving model to models/8_fold_codenn/checkpoints
12/30 04:21:04 finished saving model
12/30 04:31:31   decaying learning rate to: 0.058
12/30 04:44:56 step 104000 epoch 43 learning rate 0.058 step-time 0.714 loss 1.083
12/30 04:44:56 starting evaluation
12/30 04:49:15 test bleu=33.00 loss=102.03 penalty=1.000 ratio=1.037
12/30 04:49:15 saving model to models/8_fold_codenn/checkpoints
12/30 04:49:15 finished saving model
12/30 05:05:07   decaying learning rate to: 0.0551
12/30 05:13:31 step 106000 epoch 44 learning rate 0.0551 step-time 0.726 loss 1.014
12/30 05:13:31 starting evaluation
12/30 05:17:40 test bleu=32.48 loss=102.91 penalty=1.000 ratio=1.049
12/30 05:17:40 saving model to models/8_fold_codenn/checkpoints
12/30 05:17:40 finished saving model
12/30 05:38:57   decaying learning rate to: 0.0523
12/30 05:41:56 step 108000 epoch 45 learning rate 0.0523 step-time 0.726 loss 0.969
12/30 05:41:56 starting evaluation
12/30 05:46:09 test bleu=32.93 loss=104.17 penalty=1.000 ratio=1.043
12/30 05:46:09 saving model to models/8_fold_codenn/checkpoints
12/30 05:46:09 finished saving model
12/30 06:10:03 step 110000 epoch 45 learning rate 0.0523 step-time 0.715 loss 0.886
12/30 06:10:03 starting evaluation
12/30 06:14:20 test bleu=33.22 loss=105.68 penalty=1.000 ratio=1.034
12/30 06:14:20 saving model to models/8_fold_codenn/checkpoints
12/30 06:14:20 finished saving model
12/30 06:16:49   decaying learning rate to: 0.0497
12/30 06:38:11 step 112000 epoch 46 learning rate 0.0497 step-time 0.713 loss 0.812
12/30 06:38:11 starting evaluation
12/30 06:42:29 test bleu=33.08 loss=107.15 penalty=1.000 ratio=1.042
12/30 06:42:29 saving model to models/8_fold_codenn/checkpoints
12/30 06:42:29 finished saving model
12/30 06:50:24   decaying learning rate to: 0.0472
12/30 07:06:17 step 114000 epoch 47 learning rate 0.0472 step-time 0.712 loss 0.778
12/30 07:06:17 starting evaluation
12/30 07:10:34 test bleu=33.70 loss=108.17 penalty=1.000 ratio=1.022
12/30 07:10:34 saving model to models/8_fold_codenn/checkpoints
12/30 07:10:34 finished saving model
12/30 07:23:56   decaying learning rate to: 0.0449
12/30 07:34:39 step 116000 epoch 48 learning rate 0.0449 step-time 0.720 loss 0.739
12/30 07:34:39 starting evaluation
12/30 07:39:02 test bleu=33.58 loss=109.90 penalty=1.000 ratio=1.023
12/30 07:39:02 saving model to models/8_fold_codenn/checkpoints
12/30 07:39:02 finished saving model
12/30 07:58:20   decaying learning rate to: 0.0426
12/30 08:03:45 step 118000 epoch 49 learning rate 0.0426 step-time 0.739 loss 0.699
12/30 08:03:45 starting evaluation
12/30 08:08:19 test bleu=33.08 loss=111.10 penalty=1.000 ratio=1.039
12/30 08:08:19 saving model to models/8_fold_codenn/checkpoints
12/30 08:08:19 finished saving model
12/30 08:33:20 step 120000 epoch 50 learning rate 0.0426 step-time 0.747 loss 0.683
12/30 08:33:20 starting evaluation
12/30 08:37:42 test bleu=33.36 loss=111.20 penalty=1.000 ratio=1.034
12/30 08:37:42 saving model to models/8_fold_codenn/checkpoints
12/30 08:37:43 finished saving model
12/30 08:37:44   decaying learning rate to: 0.0405
12/30 09:02:14 step 122000 epoch 50 learning rate 0.0405 step-time 0.733 loss 0.608
12/30 09:02:14 starting evaluation
12/30 09:06:31 test bleu=33.66 loss=112.85 penalty=1.000 ratio=1.026
12/30 09:06:31 saving model to models/8_fold_codenn/checkpoints
12/30 09:06:31 finished saving model
12/30 09:11:57   decaying learning rate to: 0.0385
12/30 09:30:19 step 124000 epoch 51 learning rate 0.0385 step-time 0.712 loss 0.589
12/30 09:30:19 starting evaluation
12/30 09:34:37 test bleu=32.53 loss=113.90 penalty=1.000 ratio=1.055
12/30 09:34:37 saving model to models/8_fold_codenn/checkpoints
12/30 09:34:37 finished saving model
12/30 09:45:34   decaying learning rate to: 0.0365
12/30 09:58:29 step 126000 epoch 52 learning rate 0.0365 step-time 0.714 loss 0.555
12/30 09:58:29 starting evaluation
12/30 10:02:47 test bleu=33.13 loss=114.67 penalty=1.000 ratio=1.043
12/30 10:02:47 saving model to models/8_fold_codenn/checkpoints
12/30 10:02:47 finished saving model
12/30 10:19:09   decaying learning rate to: 0.0347
12/30 10:26:38 step 128000 epoch 53 learning rate 0.0347 step-time 0.714 loss 0.541
12/30 10:26:38 starting evaluation
12/30 10:30:56 test bleu=33.25 loss=115.59 penalty=1.000 ratio=1.043
12/30 10:30:56 saving model to models/8_fold_codenn/checkpoints
12/30 10:30:56 finished saving model
12/30 10:52:44   decaying learning rate to: 0.033
12/30 10:54:54 step 130000 epoch 54 learning rate 0.033 step-time 0.717 loss 0.528
12/30 10:54:54 starting evaluation
12/30 10:59:19 test bleu=33.38 loss=115.82 penalty=1.000 ratio=1.038
12/30 10:59:19 saving model to models/8_fold_codenn/checkpoints
12/30 10:59:19 finished saving model
12/30 11:23:39 step 132000 epoch 54 learning rate 0.033 step-time 0.728 loss 0.488
12/30 11:23:39 starting evaluation
12/30 11:27:39 test bleu=32.89 loss=116.31 penalty=1.000 ratio=1.046
12/30 11:27:39 saving model to models/8_fold_codenn/checkpoints
12/30 11:27:39 finished saving model
12/30 11:30:32   decaying learning rate to: 0.0313
12/30 11:51:49 step 134000 epoch 55 learning rate 0.0313 step-time 0.723 loss 0.455
12/30 11:51:49 starting evaluation
12/30 11:56:06 test bleu=33.08 loss=116.41 penalty=1.000 ratio=1.050
12/30 11:56:06 saving model to models/8_fold_codenn/checkpoints
12/30 11:56:06 finished saving model
12/30 12:04:08   decaying learning rate to: 0.0298
12/30 12:19:59 step 136000 epoch 56 learning rate 0.0298 step-time 0.714 loss 0.445
12/30 12:19:59 starting evaluation
12/30 12:24:15 test bleu=32.83 loss=117.84 penalty=1.000 ratio=1.053
12/30 12:24:15 saving model to models/8_fold_codenn/checkpoints
12/30 12:24:15 finished saving model
12/30 12:37:44   decaying learning rate to: 0.0283
12/30 12:48:07 step 138000 epoch 57 learning rate 0.0283 step-time 0.714 loss 0.429
12/30 12:48:07 starting evaluation
12/30 12:52:25 test bleu=32.73 loss=118.75 penalty=1.000 ratio=1.056
12/30 12:52:25 saving model to models/8_fold_codenn/checkpoints
12/30 12:52:25 finished saving model
12/30 13:11:20   decaying learning rate to: 0.0269
12/30 13:16:17 step 140000 epoch 58 learning rate 0.0269 step-time 0.714 loss 0.420
12/30 13:16:17 starting evaluation
12/30 13:20:33 test bleu=33.50 loss=118.43 penalty=1.000 ratio=1.035
12/30 13:20:33 saving model to models/8_fold_codenn/checkpoints
12/30 13:20:33 finished saving model
12/30 13:44:23 step 142000 epoch 58 learning rate 0.0269 step-time 0.713 loss 0.409
12/30 13:44:23 starting evaluation
12/30 13:48:40 test bleu=33.33 loss=118.43 penalty=1.000 ratio=1.036
12/30 13:48:40 saving model to models/8_fold_codenn/checkpoints
12/30 13:48:41 finished saving model
12/30 13:49:11   decaying learning rate to: 0.0255
12/30 14:12:37 step 144000 epoch 59 learning rate 0.0255 step-time 0.716 loss 0.371
12/30 14:12:37 starting evaluation
12/30 14:16:59 test bleu=33.27 loss=119.24 penalty=1.000 ratio=1.039
12/30 14:16:59 saving model to models/8_fold_codenn/checkpoints
12/30 14:16:59 finished saving model
12/30 14:22:57   decaying learning rate to: 0.0242
12/30 14:41:12 step 146000 epoch 60 learning rate 0.0242 step-time 0.725 loss 0.360
12/30 14:41:12 starting evaluation
12/30 14:45:16 test bleu=33.49 loss=119.72 penalty=1.000 ratio=1.030
12/30 14:45:16 saving model to models/8_fold_codenn/checkpoints
12/30 14:45:16 finished saving model
12/30 14:56:39   decaying learning rate to: 0.023
12/30 15:09:31 step 148000 epoch 61 learning rate 0.023 step-time 0.726 loss 0.351
12/30 15:09:31 starting evaluation
12/30 15:13:49 test bleu=33.23 loss=120.10 penalty=1.000 ratio=1.040
12/30 15:13:49 saving model to models/8_fold_codenn/checkpoints
12/30 15:13:49 finished saving model
12/30 15:30:16   decaying learning rate to: 0.0219
12/30 15:37:40 step 150000 epoch 62 learning rate 0.0219 step-time 0.714 loss 0.345
12/30 15:37:40 starting evaluation
12/30 15:41:56 test bleu=33.43 loss=120.47 penalty=1.000 ratio=1.034
12/30 15:41:56 saving model to models/8_fold_codenn/checkpoints
12/30 15:41:56 finished saving model
12/30 16:03:52   decaying learning rate to: 0.0208
12/30 16:05:50 step 152000 epoch 63 learning rate 0.0208 step-time 0.715 loss 0.337
12/30 16:05:50 starting evaluation
12/30 16:10:07 test bleu=33.01 loss=121.07 penalty=1.000 ratio=1.049
12/30 16:10:07 saving model to models/8_fold_codenn/checkpoints
12/30 16:10:07 finished saving model
12/30 16:33:54 step 154000 epoch 63 learning rate 0.0208 step-time 0.712 loss 0.314
12/30 16:33:54 starting evaluation
12/30 16:38:11 test bleu=33.27 loss=120.57 penalty=1.000 ratio=1.040
12/30 16:38:11 saving model to models/8_fold_codenn/checkpoints
12/30 16:38:11 finished saving model
12/30 16:41:41   decaying learning rate to: 0.0197
12/30 17:02:06 step 156000 epoch 64 learning rate 0.0197 step-time 0.716 loss 0.300
12/30 17:02:06 starting evaluation
12/30 17:06:22 test bleu=33.57 loss=121.59 penalty=1.000 ratio=1.029
12/30 17:06:22 saving model to models/8_fold_codenn/checkpoints
12/30 17:06:22 finished saving model
12/30 17:15:17   decaying learning rate to: 0.0188
12/30 17:30:18 step 158000 epoch 65 learning rate 0.0188 step-time 0.716 loss 0.292
12/30 17:30:18 starting evaluation
12/30 17:34:34 test bleu=33.14 loss=122.04 penalty=1.000 ratio=1.042
12/30 17:34:34 saving model to models/8_fold_codenn/checkpoints
12/30 17:34:34 finished saving model
12/30 17:48:56   decaying learning rate to: 0.0178
12/30 17:58:46 step 160000 epoch 66 learning rate 0.0178 step-time 0.724 loss 0.294
12/30 17:58:46 starting evaluation
12/30 18:02:59 test bleu=32.78 loss=122.32 penalty=1.000 ratio=1.056
12/30 18:02:59 saving model to models/8_fold_codenn/checkpoints
12/30 18:02:59 finished saving model
12/30 18:22:51   decaying learning rate to: 0.0169
12/30 18:27:17 step 162000 epoch 67 learning rate 0.0169 step-time 0.727 loss 0.284
12/30 18:27:17 starting evaluation
12/30 18:31:26 test bleu=33.56 loss=122.27 penalty=1.000 ratio=1.028
12/30 18:31:26 saving model to models/8_fold_codenn/checkpoints
12/30 18:31:27 finished saving model
12/30 18:55:28 step 164000 epoch 67 learning rate 0.0169 step-time 0.719 loss 0.276
12/30 18:55:28 starting evaluation
12/30 18:59:45 test bleu=33.33 loss=122.11 penalty=1.000 ratio=1.038
12/30 18:59:45 saving model to models/8_fold_codenn/checkpoints
12/30 18:59:45 finished saving model
12/30 19:00:46   decaying learning rate to: 0.0161
12/30 19:23:36 step 166000 epoch 68 learning rate 0.0161 step-time 0.714 loss 0.257
12/30 19:23:36 starting evaluation
12/30 19:27:51 test bleu=33.77 loss=122.55 penalty=1.000 ratio=1.024
12/30 19:27:51 saving model to models/8_fold_codenn/checkpoints
12/30 19:27:51 finished saving model
12/30 19:34:18   decaying learning rate to: 0.0153
12/30 19:51:40 step 168000 epoch 69 learning rate 0.0153 step-time 0.712 loss 0.257
12/30 19:51:40 starting evaluation
12/30 19:55:55 test bleu=33.59 loss=122.94 penalty=1.000 ratio=1.031
12/30 19:55:55 saving model to models/8_fold_codenn/checkpoints
12/30 19:55:55 finished saving model
12/30 20:07:51   decaying learning rate to: 0.0145
12/30 20:19:49 step 170000 epoch 70 learning rate 0.0145 step-time 0.715 loss 0.244
12/30 20:19:49 starting evaluation
12/30 20:24:04 test bleu=33.50 loss=123.16 penalty=1.000 ratio=1.033
12/30 20:24:04 saving model to models/8_fold_codenn/checkpoints
12/30 20:24:04 finished saving model
12/30 20:41:27   decaying learning rate to: 0.0138
12/30 20:47:58 step 172000 epoch 71 learning rate 0.0138 step-time 0.715 loss 0.249
12/30 20:47:58 starting evaluation
12/30 20:52:16 test bleu=32.98 loss=123.46 penalty=1.000 ratio=1.051
12/30 20:52:16 saving model to models/8_fold_codenn/checkpoints
12/30 20:52:16 finished saving model
12/30 21:14:57   decaying learning rate to: 0.0131
12/30 21:16:13 step 174000 epoch 72 learning rate 0.0131 step-time 0.717 loss 0.241
12/30 21:16:13 starting evaluation
12/30 21:20:37 test bleu=33.14 loss=123.69 penalty=1.000 ratio=1.045
12/30 21:20:37 saving model to models/8_fold_codenn/checkpoints
12/30 21:20:37 finished saving model
12/30 21:44:53 step 176000 epoch 72 learning rate 0.0131 step-time 0.726 loss 0.228
12/30 21:44:53 starting evaluation
12/30 21:48:52 test bleu=33.09 loss=123.94 penalty=1.000 ratio=1.045
12/30 21:48:52 saving model to models/8_fold_codenn/checkpoints
12/30 21:48:53 finished saving model
12/30 21:52:44   decaying learning rate to: 0.0124
12/30 22:13:02 step 178000 epoch 73 learning rate 0.0124 step-time 0.723 loss 0.219
12/30 22:13:02 starting evaluation
12/30 22:17:20 test bleu=33.23 loss=124.28 penalty=1.000 ratio=1.039
12/30 22:17:20 saving model to models/8_fold_codenn/checkpoints
12/30 22:17:21 finished saving model
12/30 22:26:23   decaying learning rate to: 0.0118
12/30 22:41:15 step 180000 epoch 74 learning rate 0.0118 step-time 0.715 loss 0.216
12/30 22:41:15 starting evaluation
12/30 22:45:32 test bleu=33.16 loss=124.37 penalty=1.000 ratio=1.042
12/30 22:45:32 saving model to models/8_fold_codenn/checkpoints
12/30 22:45:32 finished saving model
12/30 23:00:03   decaying learning rate to: 0.0112
12/30 23:09:25 step 182000 epoch 75 learning rate 0.0112 step-time 0.715 loss 0.216
12/30 23:09:25 starting evaluation
12/30 23:13:42 test bleu=33.59 loss=124.59 penalty=1.000 ratio=1.031
12/30 23:13:42 saving model to models/8_fold_codenn/checkpoints
12/30 23:13:42 finished saving model
12/30 23:33:36   decaying learning rate to: 0.0107
12/30 23:37:32 step 184000 epoch 76 learning rate 0.0107 step-time 0.713 loss 0.213
12/30 23:37:32 starting evaluation
12/30 23:41:48 test bleu=33.37 loss=124.76 penalty=1.000 ratio=1.038
12/30 23:41:48 saving model to models/8_fold_codenn/checkpoints
12/30 23:41:48 finished saving model
12/31 00:05:41 step 186000 epoch 76 learning rate 0.0107 step-time 0.714 loss 0.207
12/31 00:05:41 starting evaluation
12/31 00:09:56 test bleu=33.25 loss=125.05 penalty=1.000 ratio=1.041
12/31 00:09:56 saving model to models/8_fold_codenn/checkpoints
12/31 00:09:56 finished saving model
12/31 00:11:26   decaying learning rate to: 0.0101
12/31 00:33:43 step 188000 epoch 77 learning rate 0.0101 step-time 0.712 loss 0.194
12/31 00:33:43 starting evaluation
12/31 00:38:01 test bleu=33.18 loss=124.71 penalty=1.000 ratio=1.041
12/31 00:38:01 saving model to models/8_fold_codenn/checkpoints
12/31 00:38:01 finished saving model
12/31 00:44:57   decaying learning rate to: 0.00963
12/31 01:02:07 step 190000 epoch 78 learning rate 0.00963 step-time 0.721 loss 0.193
12/31 01:02:07 starting evaluation
12/31 01:06:13 test bleu=33.45 loss=125.35 penalty=1.000 ratio=1.033
12/31 01:06:13 saving model to models/8_fold_codenn/checkpoints
12/31 01:06:13 finished saving model
12/31 01:18:32   decaying learning rate to: 0.00915
12/31 01:30:24 step 192000 epoch 79 learning rate 0.00915 step-time 0.724 loss 0.194
12/31 01:30:24 starting evaluation
12/31 01:34:31 test bleu=33.30 loss=125.91 penalty=1.000 ratio=1.037
12/31 01:34:31 saving model to models/8_fold_codenn/checkpoints
12/31 01:34:31 finished saving model
12/31 01:51:56   decaying learning rate to: 0.00869
12/31 01:58:19 step 194000 epoch 80 learning rate 0.00869 step-time 0.712 loss 0.190
12/31 01:58:19 starting evaluation
12/31 02:02:35 test bleu=33.27 loss=126.10 penalty=1.000 ratio=1.040
12/31 02:02:35 saving model to models/8_fold_codenn/checkpoints
12/31 02:02:35 finished saving model
12/31 02:25:13   decaying learning rate to: 0.00826
12/31 02:26:11 step 196000 epoch 81 learning rate 0.00826 step-time 0.706 loss 0.190
12/31 02:26:11 starting evaluation
12/31 02:30:26 test bleu=33.13 loss=125.90 penalty=1.000 ratio=1.043
12/31 02:30:26 saving model to models/8_fold_codenn/checkpoints
12/31 02:30:26 finished saving model
12/31 02:54:04 step 198000 epoch 81 learning rate 0.00826 step-time 0.707 loss 0.178
12/31 02:54:04 starting evaluation
12/31 02:58:18 test bleu=33.35 loss=126.38 penalty=1.000 ratio=1.037
12/31 02:58:18 saving model to models/8_fold_codenn/checkpoints
12/31 02:58:18 finished saving model
12/31 03:02:47   decaying learning rate to: 0.00784
12/31 03:22:00 step 200000 epoch 82 learning rate 0.00784 step-time 0.709 loss 0.174
12/31 03:22:00 starting evaluation
12/31 03:26:14 test bleu=33.18 loss=126.31 penalty=1.000 ratio=1.042
12/31 03:26:14 saving model to models/8_fold_codenn/checkpoints
12/31 03:26:15 finished saving model
12/31 03:36:08   decaying learning rate to: 0.00745
12/31 03:49:56 step 202000 epoch 83 learning rate 0.00745 step-time 0.709 loss 0.173
12/31 03:49:56 starting evaluation
12/31 03:54:11 test bleu=33.09 loss=126.79 penalty=1.000 ratio=1.045
12/31 03:54:11 saving model to models/8_fold_codenn/checkpoints
12/31 03:54:11 finished saving model
12/31 04:09:31   decaying learning rate to: 0.00708
12/31 04:17:58 step 204000 epoch 84 learning rate 0.00708 step-time 0.711 loss 0.171
12/31 04:17:58 starting evaluation
12/31 04:22:13 test bleu=33.33 loss=126.88 penalty=1.000 ratio=1.036
12/31 04:22:13 saving model to models/8_fold_codenn/checkpoints
12/31 04:22:13 finished saving model
12/31 04:42:53   decaying learning rate to: 0.00673
12/31 04:46:15 step 206000 epoch 85 learning rate 0.00673 step-time 0.719 loss 0.172
12/31 04:46:15 starting evaluation
12/31 04:50:25 test bleu=33.44 loss=126.99 penalty=1.000 ratio=1.035
12/31 04:50:25 saving model to models/8_fold_codenn/checkpoints
12/31 04:50:25 finished saving model
12/31 05:14:36 step 208000 epoch 85 learning rate 0.00673 step-time 0.724 loss 0.169
12/31 05:14:36 starting evaluation
12/31 05:18:40 test bleu=33.45 loss=127.14 penalty=1.000 ratio=1.036
12/31 05:18:40 saving model to models/8_fold_codenn/checkpoints
12/31 05:18:41 finished saving model
12/31 05:20:30   decaying learning rate to: 0.00639
12/31 05:42:37 step 210000 epoch 86 learning rate 0.00639 step-time 0.716 loss 0.160
12/31 05:42:37 starting evaluation
12/31 05:46:52 test bleu=33.22 loss=127.24 penalty=1.000 ratio=1.039
12/31 05:46:52 saving model to models/8_fold_codenn/checkpoints
12/31 05:46:52 finished saving model
12/31 05:53:51   decaying learning rate to: 0.00607
12/31 06:10:35 step 212000 epoch 87 learning rate 0.00607 step-time 0.710 loss 0.158
12/31 06:10:35 starting evaluation
12/31 06:14:51 test bleu=33.24 loss=127.97 penalty=1.000 ratio=1.040
12/31 06:14:51 saving model to models/8_fold_codenn/checkpoints
12/31 06:14:51 finished saving model
12/31 06:27:11   decaying learning rate to: 0.00577
12/31 06:38:32 step 214000 epoch 88 learning rate 0.00577 step-time 0.709 loss 0.161
12/31 06:38:32 starting evaluation
12/31 06:42:48 test bleu=33.14 loss=127.72 penalty=1.000 ratio=1.043
12/31 06:42:48 saving model to models/8_fold_codenn/checkpoints
12/31 06:42:48 finished saving model
12/31 07:00:32   decaying learning rate to: 0.00548
12/31 07:06:25 step 216000 epoch 89 learning rate 0.00548 step-time 0.707 loss 0.158
12/31 07:06:25 starting evaluation
12/31 07:10:39 test bleu=33.47 loss=128.00 penalty=1.000 ratio=1.032
12/31 07:10:39 saving model to models/8_fold_codenn/checkpoints
12/31 07:10:39 finished saving model
12/31 07:33:55   decaying learning rate to: 0.0052
12/31 07:34:25 step 218000 epoch 90 learning rate 0.0052 step-time 0.711 loss 0.158
12/31 07:34:25 starting evaluation
12/31 07:38:41 test bleu=33.00 loss=127.85 penalty=1.000 ratio=1.049
12/31 07:38:41 saving model to models/8_fold_codenn/checkpoints
12/31 07:38:41 finished saving model
12/31 08:02:24 step 220000 epoch 90 learning rate 0.0052 step-time 0.710 loss 0.150
12/31 08:02:24 starting evaluation
12/31 08:06:40 test bleu=33.18 loss=128.23 penalty=1.000 ratio=1.040
12/31 08:06:40 saving model to models/8_fold_codenn/checkpoints
12/31 08:06:40 finished saving model
12/31 08:11:38   decaying learning rate to: 0.00494
12/31 08:30:43 step 222000 epoch 91 learning rate 0.00494 step-time 0.719 loss 0.147
12/31 08:30:43 starting evaluation
12/31 08:34:49 test bleu=33.08 loss=128.35 penalty=1.000 ratio=1.041
12/31 08:34:49 saving model to models/8_fold_codenn/checkpoints
12/31 08:34:50 finished saving model
12/31 08:45:10   decaying learning rate to: 0.0047
12/31 08:59:02 step 224000 epoch 92 learning rate 0.0047 step-time 0.724 loss 0.149
12/31 08:59:02 starting evaluation
12/31 09:03:10 test bleu=33.30 loss=128.51 penalty=1.000 ratio=1.036
12/31 09:03:10 saving model to models/8_fold_codenn/checkpoints
12/31 09:03:10 finished saving model
12/31 09:18:37   decaying learning rate to: 0.00446
12/31 09:27:01 step 226000 epoch 93 learning rate 0.00446 step-time 0.713 loss 0.147
12/31 09:27:01 starting evaluation
12/31 09:31:17 test bleu=33.06 loss=128.64 penalty=1.000 ratio=1.043
12/31 09:31:17 saving model to models/8_fold_codenn/checkpoints
12/31 09:31:17 finished saving model
12/31 09:52:00   decaying learning rate to: 0.00424
12/31 09:54:54 step 228000 epoch 94 learning rate 0.00424 step-time 0.707 loss 0.147
12/31 09:54:54 starting evaluation
12/31 09:59:10 test bleu=33.30 loss=128.93 penalty=1.000 ratio=1.041
12/31 09:59:10 saving model to models/8_fold_codenn/checkpoints
12/31 09:59:10 finished saving model
12/31 10:22:52 step 230000 epoch 94 learning rate 0.00424 step-time 0.709 loss 0.144
12/31 10:22:52 starting evaluation
12/31 10:27:07 test bleu=33.27 loss=128.78 penalty=1.000 ratio=1.040
12/31 10:27:07 saving model to models/8_fold_codenn/checkpoints
12/31 10:27:07 finished saving model
12/31 10:29:37   decaying learning rate to: 0.00403
12/31 10:50:45 step 232000 epoch 95 learning rate 0.00403 step-time 0.707 loss 0.140
12/31 10:50:45 starting evaluation
12/31 10:55:00 test bleu=33.27 loss=129.13 penalty=1.000 ratio=1.038
12/31 10:55:00 saving model to models/8_fold_codenn/checkpoints
12/31 10:55:00 finished saving model
12/31 11:02:51   decaying learning rate to: 0.00383
12/31 11:18:37 step 234000 epoch 96 learning rate 0.00383 step-time 0.706 loss 0.139
12/31 11:18:37 starting evaluation
12/31 11:22:51 test bleu=33.21 loss=129.11 penalty=1.000 ratio=1.041
12/31 11:22:51 saving model to models/8_fold_codenn/checkpoints
12/31 11:22:52 finished saving model
12/31 11:36:13   decaying learning rate to: 0.00363
12/31 11:46:37 step 236000 epoch 97 learning rate 0.00363 step-time 0.711 loss 0.139
12/31 11:46:37 starting evaluation
12/31 11:50:55 test bleu=33.54 loss=129.25 penalty=1.000 ratio=1.030
12/31 11:50:55 saving model to models/8_fold_codenn/checkpoints
12/31 11:50:55 finished saving model
12/31 12:09:39   decaying learning rate to: 0.00345
12/31 12:15:02 step 238000 epoch 98 learning rate 0.00345 step-time 0.722 loss 0.139
12/31 12:15:02 starting evaluation
12/31 12:19:04 test bleu=33.31 loss=129.48 penalty=1.000 ratio=1.038
12/31 12:19:04 saving model to models/8_fold_codenn/checkpoints
12/31 12:19:04 finished saving model
12/31 12:43:13 step 240000 epoch 99 learning rate 0.00345 step-time 0.722 loss 0.140
12/31 12:43:13 starting evaluation
12/31 12:47:26 test bleu=33.36 loss=129.24 penalty=1.000 ratio=1.037
12/31 12:47:26 saving model to models/8_fold_codenn/checkpoints
12/31 12:47:26 finished saving model
12/31 12:47:27   decaying learning rate to: 0.00328
12/31 13:11:11 step 242000 epoch 99 learning rate 0.00328 step-time 0.710 loss 0.133
12/31 13:11:11 starting evaluation
12/31 13:15:25 test bleu=33.30 loss=129.70 penalty=1.000 ratio=1.039
12/31 13:15:25 saving model to models/8_fold_codenn/checkpoints
12/31 13:15:26 finished saving model
12/31 13:20:40   decaying learning rate to: 0.00312
12/31 13:39:04 step 244000 epoch 100 learning rate 0.00312 step-time 0.707 loss 0.134
12/31 13:39:04 starting evaluation
12/31 13:43:18 test bleu=33.28 loss=129.54 penalty=1.000 ratio=1.037
12/31 13:43:18 saving model to models/8_fold_codenn/checkpoints
12/31 13:43:18 finished saving model
12/31 13:53:36 finished training
12/31 13:53:36 exiting...
12/31 13:53:36 saving model to models/8_fold_codenn/checkpoints
12/31 13:53:36 finished saving model
