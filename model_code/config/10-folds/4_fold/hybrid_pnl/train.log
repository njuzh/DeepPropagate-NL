nohup: ignoring input
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:107: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:30: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

12/26 11:12:37 label: default
12/26 11:12:37 description:
  default configuration
  next line of description
  last line
12/26 11:12:37 /root/icpc/icpc/translate/__main__.py config/10-folds/4_fold/hybrid_pnl/config.yaml --train -v
12/26 11:12:37 commit hash 74e0554cb3eb5df835cef993ad570ff8de651f71
12/26 11:12:37 tensorflow version: 1.14.0
12/26 11:12:37 program arguments
12/26 11:12:37   aggregation_method   'sum'
12/26 11:12:37   align_encoder_id     0
12/26 11:12:37   allow_growth         True
12/26 11:12:37   attention_type       'global'
12/26 11:12:37   attn_filter_length   0
12/26 11:12:37   attn_filters         0
12/26 11:12:37   attn_prev_word       False
12/26 11:12:37   attn_size            128
12/26 11:12:37   attn_temperature     1.0
12/26 11:12:37   attn_window_size     0
12/26 11:12:37   average              False
12/26 11:12:37   baseline_activation  None
12/26 11:12:37   baseline_learning_rate 0.001
12/26 11:12:37   baseline_optimizer   'adam'
12/26 11:12:37   baseline_steps       0
12/26 11:12:37   batch_mode           'standard'
12/26 11:12:37   batch_size           64
12/26 11:12:37   beam_size            5
12/26 11:12:37   bidir                True
12/26 11:12:37   bidir_projection     False
12/26 11:12:37   binary               False
12/26 11:12:37   cell_size            256
12/26 11:12:37   cell_type            'GRU'
12/26 11:12:37   character_level      False
12/26 11:12:37   checkpoints          []
12/26 11:12:37   conditional_rnn      False
12/26 11:12:37   config               'config/10-folds/4_fold/hybrid_pnl/config.yaml'
12/26 11:12:37   convolutions         None
12/26 11:12:37   data_dir             'data/gooddata/4_fold'
12/26 11:12:37   debug                False
12/26 11:12:37   decay_after_n_epoch  1
12/26 11:12:37   decay_every_n_epoch  1
12/26 11:12:37   decay_if_no_progress None
12/26 11:12:37   decoders             [{'max_len': 40, 'name': 'nl'}]
12/26 11:12:37   description          'default configuration\nnext line of description\nlast line\n'
12/26 11:12:37   dev_prefix           'test'
12/26 11:12:37   early_stopping       True
12/26 11:12:37   embedding_dropout    0.0
12/26 11:12:37   embedding_initializer None
12/26 11:12:37   embedding_size       256
12/26 11:12:37   embedding_weight_scale None
12/26 11:12:37   embeddings_on_cpu    True
12/26 11:12:37   encoders             [{'attention_type': 'global', 'max_len': 200, 'name': 'code'},
 {'attention_type': 'global', 'max_len': 80, 'name': 'pnl'}]
12/26 11:12:37   ensemble             False
12/26 11:12:37   eval_burn_in         0
12/26 11:12:37   feed_previous        0.0
12/26 11:12:37   final_state          'last'
12/26 11:12:37   freeze_variables     []
12/26 11:12:37   generate_first       True
12/26 11:12:37   gpu_id               3
12/26 11:12:37   highway_layers       0
12/26 11:12:37   initial_state_dropout 0.0
12/26 11:12:37   initializer          None
12/26 11:12:37   input_layer_dropout  0.0
12/26 11:12:37   input_layers         None
12/26 11:12:37   keep_best            5
12/26 11:12:37   keep_every_n_hours   0
12/26 11:12:37   label                'default'
12/26 11:12:37   layer_norm           False
12/26 11:12:37   layers               1
12/26 11:12:37   learning_rate        0.5
12/26 11:12:37   learning_rate_decay_factor 0.95
12/26 11:12:37   len_normalization    1.0
12/26 11:12:37   log_file             'log.txt'
12/26 11:12:37   loss_function        'xent'
12/26 11:12:37   max_dev_size         0
12/26 11:12:37   max_epochs           100
12/26 11:12:37   max_gradient_norm    5.0
12/26 11:12:37   max_len              50
12/26 11:12:37   max_steps            600000
12/26 11:12:37   max_test_size        0
12/26 11:12:37   max_to_keep          1
12/26 11:12:37   max_train_size       0
12/26 11:12:37   maxout_stride        None
12/26 11:12:37   mem_fraction         1.0
12/26 11:12:37   min_learning_rate    1e-06
12/26 11:12:37   model_dir            'models/4_fold_hybrid_pnl'
12/26 11:12:37   moving_average       None
12/26 11:12:37   no_gpu               False
12/26 11:12:37   optimizer            'sgd'
12/26 11:12:37   orthogonal_init      False
12/26 11:12:37   output               None
12/26 11:12:37   output_dropout       0.0
12/26 11:12:37   parallel_iterations  16
12/26 11:12:37   pervasive_dropout    False
12/26 11:12:37   pooling_avg          True
12/26 11:12:37   post_process_script  None
12/26 11:12:37   pred_deep_layer      False
12/26 11:12:37   pred_edits           False
12/26 11:12:37   pred_embed_proj      True
12/26 11:12:37   pred_maxout_layer    True
12/26 11:12:37   purge                False
12/26 11:12:37   raw_output           False
12/26 11:12:37   read_ahead           1
12/26 11:12:37   reconstruction_attn_weight 0.05
12/26 11:12:37   reconstruction_decoders False
12/26 11:12:37   reconstruction_weight 1.0
12/26 11:12:37   reinforce_after_n_epoch None
12/26 11:12:37   remove_unk           False
12/26 11:12:37   reverse              False
12/26 11:12:37   reverse_input        True
12/26 11:12:37   reward_function      'sentence_bleu'
12/26 11:12:37   rnn_feed_attn        True
12/26 11:12:37   rnn_input_dropout    0.0
12/26 11:12:37   rnn_output_dropout   0.0
12/26 11:12:37   rnn_state_dropout    0.0
12/26 11:12:37   save                 False
12/26 11:12:37   score_function       'corpus_bleu'
12/26 11:12:37   score_functions      ['bleu', 'loss']
12/26 11:12:37   script_dir           'scripts'
12/26 11:12:37   sgd_after_n_epoch    None
12/26 11:12:37   sgd_learning_rate    1.0
12/26 11:12:37   shuffle              True
12/26 11:12:37   softmax_temperature  1.0
12/26 11:12:37   steps_per_checkpoint 2000
12/26 11:12:37   steps_per_eval       2000
12/26 11:12:37   swap_memory          True
12/26 11:12:37   tie_embeddings       False
12/26 11:12:37   time_pooling         None
12/26 11:12:37   train                True
12/26 11:12:37   train_initial_states True
12/26 11:12:37   train_prefix         'train'
12/26 11:12:37   truncate_lines       True
12/26 11:12:37   update_first         False
12/26 11:12:37   use_baseline         False
12/26 11:12:37   use_dropout          False
12/26 11:12:37   use_lstm_full_state  False
12/26 11:12:37   use_previous_word    True
12/26 11:12:37   verbose              True
12/26 11:12:37   vocab_prefix         'vocab'
12/26 11:12:37   weight_scale         None
12/26 11:12:37   word_dropout         0.0
12/26 11:12:37 python random seed: 309688385223403153
12/26 11:12:37 tf random seed:     7284342802975975130
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:203: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

12/26 11:12:37 creating model
12/26 11:12:37 using device: /gpu:3
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:230: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

12/26 11:12:37 copying vocab to models/4_fold_hybrid_pnl/data/vocab.code
12/26 11:12:37 copying vocab to models/4_fold_hybrid_pnl/data/vocab.pnl
12/26 11:12:37 copying vocab to models/4_fold_hybrid_pnl/data/vocab.nl
12/26 11:12:37 reading vocabularies
12/26 11:12:37 creating model
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:60: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:111: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:33: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5b6133eb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5b6133eb00>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:226: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5b6133eef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5b6133eef0>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5be5de26a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5be5de26a0>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5be5de2550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5be5de2550>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5d8e198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5d8e198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:838: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5a76a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5a76a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5a72978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5a72978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:432: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:435: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be59cfbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be59cfbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be59a79b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be59a79b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5809da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5809da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5809be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5809be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5569eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5569eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5565e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5565e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5544fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5544fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5544fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5544fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5544fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5be5544fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:919: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5be53ca198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5be53ca198>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:131: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:223: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5b84b24e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5b84b24e10>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5b84ac0f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5b84ac0f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5b84ae0e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5b84ae0e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5b84ae0048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5b84ae0048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5b84a33c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5b84a33c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5b84a336a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5b84a336a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5b849d9588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5b849d9588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5b849d9588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5b849d9588>>: AssertionError: Bad argument number for Name: 3, expecting 4
12/26 11:12:44 model parameters (45)
12/26 11:12:44   baseline_step:0 ()
12/26 11:12:44   decoder_nl/attention_code/U_a/kernel:0 (512, 128)
12/26 11:12:44   decoder_nl/attention_code/W_a/bias:0 (128,)
12/26 11:12:44   decoder_nl/attention_code/W_a/kernel:0 (256, 128)
12/26 11:12:44   decoder_nl/attention_code/v_a:0 (128,)
12/26 11:12:44   decoder_nl/attention_pnl/U_a/kernel:0 (512, 128)
12/26 11:12:44   decoder_nl/attention_pnl/W_a/bias:0 (128,)
12/26 11:12:44   decoder_nl/attention_pnl/W_a/kernel:0 (256, 128)
12/26 11:12:44   decoder_nl/attention_pnl/v_a:0 (128,)
12/26 11:12:44   decoder_nl/code_pnl/initial_state_projection/bias:0 (256,)
12/26 11:12:44   decoder_nl/code_pnl/initial_state_projection/kernel:0 (512, 256)
12/26 11:12:44   decoder_nl/gru_cell/candidate/bias:0 (256,)
12/26 11:12:44   decoder_nl/gru_cell/candidate/kernel:0 (1024, 256)
12/26 11:12:44   decoder_nl/gru_cell/gates/bias:0 (512,)
12/26 11:12:44   decoder_nl/gru_cell/gates/kernel:0 (1024, 512)
12/26 11:12:44   decoder_nl/maxout/bias:0 (256,)
12/26 11:12:44   decoder_nl/maxout/kernel:0 (1024, 256)
12/26 11:12:44   decoder_nl/softmax0/kernel:0 (128, 256)
12/26 11:12:44   decoder_nl/softmax1/bias:0 (37998,)
12/26 11:12:44   decoder_nl/softmax1/kernel:0 (256, 37998)
12/26 11:12:44   embedding_code:0 (50000, 256)
12/26 11:12:44   embedding_nl:0 (37998, 256)
12/26 11:12:44   embedding_pnl:0 (37587, 256)
12/26 11:12:44   encoder_code/initial_state_bw:0 (256,)
12/26 11:12:44   encoder_code/initial_state_fw:0 (256,)
12/26 11:12:44   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/26 11:12:44   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/26 11:12:44   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/26 11:12:44   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/26 11:12:44   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/26 11:12:44   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/26 11:12:44   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/26 11:12:44   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/26 11:12:44   encoder_pnl/initial_state_bw:0 (256,)
12/26 11:12:44   encoder_pnl/initial_state_fw:0 (256,)
12/26 11:12:44   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/26 11:12:44   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/26 11:12:44   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/26 11:12:44   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/26 11:12:44   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/26 11:12:44   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/26 11:12:44   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/26 11:12:44   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/26 11:12:44   global_step:0 ()
12/26 11:12:44   learning_rate:0 ()
12/26 11:12:44 number of parameters: 44.90M
WARNING:tensorflow:From /root/icpc/icpc/translate/translation_model.py:666: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

12/26 11:12:46 global step: 0
12/26 11:12:46 baseline step: 0
12/26 11:12:46 reading training data
12/26 11:12:46 total line count: 156721
12/26 11:12:51   lines read: 100000
12/26 11:12:55 files: data/gooddata/4_fold/train.code data/gooddata/4_fold/train.pnl data/gooddata/4_fold/train.nl
12/26 11:12:55 lines reads: 156721
12/26 11:12:55 reading development data
12/26 11:12:55 files: data/gooddata/4_fold/test.code data/gooddata/4_fold/test.pnl data/gooddata/4_fold/test.nl
12/26 11:12:55 lines reads: 17413
12/26 11:12:56 starting training
12/26 11:41:10 step 2000 epoch 1 learning rate 0.5 step-time 0.845 loss 80.820
12/26 11:41:10 starting evaluation
12/26 11:45:10 test bleu=1.08 loss=64.99 penalty=0.611 ratio=0.670
12/26 11:45:10 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 11:45:10 finished saving model
12/26 11:45:10 new best model
12/26 11:51:36   decaying learning rate to: 0.475
12/26 12:13:34 step 4000 epoch 2 learning rate 0.475 step-time 0.850 loss 59.913
12/26 12:13:34 starting evaluation
12/26 12:18:49 test bleu=2.72 loss=56.07 penalty=1.000 ratio=1.865
12/26 12:18:49 saving model to models/4_fold_hybrid_pnl/checkpoints
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
12/26 12:18:49 finished saving model
12/26 12:18:49 new best model
12/26 12:31:45   decaying learning rate to: 0.451
12/26 12:47:16 step 6000 epoch 3 learning rate 0.451 step-time 0.851 loss 52.287
12/26 12:47:16 starting evaluation
12/26 12:52:29 test bleu=9.21 loss=50.32 penalty=0.980 ratio=0.980
12/26 12:52:29 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 12:52:29 finished saving model
12/26 12:52:29 new best model
12/26 13:11:51   decaying learning rate to: 0.429
12/26 13:20:54 step 8000 epoch 4 learning rate 0.429 step-time 0.850 loss 46.821
12/26 13:20:54 starting evaluation
12/26 13:26:07 test bleu=12.14 loss=47.05 penalty=0.977 ratio=0.978
12/26 13:26:07 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 13:26:08 finished saving model
12/26 13:26:08 new best model
12/26 13:51:41   decaying learning rate to: 0.407
12/26 13:54:34 step 10000 epoch 5 learning rate 0.407 step-time 0.851 loss 42.724
12/26 13:54:34 starting evaluation
12/26 13:59:35 test bleu=15.33 loss=43.51 penalty=0.836 ratio=0.848
12/26 13:59:35 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 13:59:35 finished saving model
12/26 13:59:35 new best model
12/26 14:27:57 step 12000 epoch 5 learning rate 0.407 step-time 0.849 loss 39.228
12/26 14:27:57 starting evaluation
12/26 14:32:34 test bleu=16.51 loss=41.52 penalty=0.721 ratio=0.754
12/26 14:32:34 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 14:32:34 finished saving model
12/26 14:32:34 new best model
12/26 14:36:06   decaying learning rate to: 0.387
12/26 15:00:59 step 14000 epoch 6 learning rate 0.387 step-time 0.851 loss 35.962
12/26 15:00:59 starting evaluation
12/26 15:05:57 test bleu=19.62 loss=39.91 penalty=0.841 ratio=0.853
12/26 15:05:57 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 15:05:57 finished saving model
12/26 15:05:57 new best model
12/26 15:15:57   decaying learning rate to: 0.368
12/26 15:34:22 step 16000 epoch 7 learning rate 0.368 step-time 0.850 loss 33.664
12/26 15:34:22 starting evaluation
12/26 15:39:36 test bleu=21.43 loss=38.93 penalty=1.000 ratio=1.014
12/26 15:39:36 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 15:39:37 finished saving model
12/26 15:39:37 new best model
12/26 15:56:02   decaying learning rate to: 0.349
12/26 16:08:03 step 18000 epoch 8 learning rate 0.349 step-time 0.851 loss 31.240
12/26 16:08:03 starting evaluation
12/26 16:13:00 test bleu=23.41 loss=38.20 penalty=0.849 ratio=0.859
12/26 16:13:00 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 16:13:00 finished saving model
12/26 16:13:00 new best model
12/26 16:35:54   decaying learning rate to: 0.332
12/26 16:41:26 step 20000 epoch 9 learning rate 0.332 step-time 0.851 loss 29.397
12/26 16:41:26 starting evaluation
12/26 16:46:21 test bleu=24.63 loss=37.67 penalty=0.852 ratio=0.862
12/26 16:46:21 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 16:46:21 finished saving model
12/26 16:46:21 new best model
12/26 17:14:47 step 22000 epoch 9 learning rate 0.332 step-time 0.851 loss 27.331
12/26 17:14:47 starting evaluation
12/26 17:19:54 test bleu=25.18 loss=37.01 penalty=0.920 ratio=0.923
12/26 17:19:54 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 17:19:54 finished saving model
12/26 17:19:54 new best model
12/26 17:20:30   decaying learning rate to: 0.315
12/26 17:48:19 step 24000 epoch 10 learning rate 0.315 step-time 0.850 loss 24.665
12/26 17:48:19 starting evaluation
12/26 17:53:22 test bleu=26.48 loss=36.86 penalty=0.900 ratio=0.905
12/26 17:53:22 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 17:53:22 finished saving model
12/26 17:53:22 new best model
12/26 18:00:26   decaying learning rate to: 0.299
12/26 18:21:50 step 26000 epoch 11 learning rate 0.299 step-time 0.852 loss 22.935
12/26 18:21:50 starting evaluation
12/26 18:26:44 test bleu=27.19 loss=37.32 penalty=0.833 ratio=0.845
12/26 18:26:44 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 18:26:44 finished saving model
12/26 18:26:44 new best model
12/26 18:40:17   decaying learning rate to: 0.284
12/26 18:55:13 step 28000 epoch 12 learning rate 0.284 step-time 0.852 loss 21.146
12/26 18:55:13 starting evaluation
12/26 19:00:18 test bleu=28.31 loss=37.96 penalty=0.889 ratio=0.895
12/26 19:00:18 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 19:00:19 finished saving model
12/26 19:00:19 new best model
12/26 19:20:17   decaying learning rate to: 0.27
12/26 19:28:46 step 30000 epoch 13 learning rate 0.27 step-time 0.852 loss 19.590
12/26 19:28:46 starting evaluation
12/26 19:33:46 test bleu=29.21 loss=39.00 penalty=0.894 ratio=0.899
12/26 19:33:46 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 19:33:47 finished saving model
12/26 19:33:47 new best model
12/26 19:59:56   decaying learning rate to: 0.257
12/26 20:02:12 step 32000 epoch 14 learning rate 0.257 step-time 0.851 loss 18.232
12/26 20:02:12 starting evaluation
12/26 20:07:10 test bleu=29.66 loss=39.39 penalty=0.879 ratio=0.886
12/26 20:07:10 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 20:07:11 finished saving model
12/26 20:07:11 new best model
12/26 20:35:39 step 34000 epoch 14 learning rate 0.257 step-time 0.852 loss 16.218
12/26 20:35:39 starting evaluation
12/26 20:40:46 test bleu=30.59 loss=38.91 penalty=0.930 ratio=0.933
12/26 20:40:46 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 20:40:46 finished saving model
12/26 20:40:46 new best model
12/26 20:44:52   decaying learning rate to: 0.244
12/26 21:09:12 step 36000 epoch 15 learning rate 0.244 step-time 0.851 loss 14.630
12/26 21:09:12 starting evaluation
12/26 21:14:11 test bleu=30.65 loss=40.65 penalty=0.876 ratio=0.883
12/26 21:14:11 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 21:14:11 finished saving model
12/26 21:14:11 new best model
12/26 21:24:45   decaying learning rate to: 0.232
12/26 21:42:36 step 38000 epoch 16 learning rate 0.232 step-time 0.850 loss 13.309
12/26 21:42:36 starting evaluation
12/26 21:47:43 test bleu=31.14 loss=42.78 penalty=0.923 ratio=0.926
12/26 21:47:43 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 21:47:44 finished saving model
12/26 21:47:44 new best model
12/26 22:04:43   decaying learning rate to: 0.22
12/26 22:16:13 step 40000 epoch 17 learning rate 0.22 step-time 0.853 loss 12.240
12/26 22:16:13 starting evaluation
12/26 22:21:18 test bleu=31.55 loss=44.79 penalty=0.916 ratio=0.919
12/26 22:21:18 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 22:21:18 finished saving model
12/26 22:21:18 new best model
12/26 22:44:47   decaying learning rate to: 0.209
12/26 22:49:45 step 42000 epoch 18 learning rate 0.209 step-time 0.851 loss 11.220
12/26 22:49:45 starting evaluation
12/26 22:54:47 test bleu=32.05 loss=46.80 penalty=0.909 ratio=0.913
12/26 22:54:47 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 22:54:47 finished saving model
12/26 22:54:47 new best model
12/26 23:19:22 step 44000 epoch 18 learning rate 0.209 step-time 0.736 loss 10.082
12/26 23:19:22 starting evaluation
12/26 23:22:46 test bleu=32.78 loss=45.52 penalty=0.958 ratio=0.959
12/26 23:22:46 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 23:22:46 finished saving model
12/26 23:22:46 new best model
12/26 23:23:42   decaying learning rate to: 0.199
12/26 23:45:41 step 46000 epoch 19 learning rate 0.199 step-time 0.686 loss 8.626
12/26 23:45:41 starting evaluation
12/26 23:49:05 test bleu=32.95 loss=48.83 penalty=0.960 ratio=0.961
12/26 23:49:05 saving model to models/4_fold_hybrid_pnl/checkpoints
12/26 23:49:05 finished saving model
12/26 23:49:05 new best model
12/26 23:55:10   decaying learning rate to: 0.189
12/27 00:12:02 step 48000 epoch 20 learning rate 0.189 step-time 0.686 loss 7.802
12/27 00:12:02 starting evaluation
12/27 00:15:22 test bleu=33.45 loss=50.33 penalty=0.954 ratio=0.955
12/27 00:15:22 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 00:15:22 finished saving model
12/27 00:15:22 new best model
12/27 00:26:38   decaying learning rate to: 0.179
12/27 00:38:20 step 50000 epoch 21 learning rate 0.179 step-time 0.687 loss 7.095
12/27 00:38:20 starting evaluation
12/27 00:41:40 test bleu=33.27 loss=53.57 penalty=0.929 ratio=0.932
12/27 00:41:40 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 00:41:40 finished saving model
12/27 00:58:03   decaying learning rate to: 0.17
12/27 01:04:34 step 52000 epoch 22 learning rate 0.17 step-time 0.685 loss 6.417
12/27 01:04:34 starting evaluation
12/27 01:08:00 test bleu=33.62 loss=56.31 penalty=0.973 ratio=0.973
12/27 01:08:00 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 01:08:00 finished saving model
12/27 01:08:00 new best model
12/27 01:29:33   decaying learning rate to: 0.162
12/27 01:30:57 step 54000 epoch 23 learning rate 0.162 step-time 0.686 loss 5.818
12/27 01:30:57 starting evaluation
12/27 01:34:14 test bleu=33.53 loss=58.18 penalty=0.880 ratio=0.887
12/27 01:34:14 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 01:34:14 finished saving model
12/27 01:57:13 step 56000 epoch 23 learning rate 0.162 step-time 0.687 loss 4.921
12/27 01:57:13 starting evaluation
12/27 02:00:36 test bleu=34.23 loss=58.56 penalty=0.950 ratio=0.952
12/27 02:00:36 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 02:00:36 finished saving model
12/27 02:00:36 new best model
12/27 02:04:21   decaying learning rate to: 0.154
12/27 02:23:30 step 58000 epoch 24 learning rate 0.154 step-time 0.685 loss 4.350
12/27 02:23:30 starting evaluation
12/27 02:26:53 test bleu=34.42 loss=61.69 penalty=0.961 ratio=0.962
12/27 02:26:53 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 02:26:53 finished saving model
12/27 02:26:53 new best model
12/27 02:35:48   decaying learning rate to: 0.146
12/27 02:49:49 step 60000 epoch 25 learning rate 0.146 step-time 0.686 loss 3.893
12/27 02:49:49 starting evaluation
12/27 02:53:16 test bleu=34.43 loss=64.08 penalty=0.974 ratio=0.975
12/27 02:53:16 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 02:53:16 finished saving model
12/27 02:53:16 new best model
12/27 03:07:20   decaying learning rate to: 0.139
12/27 03:16:10 step 62000 epoch 26 learning rate 0.139 step-time 0.685 loss 3.558
12/27 03:16:10 starting evaluation
12/27 03:19:37 test bleu=33.17 loss=67.92 penalty=1.000 ratio=1.035
12/27 03:19:37 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 03:19:37 finished saving model
12/27 03:38:49   decaying learning rate to: 0.132
12/27 03:42:33 step 64000 epoch 27 learning rate 0.132 step-time 0.686 loss 3.198
12/27 03:42:33 starting evaluation
12/27 03:45:57 test bleu=34.88 loss=69.19 penalty=0.992 ratio=0.992
12/27 03:45:57 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 03:45:57 finished saving model
12/27 03:45:57 new best model
12/27 04:14:49 step 66000 epoch 27 learning rate 0.132 step-time 0.864 loss 2.820
12/27 04:14:49 starting evaluation
12/27 04:19:38 test bleu=35.16 loss=70.32 penalty=0.993 ratio=0.993
12/27 04:19:38 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 04:19:39 finished saving model
12/27 04:19:39 new best model
12/27 04:21:24   decaying learning rate to: 0.125
12/27 04:48:53 step 68000 epoch 28 learning rate 0.125 step-time 0.875 loss 2.393
12/27 04:48:53 starting evaluation
12/27 04:53:40 test bleu=35.35 loss=72.60 penalty=0.978 ratio=0.978
12/27 04:53:40 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 04:53:40 finished saving model
12/27 04:53:40 new best model
12/27 05:01:58   decaying learning rate to: 0.119
12/27 05:22:54 step 70000 epoch 29 learning rate 0.119 step-time 0.875 loss 2.175
12/27 05:22:54 starting evaluation
12/27 05:27:37 test bleu=35.20 loss=75.46 penalty=0.958 ratio=0.959
12/27 05:27:37 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 05:27:38 finished saving model
12/27 05:42:27   decaying learning rate to: 0.113
12/27 05:56:48 step 72000 epoch 30 learning rate 0.113 step-time 0.873 loss 1.970
12/27 05:56:48 starting evaluation
12/27 06:01:30 test bleu=35.24 loss=78.02 penalty=0.987 ratio=0.987
12/27 06:01:30 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 06:01:30 finished saving model
12/27 06:23:00   decaying learning rate to: 0.107
12/27 06:30:42 step 74000 epoch 31 learning rate 0.107 step-time 0.874 loss 1.800
12/27 06:30:42 starting evaluation
12/27 06:35:30 test bleu=35.46 loss=79.66 penalty=0.974 ratio=0.974
12/27 06:35:30 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 06:35:30 finished saving model
12/27 06:35:30 new best model
12/27 07:03:29   decaying learning rate to: 0.102
12/27 07:04:40 step 76000 epoch 32 learning rate 0.102 step-time 0.873 loss 1.633
12/27 07:04:40 starting evaluation
12/27 07:09:30 test bleu=35.68 loss=81.08 penalty=0.993 ratio=0.994
12/27 07:09:30 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 07:09:30 finished saving model
12/27 07:09:30 new best model
12/27 07:38:40 step 78000 epoch 32 learning rate 0.102 step-time 0.873 loss 1.393
12/27 07:38:40 starting evaluation
12/27 07:43:30 test bleu=34.83 loss=83.99 penalty=1.000 ratio=1.024
12/27 07:43:30 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 07:43:31 finished saving model
12/27 07:48:43   decaying learning rate to: 0.0969
12/27 08:12:40 step 80000 epoch 33 learning rate 0.0969 step-time 0.873 loss 1.261
12/27 08:12:40 starting evaluation
12/27 08:17:29 test bleu=35.44 loss=84.95 penalty=1.000 ratio=1.010
12/27 08:17:29 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 08:17:29 finished saving model
12/27 08:29:19   decaying learning rate to: 0.092
12/27 08:46:34 step 82000 epoch 34 learning rate 0.092 step-time 0.871 loss 1.145
12/27 08:46:34 starting evaluation
12/27 08:51:26 test bleu=35.71 loss=87.59 penalty=0.993 ratio=0.993
12/27 08:51:26 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 08:51:26 finished saving model
12/27 08:51:26 new best model
12/27 09:09:48   decaying learning rate to: 0.0874
12/27 09:20:33 step 84000 epoch 35 learning rate 0.0874 step-time 0.871 loss 1.081
12/27 09:20:33 starting evaluation
12/27 09:25:28 test bleu=34.94 loss=88.99 penalty=1.000 ratio=1.025
12/27 09:25:28 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 09:25:28 finished saving model
12/27 09:50:26   decaying learning rate to: 0.083
12/27 09:54:38 step 86000 epoch 36 learning rate 0.083 step-time 0.873 loss 0.988
12/27 09:54:38 starting evaluation
12/27 09:59:32 test bleu=35.09 loss=90.40 penalty=1.000 ratio=1.022
12/27 09:59:32 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 09:59:32 finished saving model
12/27 10:28:38 step 88000 epoch 36 learning rate 0.083 step-time 0.871 loss 0.897
12/27 10:28:38 starting evaluation
12/27 10:33:32 test bleu=36.11 loss=91.19 penalty=0.999 ratio=0.999
12/27 10:33:32 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 10:33:32 finished saving model
12/27 10:33:32 new best model
12/27 10:35:49   decaying learning rate to: 0.0789
12/27 11:02:38 step 90000 epoch 37 learning rate 0.0789 step-time 0.871 loss 0.788
12/27 11:02:38 starting evaluation
12/27 11:07:38 test bleu=36.03 loss=92.87 penalty=0.995 ratio=0.995
12/27 11:07:38 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 11:07:38 finished saving model
12/27 11:16:25   decaying learning rate to: 0.0749
12/27 11:36:38 step 92000 epoch 38 learning rate 0.0749 step-time 0.868 loss 0.742
12/27 11:36:38 starting evaluation
12/27 11:41:41 test bleu=36.07 loss=94.54 penalty=0.998 ratio=0.998
12/27 11:41:42 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 11:41:42 finished saving model
12/27 11:56:58   decaying learning rate to: 0.0712
12/27 12:10:45 step 94000 epoch 39 learning rate 0.0712 step-time 0.870 loss 0.691
12/27 12:10:45 starting evaluation
12/27 12:15:46 test bleu=36.11 loss=95.39 penalty=0.995 ratio=0.995
12/27 12:15:46 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 12:15:46 finished saving model
12/27 12:37:39   decaying learning rate to: 0.0676
12/27 12:44:45 step 96000 epoch 40 learning rate 0.0676 step-time 0.868 loss 0.661
12/27 12:44:45 starting evaluation
12/27 12:49:46 test bleu=36.45 loss=95.98 penalty=0.993 ratio=0.993
12/27 12:49:46 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 12:49:46 finished saving model
12/27 12:49:46 new best model
12/27 13:18:12   decaying learning rate to: 0.0643
12/27 13:18:47 step 98000 epoch 41 learning rate 0.0643 step-time 0.868 loss 0.620
12/27 13:18:47 starting evaluation
12/27 13:23:49 test bleu=36.23 loss=97.52 penalty=0.990 ratio=0.990
12/27 13:23:49 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 13:23:49 finished saving model
12/27 13:52:54 step 100000 epoch 41 learning rate 0.0643 step-time 0.870 loss 0.545
12/27 13:52:54 starting evaluation
12/27 13:57:56 test bleu=36.30 loss=98.83 penalty=1.000 ratio=1.002
12/27 13:57:57 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 13:57:57 finished saving model
12/27 14:03:36   decaying learning rate to: 0.061
12/27 14:26:55 step 102000 epoch 42 learning rate 0.061 step-time 0.867 loss 0.521
12/27 14:26:55 starting evaluation
12/27 14:31:59 test bleu=35.98 loss=99.51 penalty=1.000 ratio=1.008
12/27 14:31:59 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 14:31:59 finished saving model
12/27 14:44:19   decaying learning rate to: 0.058
12/27 15:00:57 step 104000 epoch 43 learning rate 0.058 step-time 0.867 loss 0.491
12/27 15:00:57 starting evaluation
12/27 15:06:04 test bleu=35.75 loss=100.24 penalty=1.000 ratio=1.018
12/27 15:06:04 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 15:06:04 finished saving model
12/27 15:24:55   decaying learning rate to: 0.0551
12/27 15:35:04 step 106000 epoch 44 learning rate 0.0551 step-time 0.868 loss 0.471
12/27 15:35:04 starting evaluation
12/27 15:40:11 test bleu=36.63 loss=100.37 penalty=0.998 ratio=0.998
12/27 15:40:11 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 15:40:11 finished saving model
12/27 15:40:11 new best model
12/27 16:05:35   decaying learning rate to: 0.0523
12/27 16:09:10 step 108000 epoch 45 learning rate 0.0523 step-time 0.867 loss 0.452
12/27 16:09:10 starting evaluation
12/27 16:14:17 test bleu=36.25 loss=101.71 penalty=1.000 ratio=1.007
12/27 16:14:17 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 16:14:17 finished saving model
12/27 16:43:15 step 110000 epoch 45 learning rate 0.0523 step-time 0.867 loss 0.422
12/27 16:43:15 starting evaluation
12/27 16:48:24 test bleu=35.23 loss=101.32 penalty=1.000 ratio=1.040
12/27 16:48:24 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 16:48:24 finished saving model
12/27 16:51:12   decaying learning rate to: 0.0497
12/27 17:17:21 step 112000 epoch 46 learning rate 0.0497 step-time 0.866 loss 0.383
12/27 17:17:21 starting evaluation
12/27 17:22:28 test bleu=36.69 loss=101.88 penalty=0.996 ratio=0.996
12/27 17:22:28 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 17:22:28 finished saving model
12/27 17:22:28 new best model
12/27 17:31:49   decaying learning rate to: 0.0472
12/27 17:51:31 step 114000 epoch 47 learning rate 0.0472 step-time 0.869 loss 0.366
12/27 17:51:31 starting evaluation
12/27 17:56:39 test bleu=36.06 loss=102.61 penalty=1.000 ratio=1.017
12/27 17:56:39 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 17:56:39 finished saving model
12/27 18:12:25   decaying learning rate to: 0.0449
12/27 18:25:37 step 116000 epoch 48 learning rate 0.0449 step-time 0.867 loss 0.354
12/27 18:25:37 starting evaluation
12/27 18:30:47 test bleu=35.79 loss=102.85 penalty=1.000 ratio=1.026
12/27 18:30:47 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 18:30:47 finished saving model
12/27 18:53:12   decaying learning rate to: 0.0426
12/27 18:59:42 step 118000 epoch 49 learning rate 0.0426 step-time 0.865 loss 0.339
12/27 18:59:42 starting evaluation
12/27 19:04:49 test bleu=36.83 loss=102.39 penalty=1.000 ratio=1.000
12/27 19:04:49 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 19:04:49 finished saving model
12/27 19:04:49 new best model
12/27 19:33:48 step 120000 epoch 50 learning rate 0.0426 step-time 0.868 loss 0.323
12/27 19:33:48 starting evaluation
12/27 19:38:58 test bleu=36.49 loss=102.32 penalty=1.000 ratio=1.011
12/27 19:38:58 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 19:38:58 finished saving model
12/27 19:38:59   decaying learning rate to: 0.0405
12/27 20:07:55 step 122000 epoch 50 learning rate 0.0405 step-time 0.866 loss 0.285
12/27 20:07:55 starting evaluation
12/27 20:13:04 test bleu=36.79 loss=102.59 penalty=0.998 ratio=0.998
12/27 20:13:04 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 20:13:04 finished saving model
12/27 20:19:17   decaying learning rate to: 0.0385
12/27 20:42:02 step 124000 epoch 51 learning rate 0.0385 step-time 0.867 loss 0.274
12/27 20:42:02 starting evaluation
12/27 20:47:11 test bleu=36.41 loss=102.69 penalty=1.000 ratio=1.013
12/27 20:47:11 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 20:47:11 finished saving model
12/27 21:00:02   decaying learning rate to: 0.0365
12/27 21:16:06 step 126000 epoch 52 learning rate 0.0365 step-time 0.865 loss 0.255
12/27 21:16:06 starting evaluation
12/27 21:21:16 test bleu=36.68 loss=102.64 penalty=1.000 ratio=1.009
12/27 21:21:16 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 21:21:16 finished saving model
12/27 21:40:43   decaying learning rate to: 0.0347
12/27 21:50:15 step 128000 epoch 53 learning rate 0.0347 step-time 0.867 loss 0.249
12/27 21:50:15 starting evaluation
12/27 21:55:24 test bleu=37.09 loss=102.75 penalty=0.996 ratio=0.996
12/27 21:55:24 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 21:55:24 finished saving model
12/27 21:55:24 new best model
12/27 22:21:22   decaying learning rate to: 0.033
12/27 22:24:22 step 130000 epoch 54 learning rate 0.033 step-time 0.867 loss 0.235
12/27 22:24:22 starting evaluation
12/27 22:29:31 test bleu=37.01 loss=102.39 penalty=0.995 ratio=0.995
12/27 22:29:31 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 22:29:31 finished saving model
12/27 22:58:26 step 132000 epoch 54 learning rate 0.033 step-time 0.865 loss 0.223
12/27 22:58:26 starting evaluation
12/27 23:03:35 test bleu=36.62 loss=102.76 penalty=1.000 ratio=1.013
12/27 23:03:35 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 23:03:35 finished saving model
12/27 23:06:58   decaying learning rate to: 0.0313
12/27 23:32:32 step 134000 epoch 55 learning rate 0.0313 step-time 0.867 loss 0.200
12/27 23:32:32 starting evaluation
12/27 23:37:40 test bleu=36.78 loss=102.85 penalty=1.000 ratio=1.003
12/27 23:37:40 saving model to models/4_fold_hybrid_pnl/checkpoints
12/27 23:37:41 finished saving model
12/27 23:47:34   decaying learning rate to: 0.0298
12/28 00:06:37 step 136000 epoch 56 learning rate 0.0298 step-time 0.866 loss 0.193
12/28 00:06:37 starting evaluation
12/28 00:11:46 test bleu=37.06 loss=102.90 penalty=0.999 ratio=0.999
12/28 00:11:46 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 00:11:46 finished saving model
12/28 00:28:08   decaying learning rate to: 0.0283
12/28 00:40:43 step 138000 epoch 57 learning rate 0.0283 step-time 0.866 loss 0.190
12/28 00:40:43 starting evaluation
12/28 00:45:53 test bleu=36.68 loss=103.56 penalty=1.000 ratio=1.015
12/28 00:45:53 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 00:45:53 finished saving model
12/28 01:08:49   decaying learning rate to: 0.0269
12/28 01:14:44 step 140000 epoch 58 learning rate 0.0269 step-time 0.864 loss 0.185
12/28 01:14:44 starting evaluation
12/28 01:19:52 test bleu=37.07 loss=102.70 penalty=0.996 ratio=0.996
12/28 01:19:52 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 01:19:52 finished saving model
12/28 01:48:52 step 142000 epoch 58 learning rate 0.0269 step-time 0.868 loss 0.175
12/28 01:48:52 starting evaluation
12/28 01:54:01 test bleu=36.94 loss=103.22 penalty=1.000 ratio=1.001
12/28 01:54:01 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 01:54:01 finished saving model
12/28 01:54:38   decaying learning rate to: 0.0255
12/28 02:24:11 step 144000 epoch 59 learning rate 0.0255 step-time 0.903 loss 0.160
12/28 02:24:11 starting evaluation
12/28 02:29:26 test bleu=36.97 loss=103.38 penalty=0.998 ratio=0.998
12/28 02:29:26 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 02:29:27 finished saving model
12/28 02:36:38   decaying learning rate to: 0.0242
12/28 02:59:36 step 146000 epoch 60 learning rate 0.0242 step-time 0.902 loss 0.153
12/28 02:59:36 starting evaluation
12/28 03:04:51 test bleu=36.80 loss=103.78 penalty=1.000 ratio=1.009
12/28 03:04:51 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 03:04:52 finished saving model
12/28 03:18:46   decaying learning rate to: 0.023
12/28 03:34:52 step 148000 epoch 61 learning rate 0.023 step-time 0.898 loss 0.155
12/28 03:34:52 starting evaluation
12/28 03:40:07 test bleu=37.09 loss=103.42 penalty=0.996 ratio=0.996
12/28 03:40:07 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 03:40:08 finished saving model
12/28 03:40:08 new best model
12/28 04:01:09   decaying learning rate to: 0.0219
12/28 04:10:28 step 150000 epoch 62 learning rate 0.0219 step-time 0.908 loss 0.150
12/28 04:10:28 starting evaluation
12/28 04:15:44 test bleu=36.99 loss=103.61 penalty=1.000 ratio=1.007
12/28 04:15:44 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 04:15:44 finished saving model
12/28 04:43:17   decaying learning rate to: 0.0208
12/28 04:45:40 step 152000 epoch 63 learning rate 0.0208 step-time 0.896 loss 0.144
12/28 04:45:40 starting evaluation
12/28 04:50:48 test bleu=37.17 loss=104.13 penalty=0.989 ratio=0.989
12/28 04:50:48 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 04:50:48 finished saving model
12/28 04:50:48 new best model
12/28 05:19:43 step 154000 epoch 63 learning rate 0.0208 step-time 0.865 loss 0.134
12/28 05:19:43 starting evaluation
12/28 05:24:50 test bleu=37.09 loss=103.58 penalty=1.000 ratio=1.001
12/28 05:24:50 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 05:24:50 finished saving model
12/28 05:28:51   decaying learning rate to: 0.0197
12/28 05:53:47 step 156000 epoch 64 learning rate 0.0197 step-time 0.866 loss 0.130
12/28 05:53:47 starting evaluation
12/28 05:58:54 test bleu=37.19 loss=103.80 penalty=0.998 ratio=0.998
12/28 05:58:54 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 05:58:55 finished saving model
12/28 05:58:55 new best model
12/28 06:09:24   decaying learning rate to: 0.0188
12/28 06:28:04 step 158000 epoch 65 learning rate 0.0188 step-time 0.872 loss 0.124
12/28 06:28:04 starting evaluation
12/28 06:33:13 test bleu=37.35 loss=104.29 penalty=0.999 ratio=0.999
12/28 06:33:13 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 06:33:13 finished saving model
12/28 06:33:13 new best model
12/28 06:50:31   decaying learning rate to: 0.0178
12/28 07:02:37 step 160000 epoch 66 learning rate 0.0178 step-time 0.880 loss 0.123
12/28 07:02:37 starting evaluation
12/28 07:07:44 test bleu=37.27 loss=104.30 penalty=0.980 ratio=0.980
12/28 07:07:44 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 07:07:45 finished saving model
12/28 07:31:44   decaying learning rate to: 0.0169
12/28 07:37:11 step 162000 epoch 67 learning rate 0.0169 step-time 0.881 loss 0.123
12/28 07:37:11 starting evaluation
12/28 07:42:20 test bleu=37.26 loss=104.26 penalty=1.000 ratio=1.000
12/28 07:42:20 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 07:42:21 finished saving model
12/28 08:11:45 step 164000 epoch 67 learning rate 0.0169 step-time 0.880 loss 0.118
12/28 08:11:45 starting evaluation
12/28 08:16:55 test bleu=37.17 loss=104.11 penalty=0.995 ratio=0.995
12/28 08:16:55 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 08:16:55 finished saving model
12/28 08:18:08   decaying learning rate to: 0.0161
12/28 08:46:19 step 166000 epoch 68 learning rate 0.0161 step-time 0.880 loss 0.110
12/28 08:46:19 starting evaluation
12/28 08:51:29 test bleu=37.26 loss=104.68 penalty=1.000 ratio=1.000
12/28 08:51:29 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 08:51:29 finished saving model
12/28 08:59:08   decaying learning rate to: 0.0153
12/28 09:20:55 step 168000 epoch 69 learning rate 0.0153 step-time 0.881 loss 0.109
12/28 09:20:55 starting evaluation
12/28 09:26:04 test bleu=36.94 loss=104.43 penalty=1.000 ratio=1.009
12/28 09:26:04 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 09:26:04 finished saving model
12/28 09:40:20   decaying learning rate to: 0.0145
12/28 09:55:30 step 170000 epoch 70 learning rate 0.0145 step-time 0.881 loss 0.105
12/28 09:55:30 starting evaluation
12/28 10:00:38 test bleu=37.27 loss=104.98 penalty=1.000 ratio=1.001
12/28 10:00:38 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 10:00:38 finished saving model
12/28 10:21:35   decaying learning rate to: 0.0138
12/28 10:30:03 step 172000 epoch 71 learning rate 0.0138 step-time 0.880 loss 0.107
12/28 10:30:03 starting evaluation
12/28 10:35:12 test bleu=37.23 loss=105.09 penalty=1.000 ratio=1.001
12/28 10:35:12 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 10:35:13 finished saving model
12/28 11:02:47   decaying learning rate to: 0.0131
12/28 11:04:35 step 174000 epoch 72 learning rate 0.0131 step-time 0.879 loss 0.105
12/28 11:04:35 starting evaluation
12/28 11:09:45 test bleu=37.28 loss=105.26 penalty=0.998 ratio=0.998
12/28 11:09:45 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 11:09:45 finished saving model
12/28 11:39:10 step 176000 epoch 72 learning rate 0.0131 step-time 0.880 loss 0.097
12/28 11:39:10 starting evaluation
12/28 11:44:17 test bleu=37.30 loss=105.42 penalty=0.996 ratio=0.996
12/28 11:44:17 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 11:44:18 finished saving model
12/28 11:49:05   decaying learning rate to: 0.0124
12/28 12:13:24 step 178000 epoch 73 learning rate 0.0124 step-time 0.871 loss 0.096
12/28 12:13:24 starting evaluation
12/28 12:18:30 test bleu=37.19 loss=105.42 penalty=1.000 ratio=1.005
12/28 12:18:30 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 12:18:31 finished saving model
12/28 12:29:27   decaying learning rate to: 0.0118
12/28 12:47:00 step 180000 epoch 74 learning rate 0.0118 step-time 0.853 loss 0.096
12/28 12:47:00 starting evaluation
12/28 12:52:05 test bleu=37.30 loss=105.59 penalty=1.000 ratio=1.001
12/28 12:52:05 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 12:52:06 finished saving model
12/28 13:09:30   decaying learning rate to: 0.0112
12/28 13:20:39 step 182000 epoch 75 learning rate 0.0112 step-time 0.855 loss 0.093
12/28 13:20:39 starting evaluation
12/28 13:25:46 test bleu=37.25 loss=105.93 penalty=1.000 ratio=1.000
12/28 13:25:46 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 13:25:46 finished saving model
12/28 13:49:34   decaying learning rate to: 0.0107
12/28 13:54:16 step 184000 epoch 76 learning rate 0.0107 step-time 0.853 loss 0.092
12/28 13:54:16 starting evaluation
12/28 13:59:22 test bleu=37.32 loss=105.73 penalty=1.000 ratio=1.001
12/28 13:59:22 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 13:59:22 finished saving model
12/28 14:27:51 step 186000 epoch 76 learning rate 0.0107 step-time 0.853 loss 0.090
12/28 14:27:51 starting evaluation
12/28 14:32:56 test bleu=37.28 loss=105.80 penalty=0.994 ratio=0.994
12/28 14:32:56 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 14:32:56 finished saving model
12/28 14:34:43   decaying learning rate to: 0.0101
12/28 15:01:25 step 188000 epoch 77 learning rate 0.0101 step-time 0.852 loss 0.085
12/28 15:01:25 starting evaluation
12/28 15:06:32 test bleu=37.35 loss=106.01 penalty=0.998 ratio=0.998
12/28 15:06:32 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 15:06:32 finished saving model
12/28 15:06:32 new best model
12/28 15:14:33   decaying learning rate to: 0.00963
12/28 15:35:01 step 190000 epoch 78 learning rate 0.00963 step-time 0.852 loss 0.083
12/28 15:35:01 starting evaluation
12/28 15:40:08 test bleu=36.75 loss=106.23 penalty=1.000 ratio=1.015
12/28 15:40:08 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 15:40:08 finished saving model
12/28 15:54:30   decaying learning rate to: 0.00915
12/28 16:08:38 step 192000 epoch 79 learning rate 0.00915 step-time 0.853 loss 0.084
12/28 16:08:38 starting evaluation
12/28 16:13:43 test bleu=37.29 loss=106.49 penalty=1.000 ratio=1.001
12/28 16:13:43 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 16:13:44 finished saving model
12/28 16:34:59   decaying learning rate to: 0.00869
12/28 16:42:42 step 194000 epoch 80 learning rate 0.00869 step-time 0.867 loss 0.085
12/28 16:42:42 starting evaluation
12/28 16:47:49 test bleu=37.35 loss=106.50 penalty=0.993 ratio=0.993
12/28 16:47:49 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 16:47:49 finished saving model
12/28 17:15:38   decaying learning rate to: 0.00826
12/28 17:16:48 step 196000 epoch 81 learning rate 0.00826 step-time 0.868 loss 0.084
12/28 17:16:48 starting evaluation
12/28 17:21:58 test bleu=37.18 loss=106.31 penalty=1.000 ratio=1.005
12/28 17:21:58 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 17:21:58 finished saving model
12/28 17:50:53 step 198000 epoch 81 learning rate 0.00826 step-time 0.866 loss 0.079
12/28 17:50:53 starting evaluation
12/28 17:56:00 test bleu=37.26 loss=106.58 penalty=1.000 ratio=1.003
12/28 17:56:00 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 17:56:00 finished saving model
12/28 18:01:20   decaying learning rate to: 0.00784
12/28 18:24:01 step 200000 epoch 82 learning rate 0.00784 step-time 0.839 loss 0.075
12/28 18:24:01 starting evaluation
12/28 18:29:04 test bleu=37.28 loss=106.69 penalty=1.000 ratio=1.002
12/28 18:29:04 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 18:29:04 finished saving model
12/28 18:40:11   decaying learning rate to: 0.00745
12/28 18:56:50 step 202000 epoch 83 learning rate 0.00745 step-time 0.831 loss 0.078
12/28 18:56:50 starting evaluation
12/28 19:01:52 test bleu=37.21 loss=106.95 penalty=1.000 ratio=1.003
12/28 19:01:52 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 19:01:52 finished saving model
12/28 19:19:20   decaying learning rate to: 0.00708
12/28 19:29:40 step 204000 epoch 84 learning rate 0.00708 step-time 0.832 loss 0.077
12/28 19:29:40 starting evaluation
12/28 19:34:43 test bleu=37.30 loss=106.98 penalty=0.998 ratio=0.998
12/28 19:34:43 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 19:34:43 finished saving model
12/28 19:58:29   decaying learning rate to: 0.00673
12/28 20:02:30 step 206000 epoch 85 learning rate 0.00673 step-time 0.832 loss 0.076
12/28 20:02:30 starting evaluation
12/28 20:07:33 test bleu=36.93 loss=107.32 penalty=1.000 ratio=1.010
12/28 20:07:33 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 20:07:33 finished saving model
12/28 20:35:19 step 208000 epoch 85 learning rate 0.00673 step-time 0.831 loss 0.075
12/28 20:35:19 starting evaluation
12/28 20:40:23 test bleu=37.06 loss=107.09 penalty=1.000 ratio=1.006
12/28 20:40:23 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 20:40:23 finished saving model
12/28 20:42:41   decaying learning rate to: 0.00639
12/28 21:08:07 step 210000 epoch 86 learning rate 0.00639 step-time 0.830 loss 0.072
12/28 21:08:07 starting evaluation
12/28 21:13:09 test bleu=37.27 loss=107.43 penalty=1.000 ratio=1.003
12/28 21:13:09 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 21:13:09 finished saving model
12/28 21:21:39   decaying learning rate to: 0.00607
12/28 21:41:00 step 212000 epoch 87 learning rate 0.00607 step-time 0.833 loss 0.072
12/28 21:41:00 starting evaluation
12/28 21:46:03 test bleu=36.99 loss=107.56 penalty=1.000 ratio=1.008
12/28 21:46:03 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 21:46:03 finished saving model
12/28 22:00:37   decaying learning rate to: 0.00577
12/28 22:13:49 step 214000 epoch 88 learning rate 0.00577 step-time 0.831 loss 0.073
12/28 22:13:49 starting evaluation
12/28 22:18:52 test bleu=37.12 loss=107.55 penalty=1.000 ratio=1.006
12/28 22:18:52 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 22:18:52 finished saving model
12/28 22:39:47   decaying learning rate to: 0.00548
12/28 22:46:39 step 216000 epoch 89 learning rate 0.00548 step-time 0.832 loss 0.072
12/28 22:46:39 starting evaluation
12/28 22:51:43 test bleu=37.26 loss=107.63 penalty=1.000 ratio=1.001
12/28 22:51:43 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 22:51:44 finished saving model
12/28 23:18:56   decaying learning rate to: 0.0052
12/28 23:19:29 step 218000 epoch 90 learning rate 0.0052 step-time 0.831 loss 0.071
12/28 23:19:29 starting evaluation
12/28 23:24:32 test bleu=37.12 loss=107.68 penalty=1.000 ratio=1.005
12/28 23:24:32 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 23:24:32 finished saving model
12/28 23:52:20 step 220000 epoch 90 learning rate 0.0052 step-time 0.832 loss 0.068
12/28 23:52:20 starting evaluation
12/28 23:57:24 test bleu=37.25 loss=107.90 penalty=1.000 ratio=1.003
12/28 23:57:24 saving model to models/4_fold_hybrid_pnl/checkpoints
12/28 23:57:24 finished saving model
12/29 00:03:10   decaying learning rate to: 0.00494
12/29 00:25:10 step 222000 epoch 91 learning rate 0.00494 step-time 0.831 loss 0.068
12/29 00:25:10 starting evaluation
12/29 00:30:13 test bleu=37.17 loss=107.85 penalty=1.000 ratio=1.005
12/29 00:30:13 saving model to models/4_fold_hybrid_pnl/checkpoints
12/29 00:30:13 finished saving model
12/29 00:42:01   decaying learning rate to: 0.0047
12/29 00:57:59 step 224000 epoch 92 learning rate 0.0047 step-time 0.831 loss 0.069
12/29 00:57:59 starting evaluation
12/29 01:03:01 test bleu=37.31 loss=107.98 penalty=1.000 ratio=1.001
12/29 01:03:01 saving model to models/4_fold_hybrid_pnl/checkpoints
12/29 01:03:01 finished saving model
12/29 01:21:03   decaying learning rate to: 0.00446
12/29 01:30:46 step 226000 epoch 93 learning rate 0.00446 step-time 0.830 loss 0.067
12/29 01:30:46 starting evaluation
12/29 01:35:50 test bleu=37.15 loss=108.08 penalty=1.000 ratio=1.004
12/29 01:35:50 saving model to models/4_fold_hybrid_pnl/checkpoints
12/29 01:35:50 finished saving model
12/29 02:00:20   decaying learning rate to: 0.00424
12/29 02:03:57 step 228000 epoch 94 learning rate 0.00424 step-time 0.842 loss 0.067
12/29 02:03:57 starting evaluation
12/29 02:09:06 test bleu=37.34 loss=108.09 penalty=0.997 ratio=0.997
12/29 02:09:06 saving model to models/4_fold_hybrid_pnl/checkpoints
12/29 02:09:06 finished saving model
12/29 02:39:05 step 230000 epoch 94 learning rate 0.00424 step-time 0.897 loss 0.067
12/29 02:39:05 starting evaluation
12/29 02:44:19 test bleu=37.12 loss=108.23 penalty=1.000 ratio=1.005
12/29 02:44:19 saving model to models/4_fold_hybrid_pnl/checkpoints
12/29 02:44:19 finished saving model
12/29 02:47:32   decaying learning rate to: 0.00403
12/29 03:14:15 step 232000 epoch 95 learning rate 0.00403 step-time 0.896 loss 0.065
12/29 03:14:15 starting evaluation
12/29 03:19:24 test bleu=37.28 loss=108.42 penalty=1.000 ratio=1.001
12/29 03:19:24 saving model to models/4_fold_hybrid_pnl/checkpoints
12/29 03:19:24 finished saving model
12/29 03:29:15   decaying learning rate to: 0.00383
12/29 03:47:59 step 234000 epoch 96 learning rate 0.00383 step-time 0.856 loss 0.065
12/29 03:47:59 starting evaluation
12/29 03:53:02 test bleu=37.31 loss=108.31 penalty=0.999 ratio=0.999
12/29 03:53:02 saving model to models/4_fold_hybrid_pnl/checkpoints
12/29 03:53:02 finished saving model
12/29 04:08:11   decaying learning rate to: 0.00363
12/29 04:20:47 step 236000 epoch 97 learning rate 0.00363 step-time 0.831 loss 0.065
12/29 04:20:47 starting evaluation
12/29 04:25:51 test bleu=37.11 loss=108.41 penalty=1.000 ratio=1.005
12/29 04:25:51 saving model to models/4_fold_hybrid_pnl/checkpoints
12/29 04:25:51 finished saving model
12/29 04:48:21   decaying learning rate to: 0.00345
12/29 04:54:56 step 238000 epoch 98 learning rate 0.00345 step-time 0.870 loss 0.066
12/29 04:54:56 starting evaluation
12/29 05:00:04 test bleu=37.27 loss=108.56 penalty=1.000 ratio=1.001
12/29 05:00:04 saving model to models/4_fold_hybrid_pnl/checkpoints
12/29 05:00:04 finished saving model
12/29 05:29:10 step 240000 epoch 99 learning rate 0.00345 step-time 0.871 loss 0.065
12/29 05:29:10 starting evaluation
12/29 05:34:18 test bleu=36.85 loss=108.59 penalty=1.000 ratio=1.011
12/29 05:34:18 saving model to models/4_fold_hybrid_pnl/checkpoints
12/29 05:34:18 finished saving model
12/29 05:34:20   decaying learning rate to: 0.00328
12/29 06:03:22 step 242000 epoch 99 learning rate 0.00328 step-time 0.870 loss 0.063
12/29 06:03:22 starting evaluation
12/29 06:08:30 test bleu=37.16 loss=108.63 penalty=1.000 ratio=1.003
12/29 06:08:30 saving model to models/4_fold_hybrid_pnl/checkpoints
12/29 06:08:31 finished saving model
12/29 06:15:10   decaying learning rate to: 0.00312
12/29 06:37:31 step 244000 epoch 100 learning rate 0.00312 step-time 0.868 loss 0.063
12/29 06:37:31 starting evaluation
12/29 06:42:42 test bleu=37.07 loss=108.77 penalty=1.000 ratio=1.006
12/29 06:42:42 saving model to models/4_fold_hybrid_pnl/checkpoints
12/29 06:42:43 finished saving model
12/29 06:55:27 finished training
12/29 06:55:27 exiting...
12/29 06:55:27 saving model to models/4_fold_hybrid_pnl/checkpoints
12/29 06:55:27 finished saving model
