nohup: ignoring input
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:107: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:30: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

12/26 11:22:16 label: default
12/26 11:22:16 description:
  default configuration
  next line of description
  last line
12/26 11:22:16 /root/icpc/icpc/translate/__main__.py config/10-folds/5_fold/codenn/config.yaml --train -v
12/26 11:22:16 commit hash 74e0554cb3eb5df835cef993ad570ff8de651f71
12/26 11:22:16 tensorflow version: 1.14.0
12/26 11:22:16 program arguments
12/26 11:22:16   aggregation_method   'sum'
12/26 11:22:16   align_encoder_id     0
12/26 11:22:16   allow_growth         True
12/26 11:22:16   attention_type       'global'
12/26 11:22:16   attn_filter_length   0
12/26 11:22:16   attn_filters         0
12/26 11:22:16   attn_prev_word       False
12/26 11:22:16   attn_size            128
12/26 11:22:16   attn_temperature     1.0
12/26 11:22:16   attn_window_size     0
12/26 11:22:16   average              False
12/26 11:22:16   baseline_activation  None
12/26 11:22:16   baseline_learning_rate 0.001
12/26 11:22:16   baseline_optimizer   'adam'
12/26 11:22:16   baseline_steps       0
12/26 11:22:16   batch_mode           'standard'
12/26 11:22:16   batch_size           64
12/26 11:22:16   beam_size            5
12/26 11:22:16   bidir                True
12/26 11:22:16   bidir_projection     False
12/26 11:22:16   binary               False
12/26 11:22:16   cell_size            256
12/26 11:22:16   cell_type            'GRU'
12/26 11:22:16   character_level      False
12/26 11:22:16   checkpoints          []
12/26 11:22:16   conditional_rnn      False
12/26 11:22:16   config               'config/10-folds/5_fold/codenn/config.yaml'
12/26 11:22:16   convolutions         None
12/26 11:22:16   data_dir             'data/gooddata/5_fold'
12/26 11:22:16   debug                False
12/26 11:22:16   decay_after_n_epoch  1
12/26 11:22:16   decay_every_n_epoch  1
12/26 11:22:16   decay_if_no_progress None
12/26 11:22:16   decoders             [{'max_len': 40, 'name': 'nl'}]
12/26 11:22:16   description          'default configuration\nnext line of description\nlast line\n'
12/26 11:22:16   dev_prefix           'test'
12/26 11:22:16   early_stopping       True
12/26 11:22:16   embedding_dropout    0.0
12/26 11:22:16   embedding_initializer None
12/26 11:22:16   embedding_size       256
12/26 11:22:16   embedding_weight_scale None
12/26 11:22:16   embeddings_on_cpu    True
12/26 11:22:16   encoders             [{'attention_type': 'global', 'max_len': 200, 'name': 'code'}]
12/26 11:22:16   ensemble             False
12/26 11:22:16   eval_burn_in         0
12/26 11:22:16   feed_previous        0.0
12/26 11:22:16   final_state          'last'
12/26 11:22:16   freeze_variables     []
12/26 11:22:16   generate_first       True
12/26 11:22:16   gpu_id               1
12/26 11:22:16   highway_layers       0
12/26 11:22:16   initial_state_dropout 0.0
12/26 11:22:16   initializer          None
12/26 11:22:16   input_layer_dropout  0.0
12/26 11:22:16   input_layers         None
12/26 11:22:16   keep_best            5
12/26 11:22:16   keep_every_n_hours   0
12/26 11:22:16   label                'default'
12/26 11:22:16   layer_norm           False
12/26 11:22:16   layers               1
12/26 11:22:16   learning_rate        0.5
12/26 11:22:16   learning_rate_decay_factor 0.95
12/26 11:22:16   len_normalization    1.0
12/26 11:22:16   log_file             'log.txt'
12/26 11:22:16   loss_function        'xent'
12/26 11:22:16   max_dev_size         0
12/26 11:22:16   max_epochs           100
12/26 11:22:16   max_gradient_norm    5.0
12/26 11:22:16   max_len              50
12/26 11:22:16   max_steps            600000
12/26 11:22:16   max_test_size        0
12/26 11:22:16   max_to_keep          1
12/26 11:22:16   max_train_size       0
12/26 11:22:16   maxout_stride        None
12/26 11:22:16   mem_fraction         1.0
12/26 11:22:16   min_learning_rate    1e-06
12/26 11:22:16   model_dir            'models/5_fold_codenn'
12/26 11:22:16   moving_average       None
12/26 11:22:16   no_gpu               False
12/26 11:22:16   optimizer            'sgd'
12/26 11:22:16   orthogonal_init      False
12/26 11:22:16   output               None
12/26 11:22:16   output_dropout       0.0
12/26 11:22:16   parallel_iterations  16
12/26 11:22:16   pervasive_dropout    False
12/26 11:22:16   pooling_avg          True
12/26 11:22:16   post_process_script  None
12/26 11:22:16   pred_deep_layer      False
12/26 11:22:16   pred_edits           False
12/26 11:22:16   pred_embed_proj      True
12/26 11:22:16   pred_maxout_layer    True
12/26 11:22:16   purge                False
12/26 11:22:16   raw_output           False
12/26 11:22:16   read_ahead           1
12/26 11:22:16   reconstruction_attn_weight 0.05
12/26 11:22:16   reconstruction_decoders False
12/26 11:22:16   reconstruction_weight 1.0
12/26 11:22:16   reinforce_after_n_epoch None
12/26 11:22:16   remove_unk           False
12/26 11:22:16   reverse              False
12/26 11:22:16   reverse_input        True
12/26 11:22:16   reward_function      'sentence_bleu'
12/26 11:22:16   rnn_feed_attn        True
12/26 11:22:16   rnn_input_dropout    0.0
12/26 11:22:16   rnn_output_dropout   0.0
12/26 11:22:16   rnn_state_dropout    0.0
12/26 11:22:16   save                 False
12/26 11:22:16   score_function       'corpus_bleu'
12/26 11:22:16   score_functions      ['bleu', 'loss']
12/26 11:22:16   script_dir           'scripts'
12/26 11:22:16   sgd_after_n_epoch    None
12/26 11:22:16   sgd_learning_rate    1.0
12/26 11:22:16   shuffle              True
12/26 11:22:16   softmax_temperature  1.0
12/26 11:22:16   steps_per_checkpoint 2000
12/26 11:22:16   steps_per_eval       2000
12/26 11:22:16   swap_memory          True
12/26 11:22:16   tie_embeddings       False
12/26 11:22:16   time_pooling         None
12/26 11:22:16   train                True
12/26 11:22:16   train_initial_states True
12/26 11:22:16   train_prefix         'train'
12/26 11:22:16   truncate_lines       True
12/26 11:22:16   update_first         False
12/26 11:22:16   use_baseline         False
12/26 11:22:16   use_dropout          False
12/26 11:22:16   use_lstm_full_state  False
12/26 11:22:16   use_previous_word    True
12/26 11:22:16   verbose              True
12/26 11:22:16   vocab_prefix         'vocab'
12/26 11:22:16   weight_scale         None
12/26 11:22:16   word_dropout         0.0
12/26 11:22:16 python random seed: 4539865653813758762
12/26 11:22:16 tf random seed:     927182339472904057
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:203: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

12/26 11:22:16 creating model
12/26 11:22:16 using device: /gpu:1
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:230: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

12/26 11:22:16 copying vocab to models/5_fold_codenn/data/vocab.code
12/26 11:22:16 copying vocab to models/5_fold_codenn/data/vocab.nl
12/26 11:22:16 reading vocabularies
12/26 11:22:16 creating model
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:60: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:111: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:33: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fc6b4f224a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fc6b4f224a8>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:226: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fc6b4f22a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fc6b4f22a90>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc6b4f17d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc6b4f17d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:838: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc739d91f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc739d91f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc739d91f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc739d91f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:432: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:435: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc739ad4e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc739ad4e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc739a8e7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc739a8e7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc739ae5cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc739ae5cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc739845048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc739845048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc73985d240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc73985d240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:919: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fc73c3f74e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fc73c3f74e0>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:131: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:223: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fc6e034e978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fc6e034e978>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc6e02ebb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc6e02ebb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc6e0296ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc6e0296ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc6e0296f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc6e0296f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc6e0382e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc6e0382e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc6e0382e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc6e0382e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
12/26 11:22:21 model parameters (30)
12/26 11:22:21   baseline_step:0 ()
12/26 11:22:21   decoder_nl/attention_code/U_a/kernel:0 (512, 128)
12/26 11:22:21   decoder_nl/attention_code/W_a/bias:0 (128,)
12/26 11:22:21   decoder_nl/attention_code/W_a/kernel:0 (256, 128)
12/26 11:22:21   decoder_nl/attention_code/v_a:0 (128,)
12/26 11:22:21   decoder_nl/code/initial_state_projection/bias:0 (256,)
12/26 11:22:21   decoder_nl/code/initial_state_projection/kernel:0 (256, 256)
12/26 11:22:21   decoder_nl/gru_cell/candidate/bias:0 (256,)
12/26 11:22:21   decoder_nl/gru_cell/candidate/kernel:0 (1024, 256)
12/26 11:22:21   decoder_nl/gru_cell/gates/bias:0 (512,)
12/26 11:22:21   decoder_nl/gru_cell/gates/kernel:0 (1024, 512)
12/26 11:22:21   decoder_nl/maxout/bias:0 (256,)
12/26 11:22:21   decoder_nl/maxout/kernel:0 (1024, 256)
12/26 11:22:21   decoder_nl/softmax0/kernel:0 (128, 256)
12/26 11:22:21   decoder_nl/softmax1/bias:0 (37966,)
12/26 11:22:21   decoder_nl/softmax1/kernel:0 (256, 37966)
12/26 11:22:21   embedding_code:0 (50000, 256)
12/26 11:22:21   embedding_nl:0 (37966, 256)
12/26 11:22:21   encoder_code/initial_state_bw:0 (256,)
12/26 11:22:21   encoder_code/initial_state_fw:0 (256,)
12/26 11:22:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/26 11:22:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/26 11:22:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/26 11:22:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/26 11:22:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/26 11:22:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/26 11:22:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/26 11:22:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/26 11:22:21   global_step:0 ()
12/26 11:22:21   learning_rate:0 ()
12/26 11:22:21 number of parameters: 34.31M
WARNING:tensorflow:From /root/icpc/icpc/translate/translation_model.py:666: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

12/26 11:22:22 global step: 0
12/26 11:22:22 baseline step: 0
12/26 11:22:22 reading training data
12/26 11:22:22 total line count: 156721
12/26 11:22:26   lines read: 100000
12/26 11:22:28 files: data/gooddata/5_fold/train.code data/gooddata/5_fold/train.nl
12/26 11:22:28 lines reads: 156721
12/26 11:22:28 reading development data
12/26 11:22:29 files: data/gooddata/5_fold/test.code data/gooddata/5_fold/test.nl
12/26 11:22:29 lines reads: 17413
12/26 11:22:29 starting training
12/26 11:44:56 step 2000 epoch 1 learning rate 0.5 step-time 0.671 loss 78.053
12/26 11:44:56 starting evaluation
12/26 11:49:10 test bleu=0.72 loss=63.23 penalty=1.000 ratio=2.047
12/26 11:49:10 saving model to models/5_fold_codenn/checkpoints
12/26 11:49:11 finished saving model
12/26 11:49:11 new best model
12/26 11:54:00   decaying learning rate to: 0.475
12/26 12:11:39 step 4000 epoch 2 learning rate 0.475 step-time 0.672 loss 58.975
12/26 12:11:39 starting evaluation
12/26 12:15:52 test bleu=2.14 loss=55.25 penalty=1.000 ratio=1.841
12/26 12:15:52 saving model to models/5_fold_codenn/checkpoints
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
12/26 12:15:52 finished saving model
12/26 12:15:52 new best model
12/26 12:25:50   decaying learning rate to: 0.451
12/26 12:38:23 step 6000 epoch 3 learning rate 0.451 step-time 0.674 loss 52.193
12/26 12:38:23 starting evaluation
12/26 12:42:28 test bleu=7.21 loss=51.30 penalty=0.744 ratio=0.771
12/26 12:42:28 saving model to models/5_fold_codenn/checkpoints
12/26 12:42:28 finished saving model
12/26 12:42:28 new best model
12/26 12:57:29   decaying learning rate to: 0.429
12/26 13:04:54 step 8000 epoch 4 learning rate 0.429 step-time 0.671 loss 47.867
12/26 13:04:54 starting evaluation
12/26 13:08:55 test bleu=9.43 loss=48.12 penalty=0.816 ratio=0.831
12/26 13:08:55 saving model to models/5_fold_codenn/checkpoints
12/26 13:08:55 finished saving model
12/26 13:08:55 new best model
12/26 13:29:02   decaying learning rate to: 0.407
12/26 13:31:21 step 10000 epoch 5 learning rate 0.407 step-time 0.671 loss 43.988
12/26 13:31:21 starting evaluation
12/26 13:35:14 test bleu=11.30 loss=45.54 penalty=0.771 ratio=0.793
12/26 13:35:14 saving model to models/5_fold_codenn/checkpoints
12/26 13:35:14 finished saving model
12/26 13:35:14 new best model
12/26 13:57:44 step 12000 epoch 5 learning rate 0.407 step-time 0.673 loss 40.841
12/26 13:57:44 starting evaluation
12/26 14:01:40 test bleu=12.66 loss=43.29 penalty=0.768 ratio=0.791
12/26 14:01:40 saving model to models/5_fold_codenn/checkpoints
12/26 14:01:40 finished saving model
12/26 14:01:40 new best model
12/26 14:04:17   decaying learning rate to: 0.387
12/26 14:24:11 step 14000 epoch 6 learning rate 0.387 step-time 0.673 loss 37.764
12/26 14:24:11 starting evaluation
12/26 14:28:10 test bleu=14.92 loss=42.08 penalty=0.812 ratio=0.827
12/26 14:28:10 saving model to models/5_fold_codenn/checkpoints
12/26 14:28:10 finished saving model
12/26 14:28:10 new best model
12/26 14:35:48   decaying learning rate to: 0.368
12/26 14:50:39 step 16000 epoch 7 learning rate 0.368 step-time 0.673 loss 35.436
12/26 14:50:39 starting evaluation
12/26 14:54:31 test bleu=16.31 loss=41.23 penalty=0.782 ratio=0.802
12/26 14:54:31 saving model to models/5_fold_codenn/checkpoints
12/26 14:54:32 finished saving model
12/26 14:54:32 new best model
12/26 15:07:13   decaying learning rate to: 0.349
12/26 15:16:59 step 18000 epoch 8 learning rate 0.349 step-time 0.672 loss 33.138
12/26 15:16:59 starting evaluation
12/26 15:21:00 test bleu=18.10 loss=40.53 penalty=0.819 ratio=0.833
12/26 15:21:00 saving model to models/5_fold_codenn/checkpoints
12/26 15:21:00 finished saving model
12/26 15:21:00 new best model
12/26 15:38:51   decaying learning rate to: 0.332
12/26 15:43:30 step 20000 epoch 9 learning rate 0.332 step-time 0.673 loss 31.325
12/26 15:43:30 starting evaluation
12/26 15:47:26 test bleu=19.30 loss=40.21 penalty=0.831 ratio=0.844
12/26 15:47:26 saving model to models/5_fold_codenn/checkpoints
12/26 15:47:26 finished saving model
12/26 15:47:26 new best model
12/26 16:09:55 step 22000 epoch 9 learning rate 0.332 step-time 0.672 loss 29.243
12/26 16:09:55 starting evaluation
12/26 16:14:05 test bleu=19.96 loss=39.49 penalty=0.911 ratio=0.915
12/26 16:14:05 saving model to models/5_fold_codenn/checkpoints
12/26 16:14:05 finished saving model
12/26 16:14:05 new best model
12/26 16:14:30   decaying learning rate to: 0.315
12/26 16:36:39 step 24000 epoch 10 learning rate 0.315 step-time 0.675 loss 26.743
12/26 16:36:39 starting evaluation
12/26 16:40:35 test bleu=21.22 loss=39.75 penalty=0.833 ratio=0.846
12/26 16:40:35 saving model to models/5_fold_codenn/checkpoints
12/26 16:40:35 finished saving model
12/26 16:40:35 new best model
12/26 16:45:56   decaying learning rate to: 0.299
12/26 17:03:06 step 26000 epoch 11 learning rate 0.299 step-time 0.673 loss 25.007
12/26 17:03:06 starting evaluation
12/26 17:07:09 test bleu=21.99 loss=40.02 penalty=0.932 ratio=0.934
12/26 17:07:09 saving model to models/5_fold_codenn/checkpoints
12/26 17:07:09 finished saving model
12/26 17:07:09 new best model
12/26 17:17:35   decaying learning rate to: 0.284
12/26 17:29:39 step 28000 epoch 12 learning rate 0.284 step-time 0.673 loss 23.541
12/26 17:29:39 starting evaluation
12/26 17:33:42 test bleu=23.98 loss=40.32 penalty=0.944 ratio=0.945
12/26 17:33:42 saving model to models/5_fold_codenn/checkpoints
12/26 17:33:42 finished saving model
12/26 17:33:42 new best model
12/26 17:49:15   decaying learning rate to: 0.27
12/26 17:56:14 step 30000 epoch 13 learning rate 0.27 step-time 0.674 loss 22.042
12/26 17:56:14 starting evaluation
12/26 18:00:14 test bleu=24.37 loss=40.89 penalty=0.884 ratio=0.890
12/26 18:00:14 saving model to models/5_fold_codenn/checkpoints
12/26 18:00:14 finished saving model
12/26 18:00:14 new best model
12/26 18:20:53   decaying learning rate to: 0.257
12/26 18:22:44 step 32000 epoch 14 learning rate 0.257 step-time 0.673 loss 20.664
12/26 18:22:44 starting evaluation
12/26 18:26:47 test bleu=25.49 loss=41.86 penalty=0.919 ratio=0.922
12/26 18:26:47 saving model to models/5_fold_codenn/checkpoints
12/26 18:26:47 finished saving model
12/26 18:26:47 new best model
12/26 18:49:22 step 34000 epoch 14 learning rate 0.257 step-time 0.676 loss 18.853
12/26 18:49:22 starting evaluation
12/26 18:53:22 test bleu=25.90 loss=40.79 penalty=0.908 ratio=0.912
12/26 18:53:22 saving model to models/5_fold_codenn/checkpoints
12/26 18:53:22 finished saving model
12/26 18:53:22 new best model
12/26 18:56:26   decaying learning rate to: 0.244
12/26 19:15:54 step 36000 epoch 15 learning rate 0.244 step-time 0.674 loss 17.315
12/26 19:15:54 starting evaluation
12/26 19:19:55 test bleu=26.05 loss=42.23 penalty=0.894 ratio=0.899
12/26 19:19:55 saving model to models/5_fold_codenn/checkpoints
12/26 19:19:55 finished saving model
12/26 19:19:55 new best model
12/26 19:28:01   decaying learning rate to: 0.232
12/26 19:42:25 step 38000 epoch 16 learning rate 0.232 step-time 0.673 loss 16.113
12/26 19:42:25 starting evaluation
12/26 19:46:25 test bleu=27.02 loss=43.61 penalty=0.890 ratio=0.895
12/26 19:46:25 saving model to models/5_fold_codenn/checkpoints
12/26 19:46:25 finished saving model
12/26 19:46:25 new best model
12/26 19:59:44   decaying learning rate to: 0.22
12/26 20:08:58 step 40000 epoch 17 learning rate 0.22 step-time 0.675 loss 15.112
12/26 20:08:58 starting evaluation
12/26 20:12:58 test bleu=27.02 loss=46.29 penalty=0.884 ratio=0.890
12/26 20:12:58 saving model to models/5_fold_codenn/checkpoints
12/26 20:12:58 finished saving model
12/26 20:12:58 new best model
12/26 20:31:19   decaying learning rate to: 0.209
12/26 20:35:29 step 42000 epoch 18 learning rate 0.209 step-time 0.673 loss 14.105
12/26 20:35:29 starting evaluation
12/26 20:39:34 test bleu=28.02 loss=46.64 penalty=0.990 ratio=0.990
12/26 20:39:34 saving model to models/5_fold_codenn/checkpoints
12/26 20:39:34 finished saving model
12/26 20:39:34 new best model
12/26 21:02:07 step 44000 epoch 18 learning rate 0.209 step-time 0.674 loss 13.049
12/26 21:02:07 starting evaluation
12/26 21:06:07 test bleu=28.35 loss=45.97 penalty=0.936 ratio=0.938
12/26 21:06:07 saving model to models/5_fold_codenn/checkpoints
12/26 21:06:07 finished saving model
12/26 21:06:07 new best model
12/26 21:06:58   decaying learning rate to: 0.199
12/26 21:28:40 step 46000 epoch 19 learning rate 0.199 step-time 0.674 loss 11.567
12/26 21:28:40 starting evaluation
12/26 21:32:43 test bleu=28.70 loss=47.83 penalty=0.938 ratio=0.939
12/26 21:32:43 saving model to models/5_fold_codenn/checkpoints
12/26 21:32:43 finished saving model
12/26 21:32:43 new best model
12/26 21:38:30   decaying learning rate to: 0.189
12/26 21:55:13 step 48000 epoch 20 learning rate 0.189 step-time 0.673 loss 10.781
12/26 21:55:13 starting evaluation
12/26 21:59:11 test bleu=28.81 loss=50.09 penalty=0.895 ratio=0.900
12/26 21:59:11 saving model to models/5_fold_codenn/checkpoints
12/26 21:59:12 finished saving model
12/26 21:59:12 new best model
12/26 22:10:07   decaying learning rate to: 0.179
12/26 22:21:39 step 50000 epoch 21 learning rate 0.179 step-time 0.672 loss 10.009
12/26 22:21:39 starting evaluation
12/26 22:25:47 test bleu=29.11 loss=51.92 penalty=1.000 ratio=1.006
12/26 22:25:47 saving model to models/5_fold_codenn/checkpoints
12/26 22:25:47 finished saving model
12/26 22:25:47 new best model
12/26 22:41:51   decaying learning rate to: 0.17
12/26 22:48:20 step 52000 epoch 22 learning rate 0.17 step-time 0.674 loss 9.330
12/26 22:48:20 starting evaluation
12/26 22:52:22 test bleu=29.65 loss=53.99 penalty=0.955 ratio=0.956
12/26 22:52:22 saving model to models/5_fold_codenn/checkpoints
12/26 22:52:23 finished saving model
12/26 22:52:23 new best model
12/26 23:13:21   decaying learning rate to: 0.162
12/26 23:14:43 step 54000 epoch 23 learning rate 0.162 step-time 0.668 loss 8.693
12/26 23:14:43 starting evaluation
12/26 23:18:43 test bleu=29.79 loss=55.55 penalty=0.930 ratio=0.932
12/26 23:18:43 saving model to models/5_fold_codenn/checkpoints
12/26 23:18:44 finished saving model
12/26 23:18:44 new best model
12/26 23:40:52 step 56000 epoch 23 learning rate 0.162 step-time 0.662 loss 7.683
12/26 23:40:52 starting evaluation
12/26 23:44:56 test bleu=30.39 loss=55.69 penalty=0.987 ratio=0.987
12/26 23:44:56 saving model to models/5_fold_codenn/checkpoints
12/26 23:44:56 finished saving model
12/26 23:44:56 new best model
12/26 23:48:21   decaying learning rate to: 0.154
12/27 00:07:05 step 58000 epoch 24 learning rate 0.154 step-time 0.662 loss 7.036
12/27 00:07:05 starting evaluation
12/27 00:11:06 test bleu=30.20 loss=58.66 penalty=0.942 ratio=0.944
12/27 00:11:06 saving model to models/5_fold_codenn/checkpoints
12/27 00:11:06 finished saving model
12/27 00:19:28   decaying learning rate to: 0.146
12/27 00:33:11 step 60000 epoch 25 learning rate 0.146 step-time 0.661 loss 6.514
12/27 00:33:11 starting evaluation
12/27 00:37:16 test bleu=30.53 loss=60.80 penalty=1.000 ratio=1.004
12/27 00:37:16 saving model to models/5_fold_codenn/checkpoints
12/27 00:37:16 finished saving model
12/27 00:37:16 new best model
12/27 00:50:43   decaying learning rate to: 0.139
12/27 00:59:24 step 62000 epoch 26 learning rate 0.139 step-time 0.662 loss 6.063
12/27 00:59:24 starting evaluation
12/27 01:03:28 test bleu=30.72 loss=63.12 penalty=1.000 ratio=1.009
12/27 01:03:28 saving model to models/5_fold_codenn/checkpoints
12/27 01:03:29 finished saving model
12/27 01:03:29 new best model
12/27 01:21:58   decaying learning rate to: 0.132
12/27 01:25:37 step 64000 epoch 27 learning rate 0.132 step-time 0.662 loss 5.627
12/27 01:25:37 starting evaluation
12/27 01:29:42 test bleu=31.14 loss=65.00 penalty=0.991 ratio=0.991
12/27 01:29:42 saving model to models/5_fold_codenn/checkpoints
12/27 01:29:42 finished saving model
12/27 01:29:42 new best model
12/27 01:51:48 step 66000 epoch 27 learning rate 0.132 step-time 0.661 loss 5.131
12/27 01:51:48 starting evaluation
12/27 01:55:48 test bleu=31.37 loss=65.38 penalty=0.920 ratio=0.923
12/27 01:55:48 saving model to models/5_fold_codenn/checkpoints
12/27 01:55:48 finished saving model
12/27 01:55:48 new best model
12/27 01:57:03   decaying learning rate to: 0.125
12/27 02:17:56 step 68000 epoch 28 learning rate 0.125 step-time 0.662 loss 4.533
12/27 02:17:56 starting evaluation
12/27 02:22:02 test bleu=31.32 loss=68.31 penalty=0.966 ratio=0.967
12/27 02:22:02 saving model to models/5_fold_codenn/checkpoints
12/27 02:22:02 finished saving model
12/27 02:28:11   decaying learning rate to: 0.119
12/27 02:44:10 step 70000 epoch 29 learning rate 0.119 step-time 0.662 loss 4.208
12/27 02:44:10 starting evaluation
12/27 02:48:13 test bleu=31.50 loss=70.07 penalty=0.974 ratio=0.974
12/27 02:48:13 saving model to models/5_fold_codenn/checkpoints
12/27 02:48:13 finished saving model
12/27 02:48:13 new best model
12/27 02:59:23   decaying learning rate to: 0.113
12/27 03:10:21 step 72000 epoch 30 learning rate 0.113 step-time 0.662 loss 3.897
12/27 03:10:21 starting evaluation
12/27 03:14:24 test bleu=31.34 loss=73.14 penalty=0.994 ratio=0.994
12/27 03:14:24 saving model to models/5_fold_codenn/checkpoints
12/27 03:14:24 finished saving model
12/27 03:30:36   decaying learning rate to: 0.107
12/27 03:36:31 step 74000 epoch 31 learning rate 0.107 step-time 0.661 loss 3.642
12/27 03:36:31 starting evaluation
12/27 03:40:35 test bleu=31.74 loss=75.61 penalty=0.973 ratio=0.974
12/27 03:40:35 saving model to models/5_fold_codenn/checkpoints
12/27 03:40:35 finished saving model
12/27 03:40:35 new best model
12/27 04:02:19   decaying learning rate to: 0.102
12/27 04:03:16 step 76000 epoch 32 learning rate 0.102 step-time 0.678 loss 3.385
12/27 04:03:16 starting evaluation
12/27 04:07:21 test bleu=31.89 loss=76.91 penalty=0.954 ratio=0.955
12/27 04:07:21 saving model to models/5_fold_codenn/checkpoints
12/27 04:07:21 finished saving model
12/27 04:07:21 new best model
12/27 04:30:09 step 78000 epoch 32 learning rate 0.102 step-time 0.682 loss 2.951
12/27 04:30:09 starting evaluation
12/27 04:34:20 test bleu=31.28 loss=79.02 penalty=1.000 ratio=1.021
12/27 04:34:20 saving model to models/5_fold_codenn/checkpoints
12/27 04:34:20 finished saving model
12/27 04:38:19   decaying learning rate to: 0.0969
12/27 04:57:07 step 80000 epoch 33 learning rate 0.0969 step-time 0.681 loss 2.728
12/27 04:57:07 starting evaluation
12/27 05:01:18 test bleu=31.07 loss=81.30 penalty=1.000 ratio=1.026
12/27 05:01:18 saving model to models/5_fold_codenn/checkpoints
12/27 05:01:18 finished saving model
12/27 05:10:29   decaying learning rate to: 0.092
12/27 05:24:08 step 82000 epoch 34 learning rate 0.092 step-time 0.683 loss 2.535
12/27 05:24:08 starting evaluation
12/27 05:28:14 test bleu=32.04 loss=83.59 penalty=0.975 ratio=0.975
12/27 05:28:14 saving model to models/5_fold_codenn/checkpoints
12/27 05:28:14 finished saving model
12/27 05:28:14 new best model
12/27 05:42:37   decaying learning rate to: 0.0874
12/27 05:51:05 step 84000 epoch 35 learning rate 0.0874 step-time 0.683 loss 2.347
12/27 05:51:05 starting evaluation
12/27 05:55:15 test bleu=31.92 loss=84.95 penalty=1.000 ratio=1.007
12/27 05:55:15 saving model to models/5_fold_codenn/checkpoints
12/27 05:55:16 finished saving model
12/27 06:14:46   decaying learning rate to: 0.083
12/27 06:18:04 step 86000 epoch 36 learning rate 0.083 step-time 0.682 loss 2.199
12/27 06:18:04 starting evaluation
12/27 06:22:14 test bleu=31.54 loss=88.02 penalty=1.000 ratio=1.022
12/27 06:22:14 saving model to models/5_fold_codenn/checkpoints
12/27 06:22:14 finished saving model
12/27 06:45:02 step 88000 epoch 36 learning rate 0.083 step-time 0.682 loss 1.984
12/27 06:45:02 starting evaluation
12/27 06:49:11 test bleu=32.34 loss=88.52 penalty=0.993 ratio=0.993
12/27 06:49:11 saving model to models/5_fold_codenn/checkpoints
12/27 06:49:11 finished saving model
12/27 06:49:11 new best model
12/27 06:50:57   decaying learning rate to: 0.0789
12/27 07:11:59 step 90000 epoch 37 learning rate 0.0789 step-time 0.682 loss 1.785
12/27 07:11:59 starting evaluation
12/27 07:16:09 test bleu=32.43 loss=90.95 penalty=1.000 ratio=1.002
12/27 07:16:10 saving model to models/5_fold_codenn/checkpoints
12/27 07:16:10 finished saving model
12/27 07:16:10 new best model
12/27 07:22:58   decaying learning rate to: 0.0749
12/27 07:39:00 step 92000 epoch 38 learning rate 0.0749 step-time 0.683 loss 1.660
12/27 07:39:00 starting evaluation
12/27 07:43:09 test bleu=31.71 loss=93.47 penalty=1.000 ratio=1.023
12/27 07:43:09 saving model to models/5_fold_codenn/checkpoints
12/27 07:43:09 finished saving model
12/27 07:55:07   decaying learning rate to: 0.0712
12/27 08:05:58 step 94000 epoch 39 learning rate 0.0712 step-time 0.682 loss 1.549
12/27 08:05:58 starting evaluation
12/27 08:10:09 test bleu=32.31 loss=94.90 penalty=1.000 ratio=1.010
12/27 08:10:09 saving model to models/5_fold_codenn/checkpoints
12/27 08:10:09 finished saving model
12/27 08:27:18   decaying learning rate to: 0.0676
12/27 08:32:56 step 96000 epoch 40 learning rate 0.0676 step-time 0.681 loss 1.465
12/27 08:32:56 starting evaluation
12/27 08:37:05 test bleu=32.36 loss=96.13 penalty=1.000 ratio=1.007
12/27 08:37:05 saving model to models/5_fold_codenn/checkpoints
12/27 08:37:05 finished saving model
12/27 08:59:24   decaying learning rate to: 0.0643
12/27 08:59:51 step 98000 epoch 41 learning rate 0.0643 step-time 0.681 loss 1.365
12/27 08:59:51 starting evaluation
12/27 09:04:02 test bleu=32.27 loss=98.39 penalty=1.000 ratio=1.013
12/27 09:04:02 saving model to models/5_fold_codenn/checkpoints
12/27 09:04:02 finished saving model
12/27 09:26:52 step 100000 epoch 41 learning rate 0.0643 step-time 0.683 loss 1.191
12/27 09:26:52 starting evaluation
12/27 09:31:02 test bleu=32.17 loss=99.02 penalty=1.000 ratio=1.017
12/27 09:31:02 saving model to models/5_fold_codenn/checkpoints
12/27 09:31:02 finished saving model
12/27 09:35:29   decaying learning rate to: 0.061
12/27 09:53:53 step 102000 epoch 42 learning rate 0.061 step-time 0.683 loss 1.131
12/27 09:53:53 starting evaluation
12/27 09:58:04 test bleu=31.61 loss=102.03 penalty=1.000 ratio=1.027
12/27 09:58:04 saving model to models/5_fold_codenn/checkpoints
12/27 09:58:04 finished saving model
12/27 10:07:37   decaying learning rate to: 0.058
12/27 10:20:48 step 104000 epoch 43 learning rate 0.058 step-time 0.680 loss 1.049
12/27 10:20:48 starting evaluation
12/27 10:24:58 test bleu=32.94 loss=103.34 penalty=0.998 ratio=0.998
12/27 10:24:58 saving model to models/5_fold_codenn/checkpoints
12/27 10:24:58 finished saving model
12/27 10:24:58 new best model
12/27 10:39:46   decaying learning rate to: 0.0551
12/27 10:47:48 step 106000 epoch 44 learning rate 0.0551 step-time 0.683 loss 1.004
12/27 10:47:48 starting evaluation
12/27 10:52:00 test bleu=31.90 loss=104.60 penalty=1.000 ratio=1.026
12/27 10:52:00 saving model to models/5_fold_codenn/checkpoints
12/27 10:52:00 finished saving model
12/27 11:11:54   decaying learning rate to: 0.0523
12/27 11:14:43 step 108000 epoch 45 learning rate 0.0523 step-time 0.679 loss 0.949
12/27 11:14:43 starting evaluation
12/27 11:18:53 test bleu=32.55 loss=106.09 penalty=1.000 ratio=1.007
12/27 11:18:53 saving model to models/5_fold_codenn/checkpoints
12/27 11:18:54 finished saving model
12/27 11:41:39 step 110000 epoch 45 learning rate 0.0523 step-time 0.681 loss 0.865
12/27 11:41:39 starting evaluation
12/27 11:45:49 test bleu=32.80 loss=107.12 penalty=0.994 ratio=0.994
12/27 11:45:49 saving model to models/5_fold_codenn/checkpoints
12/27 11:45:49 finished saving model
12/27 11:48:00   decaying learning rate to: 0.0497
12/27 12:08:39 step 112000 epoch 46 learning rate 0.0497 step-time 0.683 loss 0.794
12/27 12:08:39 starting evaluation
12/27 12:12:51 test bleu=31.84 loss=108.04 penalty=1.000 ratio=1.032
12/27 12:12:51 saving model to models/5_fold_codenn/checkpoints
12/27 12:12:51 finished saving model
12/27 12:20:03   decaying learning rate to: 0.0472
12/27 12:35:40 step 114000 epoch 47 learning rate 0.0472 step-time 0.683 loss 0.750
12/27 12:35:40 starting evaluation
12/27 12:39:52 test bleu=32.08 loss=109.04 penalty=1.000 ratio=1.023
12/27 12:39:52 saving model to models/5_fold_codenn/checkpoints
12/27 12:39:52 finished saving model
12/27 12:52:17   decaying learning rate to: 0.0449
12/27 13:02:40 step 116000 epoch 48 learning rate 0.0449 step-time 0.682 loss 0.729
12/27 13:02:40 starting evaluation
12/27 13:06:51 test bleu=32.63 loss=111.42 penalty=1.000 ratio=1.009
12/27 13:06:51 saving model to models/5_fold_codenn/checkpoints
12/27 13:06:52 finished saving model
12/27 13:24:28   decaying learning rate to: 0.0426
12/27 13:29:37 step 118000 epoch 49 learning rate 0.0426 step-time 0.681 loss 0.690
12/27 13:29:37 starting evaluation
12/27 13:33:49 test bleu=32.04 loss=112.06 penalty=1.000 ratio=1.021
12/27 13:33:49 saving model to models/5_fold_codenn/checkpoints
12/27 13:33:49 finished saving model
12/27 13:56:38 step 120000 epoch 50 learning rate 0.0426 step-time 0.682 loss 0.667
12/27 13:56:38 starting evaluation
12/27 14:00:47 test bleu=32.55 loss=112.59 penalty=1.000 ratio=1.015
12/27 14:00:47 saving model to models/5_fold_codenn/checkpoints
12/27 14:00:47 finished saving model
12/27 14:00:47   decaying learning rate to: 0.0405
12/27 14:23:36 step 122000 epoch 50 learning rate 0.0405 step-time 0.683 loss 0.591
12/27 14:23:36 starting evaluation
12/27 14:27:47 test bleu=32.66 loss=114.02 penalty=1.000 ratio=1.011
12/27 14:27:47 saving model to models/5_fold_codenn/checkpoints
12/27 14:27:48 finished saving model
12/27 14:32:38   decaying learning rate to: 0.0385
12/27 14:50:31 step 124000 epoch 51 learning rate 0.0385 step-time 0.680 loss 0.573
12/27 14:50:31 starting evaluation
12/27 14:54:42 test bleu=32.67 loss=115.31 penalty=1.000 ratio=1.007
12/27 14:54:42 saving model to models/5_fold_codenn/checkpoints
12/27 14:54:42 finished saving model
12/27 15:04:49   decaying learning rate to: 0.0365
12/27 15:17:29 step 126000 epoch 52 learning rate 0.0365 step-time 0.681 loss 0.546
12/27 15:17:29 starting evaluation
12/27 15:21:39 test bleu=32.45 loss=116.05 penalty=1.000 ratio=1.014
12/27 15:21:39 saving model to models/5_fold_codenn/checkpoints
12/27 15:21:40 finished saving model
12/27 15:37:00   decaying learning rate to: 0.0347
12/27 15:44:29 step 128000 epoch 53 learning rate 0.0347 step-time 0.683 loss 0.533
12/27 15:44:29 starting evaluation
12/27 15:48:39 test bleu=32.52 loss=116.98 penalty=1.000 ratio=1.013
12/27 15:48:39 saving model to models/5_fold_codenn/checkpoints
12/27 15:48:39 finished saving model
12/27 16:09:11   decaying learning rate to: 0.033
12/27 16:11:31 step 130000 epoch 54 learning rate 0.033 step-time 0.684 loss 0.511
12/27 16:11:31 starting evaluation
12/27 16:15:39 test bleu=32.37 loss=117.40 penalty=1.000 ratio=1.022
12/27 16:15:39 saving model to models/5_fold_codenn/checkpoints
12/27 16:15:39 finished saving model
12/27 16:38:31 step 132000 epoch 54 learning rate 0.033 step-time 0.684 loss 0.478
12/27 16:38:31 starting evaluation
12/27 16:42:41 test bleu=32.71 loss=118.10 penalty=1.000 ratio=1.012
12/27 16:42:41 saving model to models/5_fold_codenn/checkpoints
12/27 16:42:42 finished saving model
12/27 16:45:21   decaying learning rate to: 0.0313
12/27 17:05:34 step 134000 epoch 55 learning rate 0.0313 step-time 0.684 loss 0.448
12/27 17:05:34 starting evaluation
12/27 17:09:42 test bleu=33.05 loss=118.64 penalty=0.999 ratio=0.999
12/27 17:09:42 saving model to models/5_fold_codenn/checkpoints
12/27 17:09:42 finished saving model
12/27 17:09:42 new best model
12/27 17:17:25   decaying learning rate to: 0.0298
12/27 17:32:27 step 136000 epoch 56 learning rate 0.0298 step-time 0.681 loss 0.433
12/27 17:32:27 starting evaluation
12/27 17:36:37 test bleu=32.69 loss=119.18 penalty=1.000 ratio=1.011
12/27 17:36:37 saving model to models/5_fold_codenn/checkpoints
12/27 17:36:37 finished saving model
12/27 17:49:35   decaying learning rate to: 0.0283
12/27 17:59:26 step 138000 epoch 57 learning rate 0.0283 step-time 0.682 loss 0.418
12/27 17:59:26 starting evaluation
12/27 18:03:35 test bleu=32.35 loss=119.37 penalty=1.000 ratio=1.023
12/27 18:03:35 saving model to models/5_fold_codenn/checkpoints
12/27 18:03:35 finished saving model
12/27 18:21:43   decaying learning rate to: 0.0269
12/27 18:26:26 step 140000 epoch 58 learning rate 0.0269 step-time 0.684 loss 0.407
12/27 18:26:26 starting evaluation
12/27 18:30:33 test bleu=32.96 loss=120.28 penalty=1.000 ratio=1.004
12/27 18:30:33 saving model to models/5_fold_codenn/checkpoints
12/27 18:30:34 finished saving model
12/27 18:53:24 step 142000 epoch 58 learning rate 0.0269 step-time 0.683 loss 0.398
12/27 18:53:24 starting evaluation
12/27 18:57:34 test bleu=32.35 loss=119.82 penalty=1.000 ratio=1.022
12/27 18:57:34 saving model to models/5_fold_codenn/checkpoints
12/27 18:57:34 finished saving model
12/27 18:58:00   decaying learning rate to: 0.0255
12/27 19:20:25 step 144000 epoch 59 learning rate 0.0255 step-time 0.683 loss 0.359
12/27 19:20:25 starting evaluation
12/27 19:24:34 test bleu=32.63 loss=120.82 penalty=1.000 ratio=1.016
12/27 19:24:34 saving model to models/5_fold_codenn/checkpoints
12/27 19:24:34 finished saving model
12/27 19:29:57   decaying learning rate to: 0.0242
12/27 19:47:22 step 146000 epoch 60 learning rate 0.0242 step-time 0.682 loss 0.352
12/27 19:47:22 starting evaluation
12/27 19:51:30 test bleu=32.63 loss=121.28 penalty=1.000 ratio=1.011
12/27 19:51:30 saving model to models/5_fold_codenn/checkpoints
12/27 19:51:30 finished saving model
12/27 20:02:05   decaying learning rate to: 0.023
12/27 20:14:18 step 148000 epoch 61 learning rate 0.023 step-time 0.682 loss 0.339
12/27 20:14:18 starting evaluation
12/27 20:18:28 test bleu=31.95 loss=121.78 penalty=1.000 ratio=1.032
12/27 20:18:28 saving model to models/5_fold_codenn/checkpoints
12/27 20:18:28 finished saving model
12/27 20:34:18   decaying learning rate to: 0.0219
12/27 20:41:22 step 150000 epoch 62 learning rate 0.0219 step-time 0.685 loss 0.341
12/27 20:41:22 starting evaluation
12/27 20:45:30 test bleu=32.25 loss=122.20 penalty=1.000 ratio=1.029
12/27 20:45:30 saving model to models/5_fold_codenn/checkpoints
12/27 20:45:30 finished saving model
12/27 21:06:28   decaying learning rate to: 0.0208
12/27 21:08:19 step 152000 epoch 63 learning rate 0.0208 step-time 0.683 loss 0.324
12/27 21:08:19 starting evaluation
12/27 21:12:28 test bleu=32.70 loss=122.58 penalty=1.000 ratio=1.013
12/27 21:12:28 saving model to models/5_fold_codenn/checkpoints
12/27 21:12:28 finished saving model
12/27 21:35:19 step 154000 epoch 63 learning rate 0.0208 step-time 0.684 loss 0.307
12/27 21:35:19 starting evaluation
12/27 21:39:27 test bleu=32.58 loss=122.57 penalty=1.000 ratio=1.016
12/27 21:39:27 saving model to models/5_fold_codenn/checkpoints
12/27 21:39:27 finished saving model
12/27 21:42:32   decaying learning rate to: 0.0197
12/27 22:02:13 step 156000 epoch 64 learning rate 0.0197 step-time 0.681 loss 0.294
12/27 22:02:13 starting evaluation
12/27 22:06:22 test bleu=32.79 loss=122.60 penalty=1.000 ratio=1.010
12/27 22:06:22 saving model to models/5_fold_codenn/checkpoints
12/27 22:06:22 finished saving model
12/27 22:14:38   decaying learning rate to: 0.0188
12/27 22:29:13 step 158000 epoch 65 learning rate 0.0188 step-time 0.684 loss 0.288
12/27 22:29:13 starting evaluation
12/27 22:33:20 test bleu=32.62 loss=123.22 penalty=1.000 ratio=1.014
12/27 22:33:20 saving model to models/5_fold_codenn/checkpoints
12/27 22:33:21 finished saving model
12/27 22:46:49   decaying learning rate to: 0.0178
12/27 22:56:14 step 160000 epoch 66 learning rate 0.0178 step-time 0.685 loss 0.283
12/27 22:56:14 starting evaluation
12/27 23:00:23 test bleu=32.93 loss=123.30 penalty=1.000 ratio=1.006
12/27 23:00:23 saving model to models/5_fold_codenn/checkpoints
12/27 23:00:23 finished saving model
12/27 23:18:59   decaying learning rate to: 0.0169
12/27 23:23:14 step 162000 epoch 67 learning rate 0.0169 step-time 0.683 loss 0.274
12/27 23:23:14 starting evaluation
12/27 23:27:22 test bleu=32.40 loss=124.29 penalty=1.000 ratio=1.023
12/27 23:27:22 saving model to models/5_fold_codenn/checkpoints
12/27 23:27:22 finished saving model
12/27 23:50:12 step 164000 epoch 67 learning rate 0.0169 step-time 0.683 loss 0.273
12/27 23:50:12 starting evaluation
12/27 23:54:21 test bleu=32.52 loss=123.51 penalty=1.000 ratio=1.016
12/27 23:54:21 saving model to models/5_fold_codenn/checkpoints
12/27 23:54:21 finished saving model
12/27 23:55:13   decaying learning rate to: 0.0161
12/28 00:17:10 step 166000 epoch 68 learning rate 0.0161 step-time 0.682 loss 0.253
12/28 00:17:10 starting evaluation
12/28 00:21:18 test bleu=32.83 loss=124.17 penalty=1.000 ratio=1.010
12/28 00:21:18 saving model to models/5_fold_codenn/checkpoints
12/28 00:21:18 finished saving model
12/28 00:27:13   decaying learning rate to: 0.0153
12/28 00:44:11 step 168000 epoch 69 learning rate 0.0153 step-time 0.685 loss 0.247
12/28 00:44:11 starting evaluation
12/28 00:48:19 test bleu=32.56 loss=124.54 penalty=1.000 ratio=1.017
12/28 00:48:19 saving model to models/5_fold_codenn/checkpoints
12/28 00:48:19 finished saving model
12/28 00:59:23   decaying learning rate to: 0.0145
12/28 01:11:09 step 170000 epoch 70 learning rate 0.0145 step-time 0.683 loss 0.239
12/28 01:11:09 starting evaluation
12/28 01:15:18 test bleu=32.40 loss=124.67 penalty=1.000 ratio=1.022
12/28 01:15:18 saving model to models/5_fold_codenn/checkpoints
12/28 01:15:18 finished saving model
12/28 01:31:30   decaying learning rate to: 0.0138
12/28 01:38:06 step 172000 epoch 71 learning rate 0.0138 step-time 0.682 loss 0.239
12/28 01:38:06 starting evaluation
12/28 01:42:14 test bleu=32.68 loss=125.29 penalty=1.000 ratio=1.013
12/28 01:42:14 saving model to models/5_fold_codenn/checkpoints
12/28 01:42:14 finished saving model
12/28 02:04:00   decaying learning rate to: 0.0131
12/28 02:05:27 step 174000 epoch 72 learning rate 0.0131 step-time 0.694 loss 0.237
12/28 02:05:27 starting evaluation
12/28 02:09:41 test bleu=32.67 loss=125.30 penalty=1.000 ratio=1.016
12/28 02:09:41 saving model to models/5_fold_codenn/checkpoints
12/28 02:09:42 finished saving model
12/28 02:33:22 step 176000 epoch 72 learning rate 0.0131 step-time 0.708 loss 0.223
12/28 02:33:22 starting evaluation
12/28 02:37:36 test bleu=32.51 loss=125.24 penalty=1.000 ratio=1.021
12/28 02:37:36 saving model to models/5_fold_codenn/checkpoints
12/28 02:37:37 finished saving model
12/28 02:41:23   decaying learning rate to: 0.0124
12/28 03:01:27 step 178000 epoch 73 learning rate 0.0124 step-time 0.713 loss 0.216
12/28 03:01:27 starting evaluation
12/28 03:05:42 test bleu=32.01 loss=125.74 penalty=1.000 ratio=1.036
12/28 03:05:42 saving model to models/5_fold_codenn/checkpoints
12/28 03:05:42 finished saving model
12/28 03:14:46   decaying learning rate to: 0.0118
12/28 03:29:23 step 180000 epoch 74 learning rate 0.0118 step-time 0.708 loss 0.212
12/28 03:29:23 starting evaluation
12/28 03:33:39 test bleu=32.36 loss=125.69 penalty=1.000 ratio=1.025
12/28 03:33:39 saving model to models/5_fold_codenn/checkpoints
12/28 03:33:39 finished saving model
12/28 03:48:14   decaying learning rate to: 0.0112
12/28 03:57:29 step 182000 epoch 75 learning rate 0.0112 step-time 0.713 loss 0.209
12/28 03:57:29 starting evaluation
12/28 04:01:44 test bleu=32.65 loss=126.44 penalty=1.000 ratio=1.012
12/28 04:01:44 saving model to models/5_fold_codenn/checkpoints
12/28 04:01:44 finished saving model
12/28 04:21:37   decaying learning rate to: 0.0107
12/28 04:25:29 step 184000 epoch 76 learning rate 0.0107 step-time 0.710 loss 0.208
12/28 04:25:29 starting evaluation
12/28 04:29:43 test bleu=32.67 loss=126.79 penalty=1.000 ratio=1.015
12/28 04:29:43 saving model to models/5_fold_codenn/checkpoints
12/28 04:29:44 finished saving model
12/28 04:52:55 step 186000 epoch 76 learning rate 0.0107 step-time 0.694 loss 0.203
12/28 04:52:55 starting evaluation
12/28 04:57:01 test bleu=32.73 loss=126.47 penalty=1.000 ratio=1.013
12/28 04:57:01 saving model to models/5_fold_codenn/checkpoints
12/28 04:57:02 finished saving model
12/28 04:58:20   decaying learning rate to: 0.0101
12/28 05:19:53 step 188000 epoch 77 learning rate 0.0101 step-time 0.684 loss 0.192
12/28 05:19:53 starting evaluation
12/28 05:24:00 test bleu=32.37 loss=126.74 penalty=1.000 ratio=1.021
12/28 05:24:00 saving model to models/5_fold_codenn/checkpoints
12/28 05:24:00 finished saving model
12/28 05:30:23   decaying learning rate to: 0.00963
12/28 05:46:50 step 190000 epoch 78 learning rate 0.00963 step-time 0.683 loss 0.188
12/28 05:46:50 starting evaluation
12/28 05:50:57 test bleu=32.72 loss=127.16 penalty=1.000 ratio=1.010
12/28 05:50:57 saving model to models/5_fold_codenn/checkpoints
12/28 05:50:57 finished saving model
12/28 06:02:31   decaying learning rate to: 0.00915
12/28 06:13:50 step 192000 epoch 79 learning rate 0.00915 step-time 0.685 loss 0.187
12/28 06:13:50 starting evaluation
12/28 06:18:00 test bleu=32.53 loss=127.67 penalty=1.000 ratio=1.017
12/28 06:18:00 saving model to models/5_fold_codenn/checkpoints
12/28 06:18:00 finished saving model
12/28 06:35:01   decaying learning rate to: 0.00869
12/28 06:41:11 step 194000 epoch 80 learning rate 0.00869 step-time 0.693 loss 0.185
12/28 06:41:11 starting evaluation
12/28 06:45:22 test bleu=32.50 loss=127.49 penalty=1.000 ratio=1.019
12/28 06:45:22 saving model to models/5_fold_codenn/checkpoints
12/28 06:45:22 finished saving model
12/28 07:07:40   decaying learning rate to: 0.00826
12/28 07:08:35 step 196000 epoch 81 learning rate 0.00826 step-time 0.694 loss 0.187
12/28 07:08:35 starting evaluation
12/28 07:12:45 test bleu=32.52 loss=127.38 penalty=1.000 ratio=1.017
12/28 07:12:45 saving model to models/5_fold_codenn/checkpoints
12/28 07:12:45 finished saving model
12/28 07:35:59 step 198000 epoch 81 learning rate 0.00826 step-time 0.695 loss 0.174
12/28 07:35:59 starting evaluation
12/28 07:40:09 test bleu=32.51 loss=127.82 penalty=1.000 ratio=1.016
12/28 07:40:09 saving model to models/5_fold_codenn/checkpoints
12/28 07:40:09 finished saving model
12/28 07:44:16   decaying learning rate to: 0.00784
12/28 08:03:20 step 200000 epoch 82 learning rate 0.00784 step-time 0.693 loss 0.172
12/28 08:03:20 starting evaluation
12/28 08:07:28 test bleu=32.67 loss=127.77 penalty=1.000 ratio=1.013
12/28 08:07:28 saving model to models/5_fold_codenn/checkpoints
12/28 08:07:28 finished saving model
12/28 08:16:51   decaying learning rate to: 0.00745
12/28 08:30:39 step 202000 epoch 83 learning rate 0.00745 step-time 0.694 loss 0.166
12/28 08:30:39 starting evaluation
12/28 08:34:50 test bleu=32.20 loss=128.32 penalty=1.000 ratio=1.028
12/28 08:34:50 saving model to models/5_fold_codenn/checkpoints
12/28 08:34:50 finished saving model
12/28 08:49:29   decaying learning rate to: 0.00708
12/28 08:58:02 step 204000 epoch 84 learning rate 0.00708 step-time 0.694 loss 0.172
12/28 08:58:02 starting evaluation
12/28 09:02:12 test bleu=32.35 loss=128.39 penalty=1.000 ratio=1.023
12/28 09:02:12 saving model to models/5_fold_codenn/checkpoints
12/28 09:02:12 finished saving model
12/28 09:22:08   decaying learning rate to: 0.00673
12/28 09:25:25 step 206000 epoch 85 learning rate 0.00673 step-time 0.694 loss 0.168
12/28 09:25:25 starting evaluation
12/28 09:29:34 test bleu=32.45 loss=128.60 penalty=1.000 ratio=1.021
12/28 09:29:34 saving model to models/5_fold_codenn/checkpoints
12/28 09:29:34 finished saving model
12/28 09:52:47 step 208000 epoch 85 learning rate 0.00673 step-time 0.695 loss 0.164
12/28 09:52:47 starting evaluation
12/28 09:56:58 test bleu=32.56 loss=128.62 penalty=1.000 ratio=1.016
12/28 09:56:58 saving model to models/5_fold_codenn/checkpoints
12/28 09:56:58 finished saving model
12/28 09:58:46   decaying learning rate to: 0.00639
12/28 10:20:09 step 210000 epoch 86 learning rate 0.00639 step-time 0.693 loss 0.156
12/28 10:20:09 starting evaluation
12/28 10:24:19 test bleu=32.44 loss=128.71 penalty=1.000 ratio=1.021
12/28 10:24:19 saving model to models/5_fold_codenn/checkpoints
12/28 10:24:20 finished saving model
12/28 10:31:20   decaying learning rate to: 0.00607
12/28 10:47:31 step 212000 epoch 87 learning rate 0.00607 step-time 0.693 loss 0.157
12/28 10:47:31 starting evaluation
12/28 10:51:43 test bleu=32.61 loss=129.28 penalty=1.000 ratio=1.015
12/28 10:51:43 saving model to models/5_fold_codenn/checkpoints
12/28 10:51:43 finished saving model
12/28 11:03:58   decaying learning rate to: 0.00577
12/28 11:14:55 step 214000 epoch 88 learning rate 0.00577 step-time 0.694 loss 0.155
12/28 11:14:55 starting evaluation
12/28 11:19:05 test bleu=32.48 loss=129.56 penalty=1.000 ratio=1.018
12/28 11:19:05 saving model to models/5_fold_codenn/checkpoints
12/28 11:19:05 finished saving model
12/28 11:36:36   decaying learning rate to: 0.00548
12/28 11:42:17 step 216000 epoch 89 learning rate 0.00548 step-time 0.694 loss 0.154
12/28 11:42:17 starting evaluation
12/28 11:46:28 test bleu=32.56 loss=129.45 penalty=1.000 ratio=1.017
12/28 11:46:28 saving model to models/5_fold_codenn/checkpoints
12/28 11:46:28 finished saving model
12/28 12:09:02   decaying learning rate to: 0.0052
12/28 12:09:29 step 218000 epoch 90 learning rate 0.0052 step-time 0.689 loss 0.156
12/28 12:09:29 starting evaluation
12/28 12:13:37 test bleu=32.46 loss=129.47 penalty=1.000 ratio=1.018
12/28 12:13:37 saving model to models/5_fold_codenn/checkpoints
12/28 12:13:38 finished saving model
12/28 12:36:07 step 220000 epoch 90 learning rate 0.0052 step-time 0.673 loss 0.148
12/28 12:36:07 starting evaluation
12/28 12:40:14 test bleu=32.65 loss=129.75 penalty=1.000 ratio=1.014
12/28 12:40:14 saving model to models/5_fold_codenn/checkpoints
12/28 12:40:14 finished saving model
12/28 12:44:36   decaying learning rate to: 0.00494
12/28 13:02:41 step 222000 epoch 91 learning rate 0.00494 step-time 0.672 loss 0.145
12/28 13:02:41 starting evaluation
12/28 13:06:49 test bleu=32.36 loss=129.95 penalty=1.000 ratio=1.022
12/28 13:06:49 saving model to models/5_fold_codenn/checkpoints
12/28 13:06:49 finished saving model
12/28 13:16:15   decaying learning rate to: 0.0047
12/28 13:29:13 step 224000 epoch 92 learning rate 0.0047 step-time 0.670 loss 0.144
12/28 13:29:13 starting evaluation
12/28 13:33:20 test bleu=32.71 loss=130.18 penalty=1.000 ratio=1.013
12/28 13:33:20 saving model to models/5_fold_codenn/checkpoints
12/28 13:33:21 finished saving model
12/28 13:47:56   decaying learning rate to: 0.00446
12/28 13:55:48 step 226000 epoch 93 learning rate 0.00446 step-time 0.672 loss 0.145
12/28 13:55:48 starting evaluation
12/28 13:59:54 test bleu=32.55 loss=130.16 penalty=1.000 ratio=1.016
12/28 13:59:54 saving model to models/5_fold_codenn/checkpoints
12/28 13:59:54 finished saving model
12/28 14:19:40   decaying learning rate to: 0.00424
12/28 14:22:24 step 228000 epoch 94 learning rate 0.00424 step-time 0.673 loss 0.145
12/28 14:22:24 starting evaluation
12/28 14:26:31 test bleu=32.66 loss=130.34 penalty=1.000 ratio=1.014
12/28 14:26:31 saving model to models/5_fold_codenn/checkpoints
12/28 14:26:31 finished saving model
12/28 14:48:59 step 230000 epoch 94 learning rate 0.00424 step-time 0.672 loss 0.141
12/28 14:48:59 starting evaluation
12/28 14:53:07 test bleu=32.56 loss=130.26 penalty=1.000 ratio=1.018
12/28 14:53:07 saving model to models/5_fold_codenn/checkpoints
12/28 14:53:07 finished saving model
12/28 14:55:15   decaying learning rate to: 0.00403
12/28 15:15:35 step 232000 epoch 95 learning rate 0.00403 step-time 0.672 loss 0.138
12/28 15:15:35 starting evaluation
12/28 15:19:44 test bleu=32.33 loss=130.64 penalty=1.000 ratio=1.022
12/28 15:19:44 saving model to models/5_fold_codenn/checkpoints
12/28 15:19:44 finished saving model
12/28 15:26:55   decaying learning rate to: 0.00383
12/28 15:42:09 step 234000 epoch 96 learning rate 0.00383 step-time 0.671 loss 0.139
12/28 15:42:09 starting evaluation
12/28 15:46:18 test bleu=32.41 loss=130.78 penalty=1.000 ratio=1.019
12/28 15:46:18 saving model to models/5_fold_codenn/checkpoints
12/28 15:46:18 finished saving model
12/28 15:58:34   decaying learning rate to: 0.00363
12/28 16:08:45 step 236000 epoch 97 learning rate 0.00363 step-time 0.672 loss 0.136
12/28 16:08:45 starting evaluation
12/28 16:12:53 test bleu=32.43 loss=130.75 penalty=1.000 ratio=1.020
12/28 16:12:53 saving model to models/5_fold_codenn/checkpoints
12/28 16:12:53 finished saving model
12/28 16:30:33   decaying learning rate to: 0.00345
12/28 16:35:44 step 238000 epoch 98 learning rate 0.00345 step-time 0.683 loss 0.138
12/28 16:35:44 starting evaluation
12/28 16:39:53 test bleu=32.58 loss=130.92 penalty=1.000 ratio=1.016
12/28 16:39:53 saving model to models/5_fold_codenn/checkpoints
12/28 16:39:54 finished saving model
12/28 17:02:43 step 240000 epoch 99 learning rate 0.00345 step-time 0.683 loss 0.137
12/28 17:02:43 starting evaluation
12/28 17:06:54 test bleu=32.38 loss=130.91 penalty=1.000 ratio=1.021
12/28 17:06:54 saving model to models/5_fold_codenn/checkpoints
12/28 17:06:54 finished saving model
12/28 17:06:55   decaying learning rate to: 0.00328
12/28 17:29:44 step 242000 epoch 99 learning rate 0.00328 step-time 0.683 loss 0.131
12/28 17:29:44 starting evaluation
12/28 17:33:54 test bleu=32.59 loss=131.08 penalty=1.000 ratio=1.016
12/28 17:33:54 saving model to models/5_fold_codenn/checkpoints
12/28 17:33:54 finished saving model
12/28 17:38:53   decaying learning rate to: 0.00312
12/28 17:54:36 step 244000 epoch 100 learning rate 0.00312 step-time 0.619 loss 0.131
12/28 17:54:36 starting evaluation
12/28 17:57:17 test bleu=32.64 loss=131.27 penalty=1.000 ratio=1.014
12/28 17:57:17 saving model to models/5_fold_codenn/checkpoints
12/28 17:57:17 finished saving model
12/28 18:04:53 finished training
12/28 18:04:53 exiting...
12/28 18:04:53 saving model to models/5_fold_codenn/checkpoints
12/28 18:04:53 finished saving model
