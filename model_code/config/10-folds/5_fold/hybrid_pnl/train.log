nohup: ignoring input
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:107: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:30: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

12/26 11:22:48 label: default
12/26 11:22:48 description:
  default configuration
  next line of description
  last line
12/26 11:22:48 /root/icpc/icpc/translate/__main__.py config/10-folds/5_fold/hybrid_pnl/config.yaml --train -v
12/26 11:22:48 commit hash 74e0554cb3eb5df835cef993ad570ff8de651f71
12/26 11:22:48 tensorflow version: 1.14.0
12/26 11:22:48 program arguments
12/26 11:22:48   aggregation_method   'sum'
12/26 11:22:48   align_encoder_id     0
12/26 11:22:48   allow_growth         True
12/26 11:22:48   attention_type       'global'
12/26 11:22:48   attn_filter_length   0
12/26 11:22:48   attn_filters         0
12/26 11:22:48   attn_prev_word       False
12/26 11:22:48   attn_size            128
12/26 11:22:48   attn_temperature     1.0
12/26 11:22:48   attn_window_size     0
12/26 11:22:48   average              False
12/26 11:22:48   baseline_activation  None
12/26 11:22:48   baseline_learning_rate 0.001
12/26 11:22:48   baseline_optimizer   'adam'
12/26 11:22:48   baseline_steps       0
12/26 11:22:48   batch_mode           'standard'
12/26 11:22:48   batch_size           64
12/26 11:22:48   beam_size            5
12/26 11:22:48   bidir                True
12/26 11:22:48   bidir_projection     False
12/26 11:22:48   binary               False
12/26 11:22:48   cell_size            256
12/26 11:22:48   cell_type            'GRU'
12/26 11:22:48   character_level      False
12/26 11:22:48   checkpoints          []
12/26 11:22:48   conditional_rnn      False
12/26 11:22:48   config               'config/10-folds/5_fold/hybrid_pnl/config.yaml'
12/26 11:22:48   convolutions         None
12/26 11:22:48   data_dir             'data/gooddata/5_fold'
12/26 11:22:48   debug                False
12/26 11:22:48   decay_after_n_epoch  1
12/26 11:22:48   decay_every_n_epoch  1
12/26 11:22:48   decay_if_no_progress None
12/26 11:22:48   decoders             [{'max_len': 40, 'name': 'nl'}]
12/26 11:22:48   description          'default configuration\nnext line of description\nlast line\n'
12/26 11:22:48   dev_prefix           'test'
12/26 11:22:48   early_stopping       True
12/26 11:22:48   embedding_dropout    0.0
12/26 11:22:48   embedding_initializer None
12/26 11:22:48   embedding_size       256
12/26 11:22:48   embedding_weight_scale None
12/26 11:22:48   embeddings_on_cpu    True
12/26 11:22:48   encoders             [{'attention_type': 'global', 'max_len': 200, 'name': 'code'},
 {'attention_type': 'global', 'max_len': 80, 'name': 'pnl'}]
12/26 11:22:48   ensemble             False
12/26 11:22:48   eval_burn_in         0
12/26 11:22:48   feed_previous        0.0
12/26 11:22:48   final_state          'last'
12/26 11:22:48   freeze_variables     []
12/26 11:22:48   generate_first       True
12/26 11:22:48   gpu_id               0
12/26 11:22:48   highway_layers       0
12/26 11:22:48   initial_state_dropout 0.0
12/26 11:22:48   initializer          None
12/26 11:22:48   input_layer_dropout  0.0
12/26 11:22:48   input_layers         None
12/26 11:22:48   keep_best            5
12/26 11:22:48   keep_every_n_hours   0
12/26 11:22:48   label                'default'
12/26 11:22:48   layer_norm           False
12/26 11:22:48   layers               1
12/26 11:22:48   learning_rate        0.5
12/26 11:22:48   learning_rate_decay_factor 0.95
12/26 11:22:48   len_normalization    1.0
12/26 11:22:48   log_file             'log.txt'
12/26 11:22:48   loss_function        'xent'
12/26 11:22:48   max_dev_size         0
12/26 11:22:48   max_epochs           100
12/26 11:22:48   max_gradient_norm    5.0
12/26 11:22:48   max_len              50
12/26 11:22:48   max_steps            600000
12/26 11:22:48   max_test_size        0
12/26 11:22:48   max_to_keep          1
12/26 11:22:48   max_train_size       0
12/26 11:22:48   maxout_stride        None
12/26 11:22:48   mem_fraction         1.0
12/26 11:22:48   min_learning_rate    1e-06
12/26 11:22:48   model_dir            'models/5_fold_hybrid_pnl'
12/26 11:22:48   moving_average       None
12/26 11:22:48   no_gpu               False
12/26 11:22:48   optimizer            'sgd'
12/26 11:22:48   orthogonal_init      False
12/26 11:22:48   output               None
12/26 11:22:48   output_dropout       0.0
12/26 11:22:48   parallel_iterations  16
12/26 11:22:48   pervasive_dropout    False
12/26 11:22:48   pooling_avg          True
12/26 11:22:48   post_process_script  None
12/26 11:22:48   pred_deep_layer      False
12/26 11:22:48   pred_edits           False
12/26 11:22:48   pred_embed_proj      True
12/26 11:22:48   pred_maxout_layer    True
12/26 11:22:48   purge                False
12/26 11:22:48   raw_output           False
12/26 11:22:48   read_ahead           1
12/26 11:22:48   reconstruction_attn_weight 0.05
12/26 11:22:48   reconstruction_decoders False
12/26 11:22:48   reconstruction_weight 1.0
12/26 11:22:48   reinforce_after_n_epoch None
12/26 11:22:48   remove_unk           False
12/26 11:22:48   reverse              False
12/26 11:22:48   reverse_input        True
12/26 11:22:48   reward_function      'sentence_bleu'
12/26 11:22:48   rnn_feed_attn        True
12/26 11:22:48   rnn_input_dropout    0.0
12/26 11:22:48   rnn_output_dropout   0.0
12/26 11:22:48   rnn_state_dropout    0.0
12/26 11:22:48   save                 False
12/26 11:22:48   score_function       'corpus_bleu'
12/26 11:22:48   score_functions      ['bleu', 'loss']
12/26 11:22:48   script_dir           'scripts'
12/26 11:22:48   sgd_after_n_epoch    None
12/26 11:22:48   sgd_learning_rate    1.0
12/26 11:22:48   shuffle              True
12/26 11:22:48   softmax_temperature  1.0
12/26 11:22:48   steps_per_checkpoint 2000
12/26 11:22:48   steps_per_eval       2000
12/26 11:22:48   swap_memory          True
12/26 11:22:48   tie_embeddings       False
12/26 11:22:48   time_pooling         None
12/26 11:22:48   train                True
12/26 11:22:48   train_initial_states True
12/26 11:22:48   train_prefix         'train'
12/26 11:22:48   truncate_lines       True
12/26 11:22:48   update_first         False
12/26 11:22:48   use_baseline         False
12/26 11:22:48   use_dropout          False
12/26 11:22:48   use_lstm_full_state  False
12/26 11:22:48   use_previous_word    True
12/26 11:22:48   verbose              True
12/26 11:22:48   vocab_prefix         'vocab'
12/26 11:22:48   weight_scale         None
12/26 11:22:48   word_dropout         0.0
12/26 11:22:48 python random seed: 5351673336704918586
12/26 11:22:48 tf random seed:     2426644740515136919
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:203: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

12/26 11:22:48 creating model
12/26 11:22:48 using device: /gpu:0
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:230: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

12/26 11:22:48 copying vocab to models/5_fold_hybrid_pnl/data/vocab.code
12/26 11:22:48 copying vocab to models/5_fold_hybrid_pnl/data/vocab.pnl
12/26 11:22:48 copying vocab to models/5_fold_hybrid_pnl/data/vocab.nl
12/26 11:22:48 reading vocabularies
12/26 11:22:48 creating model
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:60: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:111: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:33: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ffb3f443c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ffb3f443c18>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:226: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ffb3f443d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ffb3f443d68>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ffbc7f3f438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ffbc7f3f438>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ffbc7f3f908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ffbc7f3f908>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc84a7a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc84a7a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:838: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc7bcf5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc7bcf5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc7bd8cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc7bd8cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:432: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:435: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc7af2dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc7af2dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc7b00cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc7b00cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc7963e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc7963e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc7978908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc7978908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc76c1eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc76c1eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc76c1b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc76c1b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc76b1b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc76b1b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc767fcf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc767fcf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc767fcf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffbc767fcf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:919: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ffbc7526e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ffbc7526e80>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:131: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:223: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ffb6715fc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ffb6715fc88>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffb6709c400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffb6709c400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffb6709c400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffb6709c400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffb66fecba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffb66fecba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffb66fecba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffb66fecba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffb66fd99b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffb66fd99b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffb66f2a710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffb66f2a710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffb66f2a710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffb66f2a710>>: AssertionError: Bad argument number for Name: 3, expecting 4
12/26 11:22:56 model parameters (45)
12/26 11:22:56   baseline_step:0 ()
12/26 11:22:56   decoder_nl/attention_code/U_a/kernel:0 (512, 128)
12/26 11:22:56   decoder_nl/attention_code/W_a/bias:0 (128,)
12/26 11:22:56   decoder_nl/attention_code/W_a/kernel:0 (256, 128)
12/26 11:22:56   decoder_nl/attention_code/v_a:0 (128,)
12/26 11:22:56   decoder_nl/attention_pnl/U_a/kernel:0 (512, 128)
12/26 11:22:56   decoder_nl/attention_pnl/W_a/bias:0 (128,)
12/26 11:22:56   decoder_nl/attention_pnl/W_a/kernel:0 (256, 128)
12/26 11:22:56   decoder_nl/attention_pnl/v_a:0 (128,)
12/26 11:22:56   decoder_nl/code_pnl/initial_state_projection/bias:0 (256,)
12/26 11:22:56   decoder_nl/code_pnl/initial_state_projection/kernel:0 (512, 256)
12/26 11:22:56   decoder_nl/gru_cell/candidate/bias:0 (256,)
12/26 11:22:56   decoder_nl/gru_cell/candidate/kernel:0 (1024, 256)
12/26 11:22:56   decoder_nl/gru_cell/gates/bias:0 (512,)
12/26 11:22:56   decoder_nl/gru_cell/gates/kernel:0 (1024, 512)
12/26 11:22:56   decoder_nl/maxout/bias:0 (256,)
12/26 11:22:56   decoder_nl/maxout/kernel:0 (1024, 256)
12/26 11:22:56   decoder_nl/softmax0/kernel:0 (128, 256)
12/26 11:22:56   decoder_nl/softmax1/bias:0 (37966,)
12/26 11:22:56   decoder_nl/softmax1/kernel:0 (256, 37966)
12/26 11:22:56   embedding_code:0 (50000, 256)
12/26 11:22:56   embedding_nl:0 (37966, 256)
12/26 11:22:56   embedding_pnl:0 (37453, 256)
12/26 11:22:56   encoder_code/initial_state_bw:0 (256,)
12/26 11:22:56   encoder_code/initial_state_fw:0 (256,)
12/26 11:22:56   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/26 11:22:56   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/26 11:22:56   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/26 11:22:56   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/26 11:22:56   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/26 11:22:56   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/26 11:22:56   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/26 11:22:56   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/26 11:22:56   encoder_pnl/initial_state_bw:0 (256,)
12/26 11:22:56   encoder_pnl/initial_state_fw:0 (256,)
12/26 11:22:56   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/26 11:22:56   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/26 11:22:56   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/26 11:22:56   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/26 11:22:56   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/26 11:22:56   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/26 11:22:56   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/26 11:22:56   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/26 11:22:56   global_step:0 ()
12/26 11:22:56   learning_rate:0 ()
12/26 11:22:56 number of parameters: 44.85M
WARNING:tensorflow:From /root/icpc/icpc/translate/translation_model.py:666: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

12/26 11:22:57 global step: 0
12/26 11:22:57 baseline step: 0
12/26 11:22:57 reading training data
12/26 11:22:57 total line count: 156721
12/26 11:23:03   lines read: 100000
12/26 11:23:06 files: data/gooddata/5_fold/train.code data/gooddata/5_fold/train.pnl data/gooddata/5_fold/train.nl
12/26 11:23:06 lines reads: 156721
12/26 11:23:06 reading development data
12/26 11:23:07 files: data/gooddata/5_fold/test.code data/gooddata/5_fold/test.pnl data/gooddata/5_fold/test.nl
12/26 11:23:07 lines reads: 17413
12/26 11:23:07 starting training
12/26 11:51:39 step 2000 epoch 1 learning rate 0.5 step-time 0.854 loss 80.691
12/26 11:51:39 starting evaluation
12/26 11:55:30 test bleu=0.95 loss=66.75 penalty=0.517 ratio=0.603
12/26 11:55:30 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 11:55:30 finished saving model
12/26 11:55:30 new best model
12/26 12:02:01   decaying learning rate to: 0.475
12/26 12:24:05 step 4000 epoch 2 learning rate 0.475 step-time 0.855 loss 59.577
12/26 12:24:05 starting evaluation
12/26 12:28:32 test bleu=5.28 loss=55.03 penalty=0.681 ratio=0.722
12/26 12:28:32 saving model to models/5_fold_hybrid_pnl/checkpoints
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
12/26 12:28:32 finished saving model
12/26 12:28:32 new best model
12/26 12:41:15   decaying learning rate to: 0.451
12/26 12:57:05 step 6000 epoch 3 learning rate 0.451 step-time 0.855 loss 52.061
12/26 12:57:05 starting evaluation
12/26 13:02:20 test bleu=8.11 loss=50.37 penalty=1.000 ratio=1.115
12/26 13:02:20 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 13:02:20 finished saving model
12/26 13:02:20 new best model
12/26 13:21:27   decaying learning rate to: 0.429
12/26 13:30:51 step 8000 epoch 4 learning rate 0.429 step-time 0.853 loss 46.627
12/26 13:30:51 starting evaluation
12/26 13:35:35 test bleu=12.15 loss=46.28 penalty=0.723 ratio=0.755
12/26 13:35:35 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 13:35:36 finished saving model
12/26 13:35:36 new best model
12/26 14:01:10   decaying learning rate to: 0.407
12/26 14:04:06 step 10000 epoch 5 learning rate 0.407 step-time 0.853 loss 42.540
12/26 14:04:06 starting evaluation
12/26 14:08:58 test bleu=14.96 loss=43.10 penalty=0.723 ratio=0.755
12/26 14:08:58 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 14:08:58 finished saving model
12/26 14:08:58 new best model
12/26 14:37:31 step 12000 epoch 5 learning rate 0.407 step-time 0.854 loss 38.818
12/26 14:37:31 starting evaluation
12/26 14:42:39 test bleu=17.42 loss=41.17 penalty=0.813 ratio=0.828
12/26 14:42:39 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 14:42:39 finished saving model
12/26 14:42:39 new best model
12/26 14:46:12   decaying learning rate to: 0.387
12/26 15:11:14 step 14000 epoch 6 learning rate 0.387 step-time 0.855 loss 35.744
12/26 15:11:14 starting evaluation
12/26 15:16:13 test bleu=20.23 loss=39.62 penalty=0.822 ratio=0.836
12/26 15:16:13 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 15:16:13 finished saving model
12/26 15:16:13 new best model
12/26 15:26:11   decaying learning rate to: 0.368
12/26 15:44:46 step 16000 epoch 7 learning rate 0.368 step-time 0.855 loss 33.219
12/26 15:44:46 starting evaluation
12/26 15:49:51 test bleu=22.13 loss=38.61 penalty=0.862 ratio=0.871
12/26 15:49:51 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 15:49:51 finished saving model
12/26 15:49:51 new best model
12/26 16:06:08   decaying learning rate to: 0.349
12/26 16:18:27 step 18000 epoch 8 learning rate 0.349 step-time 0.856 loss 31.104
12/26 16:18:27 starting evaluation
12/26 16:23:40 test bleu=24.17 loss=38.14 penalty=0.942 ratio=0.943
12/26 16:23:40 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 16:23:40 finished saving model
12/26 16:23:40 new best model
12/26 16:46:22   decaying learning rate to: 0.332
12/26 16:52:15 step 20000 epoch 9 learning rate 0.332 step-time 0.855 loss 29.040
12/26 16:52:15 starting evaluation
12/26 16:57:26 test bleu=25.59 loss=37.66 penalty=0.902 ratio=0.906
12/26 16:57:26 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 16:57:26 finished saving model
12/26 16:57:26 new best model
12/26 17:25:53 step 22000 epoch 9 learning rate 0.332 step-time 0.852 loss 27.051
12/26 17:25:53 starting evaluation
12/26 17:30:57 test bleu=25.80 loss=36.70 penalty=0.874 ratio=0.881
12/26 17:30:57 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 17:30:57 finished saving model
12/26 17:30:57 new best model
12/26 17:31:32   decaying learning rate to: 0.315
12/26 17:59:28 step 24000 epoch 10 learning rate 0.315 step-time 0.853 loss 24.456
12/26 17:59:28 starting evaluation
12/26 18:04:24 test bleu=26.88 loss=37.05 penalty=0.850 ratio=0.860
12/26 18:04:24 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 18:04:24 finished saving model
12/26 18:04:24 new best model
12/26 18:11:31   decaying learning rate to: 0.299
12/26 18:33:00 step 26000 epoch 11 learning rate 0.299 step-time 0.856 loss 22.641
12/26 18:33:00 starting evaluation
12/26 18:38:11 test bleu=28.34 loss=37.21 penalty=0.883 ratio=0.889
12/26 18:38:11 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 18:38:11 finished saving model
12/26 18:38:11 new best model
12/26 18:51:32   decaying learning rate to: 0.284
12/26 19:06:46 step 28000 epoch 12 learning rate 0.284 step-time 0.856 loss 21.011
12/26 19:06:46 starting evaluation
12/26 19:11:54 test bleu=28.49 loss=38.20 penalty=0.897 ratio=0.902
12/26 19:11:54 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 19:11:54 finished saving model
12/26 19:11:54 new best model
12/26 19:31:35   decaying learning rate to: 0.27
12/26 19:40:25 step 30000 epoch 13 learning rate 0.27 step-time 0.854 loss 19.452
12/26 19:40:25 starting evaluation
12/26 19:45:30 test bleu=29.77 loss=38.94 penalty=0.899 ratio=0.904
12/26 19:45:30 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 19:45:30 finished saving model
12/26 19:45:30 new best model
12/26 20:11:41   decaying learning rate to: 0.257
12/26 20:14:02 step 32000 epoch 14 learning rate 0.257 step-time 0.854 loss 18.088
12/26 20:14:02 starting evaluation
12/26 20:19:11 test bleu=29.97 loss=40.34 penalty=0.915 ratio=0.919
12/26 20:19:11 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 20:19:11 finished saving model
12/26 20:19:11 new best model
12/26 20:47:43 step 34000 epoch 14 learning rate 0.257 step-time 0.854 loss 16.103
12/26 20:47:43 starting evaluation
12/26 20:52:53 test bleu=30.75 loss=39.30 penalty=0.952 ratio=0.953
12/26 20:52:53 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 20:52:54 finished saving model
12/26 20:52:54 new best model
12/26 20:57:04   decaying learning rate to: 0.244
12/26 21:21:27 step 36000 epoch 15 learning rate 0.244 step-time 0.855 loss 14.548
12/26 21:21:27 starting evaluation
12/26 21:26:35 test bleu=30.79 loss=40.93 penalty=0.917 ratio=0.920
12/26 21:26:35 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 21:26:35 finished saving model
12/26 21:26:35 new best model
12/26 21:37:00   decaying learning rate to: 0.232
12/26 21:55:10 step 38000 epoch 16 learning rate 0.232 step-time 0.855 loss 13.298
12/26 21:55:10 starting evaluation
12/26 22:00:11 test bleu=30.71 loss=42.87 penalty=0.894 ratio=0.899
12/26 22:00:11 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 22:00:11 finished saving model
12/26 22:17:00   decaying learning rate to: 0.22
12/26 22:28:42 step 40000 epoch 17 learning rate 0.22 step-time 0.853 loss 12.213
12/26 22:28:42 starting evaluation
12/26 22:33:46 test bleu=31.43 loss=44.94 penalty=0.901 ratio=0.905
12/26 22:33:46 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 22:33:46 finished saving model
12/26 22:33:46 new best model
12/26 22:57:01   decaying learning rate to: 0.209
12/26 23:02:18 step 42000 epoch 18 learning rate 0.209 step-time 0.854 loss 11.206
12/26 23:02:18 starting evaluation
12/26 23:07:21 test bleu=31.88 loss=46.64 penalty=0.901 ratio=0.905
12/26 23:07:21 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 23:07:21 finished saving model
12/26 23:07:21 new best model
12/26 23:35:27 step 44000 epoch 18 learning rate 0.209 step-time 0.841 loss 10.123
12/26 23:35:27 starting evaluation
12/26 23:40:26 test bleu=32.73 loss=46.30 penalty=0.922 ratio=0.925
12/26 23:40:26 saving model to models/5_fold_hybrid_pnl/checkpoints
12/26 23:40:26 finished saving model
12/26 23:40:26 new best model
12/26 23:41:37   decaying learning rate to: 0.199
12/27 00:08:30 step 46000 epoch 19 learning rate 0.199 step-time 0.840 loss 8.620
12/27 00:08:30 starting evaluation
12/27 00:13:33 test bleu=32.85 loss=48.25 penalty=0.944 ratio=0.946
12/27 00:13:33 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 00:13:34 finished saving model
12/27 00:13:34 new best model
12/27 00:21:05   decaying learning rate to: 0.189
12/27 00:41:37 step 48000 epoch 20 learning rate 0.189 step-time 0.840 loss 7.913
12/27 00:41:37 starting evaluation
12/27 00:46:46 test bleu=33.28 loss=50.86 penalty=0.976 ratio=0.976
12/27 00:46:46 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 00:46:46 finished saving model
12/27 00:46:46 new best model
12/27 01:00:24   decaying learning rate to: 0.179
12/27 01:14:51 step 50000 epoch 21 learning rate 0.179 step-time 0.840 loss 7.113
12/27 01:14:51 starting evaluation
12/27 01:19:57 test bleu=33.27 loss=53.08 penalty=0.956 ratio=0.957
12/27 01:19:57 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 01:19:58 finished saving model
12/27 01:39:54   decaying learning rate to: 0.17
12/27 01:48:02 step 52000 epoch 22 learning rate 0.17 step-time 0.840 loss 6.493
12/27 01:48:02 starting evaluation
12/27 01:53:07 test bleu=33.35 loss=55.53 penalty=0.930 ratio=0.932
12/27 01:53:07 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 01:53:07 finished saving model
12/27 01:53:07 new best model
12/27 02:19:28   decaying learning rate to: 0.162
12/27 02:21:11 step 54000 epoch 23 learning rate 0.162 step-time 0.840 loss 5.850
12/27 02:21:11 starting evaluation
12/27 02:26:16 test bleu=34.05 loss=57.58 penalty=0.961 ratio=0.961
12/27 02:26:16 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 02:26:16 finished saving model
12/27 02:26:16 new best model
12/27 02:54:23 step 56000 epoch 23 learning rate 0.162 step-time 0.841 loss 4.982
12/27 02:54:23 starting evaluation
12/27 02:59:24 test bleu=33.88 loss=59.15 penalty=0.912 ratio=0.915
12/27 02:59:24 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 02:59:24 finished saving model
12/27 03:04:03   decaying learning rate to: 0.154
12/27 03:27:27 step 58000 epoch 24 learning rate 0.154 step-time 0.840 loss 4.406
12/27 03:27:27 starting evaluation
12/27 03:32:36 test bleu=34.53 loss=60.89 penalty=0.993 ratio=0.993
12/27 03:32:36 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 03:32:37 finished saving model
12/27 03:32:37 new best model
12/27 03:43:25   decaying learning rate to: 0.146
12/27 04:01:07 step 60000 epoch 25 learning rate 0.146 step-time 0.853 loss 4.004
12/27 04:01:07 starting evaluation
12/27 04:06:18 test bleu=34.25 loss=63.97 penalty=0.975 ratio=0.975
12/27 04:06:18 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 04:06:18 finished saving model
12/27 04:23:52   decaying learning rate to: 0.139
12/27 04:35:15 step 62000 epoch 26 learning rate 0.139 step-time 0.866 loss 3.606
12/27 04:35:15 starting evaluation
12/27 04:40:24 test bleu=34.38 loss=66.87 penalty=0.952 ratio=0.954
12/27 04:40:24 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 04:40:24 finished saving model
12/27 05:04:35   decaying learning rate to: 0.132
12/27 05:09:20 step 64000 epoch 27 learning rate 0.132 step-time 0.866 loss 3.262
12/27 05:09:20 starting evaluation
12/27 05:14:32 test bleu=34.54 loss=68.35 penalty=0.960 ratio=0.960
12/27 05:14:32 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 05:14:33 finished saving model
12/27 05:14:33 new best model
12/27 05:43:29 step 66000 epoch 27 learning rate 0.132 step-time 0.866 loss 2.888
12/27 05:43:29 starting evaluation
12/27 05:48:37 test bleu=34.65 loss=69.90 penalty=0.952 ratio=0.953
12/27 05:48:37 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 05:48:37 finished saving model
12/27 05:48:37 new best model
12/27 05:50:25   decaying learning rate to: 0.125
12/27 06:17:34 step 68000 epoch 28 learning rate 0.125 step-time 0.866 loss 2.446
12/27 06:17:34 starting evaluation
12/27 06:22:42 test bleu=35.08 loss=72.12 penalty=0.975 ratio=0.975
12/27 06:22:42 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 06:22:42 finished saving model
12/27 06:22:42 new best model
12/27 06:30:59   decaying learning rate to: 0.119
12/27 06:51:39 step 70000 epoch 29 learning rate 0.119 step-time 0.866 loss 2.227
12/27 06:51:39 starting evaluation
12/27 06:56:48 test bleu=34.71 loss=75.11 penalty=0.968 ratio=0.969
12/27 06:56:48 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 06:56:48 finished saving model
12/27 07:11:30   decaying learning rate to: 0.113
12/27 07:25:44 step 72000 epoch 30 learning rate 0.113 step-time 0.866 loss 2.021
12/27 07:25:44 starting evaluation
12/27 07:30:58 test bleu=34.99 loss=77.75 penalty=1.000 ratio=1.006
12/27 07:30:58 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 07:30:58 finished saving model
12/27 07:52:09   decaying learning rate to: 0.107
12/27 07:59:56 step 74000 epoch 31 learning rate 0.107 step-time 0.867 loss 1.855
12/27 07:59:56 starting evaluation
12/27 08:05:09 test bleu=35.14 loss=78.98 penalty=0.979 ratio=0.980
12/27 08:05:09 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 08:05:09 finished saving model
12/27 08:05:09 new best model
12/27 08:32:53   decaying learning rate to: 0.102
12/27 08:34:04 step 76000 epoch 32 learning rate 0.102 step-time 0.865 loss 1.672
12/27 08:34:04 starting evaluation
12/27 08:39:12 test bleu=35.35 loss=80.82 penalty=0.975 ratio=0.975
12/27 08:39:12 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 08:39:12 finished saving model
12/27 08:39:12 new best model
12/27 09:08:09 step 78000 epoch 32 learning rate 0.102 step-time 0.866 loss 1.427
12/27 09:08:09 starting evaluation
12/27 09:13:18 test bleu=35.57 loss=82.11 penalty=0.994 ratio=0.994
12/27 09:13:18 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 09:13:18 finished saving model
12/27 09:13:18 new best model
12/27 09:18:44   decaying learning rate to: 0.0969
12/27 09:42:16 step 80000 epoch 33 learning rate 0.0969 step-time 0.867 loss 1.285
12/27 09:42:16 starting evaluation
12/27 09:47:29 test bleu=35.19 loss=84.33 penalty=1.000 ratio=1.011
12/27 09:47:29 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 09:47:29 finished saving model
12/27 09:59:13   decaying learning rate to: 0.092
12/27 10:16:27 step 82000 epoch 34 learning rate 0.092 step-time 0.867 loss 1.189
12/27 10:16:27 starting evaluation
12/27 10:21:41 test bleu=35.28 loss=87.06 penalty=1.000 ratio=1.010
12/27 10:21:41 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 10:21:41 finished saving model
12/27 10:39:50   decaying learning rate to: 0.0874
12/27 10:50:37 step 84000 epoch 35 learning rate 0.0874 step-time 0.866 loss 1.097
12/27 10:50:37 starting evaluation
12/27 10:55:50 test bleu=35.85 loss=88.93 penalty=0.997 ratio=0.997
12/27 10:55:50 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 10:55:50 finished saving model
12/27 10:55:50 new best model
12/27 11:20:39   decaying learning rate to: 0.083
12/27 11:24:47 step 86000 epoch 36 learning rate 0.083 step-time 0.866 loss 1.014
12/27 11:24:47 starting evaluation
12/27 11:29:59 test bleu=35.58 loss=90.14 penalty=1.000 ratio=1.006
12/27 11:29:59 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 11:30:00 finished saving model
12/27 11:58:59 step 88000 epoch 36 learning rate 0.083 step-time 0.867 loss 0.913
12/27 11:58:59 starting evaluation
12/27 12:04:07 test bleu=35.85 loss=91.12 penalty=0.981 ratio=0.981
12/27 12:04:07 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 12:04:08 finished saving model
12/27 12:04:08 new best model
12/27 12:06:33   decaying learning rate to: 0.0789
12/27 12:33:05 step 90000 epoch 37 learning rate 0.0789 step-time 0.867 loss 0.810
12/27 12:33:05 starting evaluation
12/27 12:38:13 test bleu=35.90 loss=92.35 penalty=0.961 ratio=0.962
12/27 12:38:13 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 12:38:13 finished saving model
12/27 12:38:13 new best model
12/27 12:47:02   decaying learning rate to: 0.0749
12/27 13:07:09 step 92000 epoch 38 learning rate 0.0749 step-time 0.866 loss 0.754
12/27 13:07:09 starting evaluation
12/27 13:12:18 test bleu=35.99 loss=94.02 penalty=0.996 ratio=0.996
12/27 13:12:18 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 13:12:18 finished saving model
12/27 13:12:18 new best model
12/27 13:27:36   decaying learning rate to: 0.0712
12/27 13:41:18 step 94000 epoch 39 learning rate 0.0712 step-time 0.868 loss 0.711
12/27 13:41:18 starting evaluation
12/27 13:46:27 test bleu=35.99 loss=95.81 penalty=0.960 ratio=0.961
12/27 13:46:27 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 13:46:27 finished saving model
12/27 14:08:12   decaying learning rate to: 0.0676
12/27 14:15:23 step 96000 epoch 40 learning rate 0.0676 step-time 0.866 loss 0.665
12/27 14:15:23 starting evaluation
12/27 14:20:35 test bleu=35.76 loss=96.22 penalty=1.000 ratio=1.008
12/27 14:20:35 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 14:20:35 finished saving model
12/27 14:48:58   decaying learning rate to: 0.0643
12/27 14:49:33 step 98000 epoch 41 learning rate 0.0643 step-time 0.867 loss 0.634
12/27 14:49:33 starting evaluation
12/27 14:54:46 test bleu=36.00 loss=97.50 penalty=1.000 ratio=1.004
12/27 14:54:46 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 14:54:46 finished saving model
12/27 14:54:46 new best model
12/27 15:23:41 step 100000 epoch 41 learning rate 0.0643 step-time 0.865 loss 0.554
12/27 15:23:41 starting evaluation
12/27 15:28:51 test bleu=35.75 loss=98.12 penalty=1.000 ratio=1.007
12/27 15:28:51 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 15:28:51 finished saving model
12/27 15:34:51   decaying learning rate to: 0.061
12/27 15:57:48 step 102000 epoch 42 learning rate 0.061 step-time 0.867 loss 0.518
12/27 15:57:48 starting evaluation
12/27 16:02:57 test bleu=36.34 loss=99.15 penalty=0.986 ratio=0.986
12/27 16:02:57 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 16:02:57 finished saving model
12/27 16:02:57 new best model
12/27 16:15:14   decaying learning rate to: 0.058
12/27 16:31:56 step 104000 epoch 43 learning rate 0.058 step-time 0.867 loss 0.502
12/27 16:31:56 starting evaluation
12/27 16:37:06 test bleu=36.23 loss=99.77 penalty=0.979 ratio=0.979
12/27 16:37:06 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 16:37:06 finished saving model
12/27 16:55:51   decaying learning rate to: 0.0551
12/27 17:06:02 step 106000 epoch 44 learning rate 0.0551 step-time 0.866 loss 0.481
12/27 17:06:02 starting evaluation
12/27 17:11:13 test bleu=36.28 loss=100.57 penalty=0.995 ratio=0.995
12/27 17:11:13 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 17:11:13 finished saving model
12/27 17:36:40   decaying learning rate to: 0.0523
12/27 17:40:11 step 108000 epoch 45 learning rate 0.0523 step-time 0.867 loss 0.458
12/27 17:40:11 starting evaluation
12/27 17:45:21 test bleu=36.32 loss=101.65 penalty=0.984 ratio=0.984
12/27 17:45:21 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 17:45:21 finished saving model
12/27 18:14:19 step 110000 epoch 45 learning rate 0.0523 step-time 0.867 loss 0.423
12/27 18:14:19 starting evaluation
12/27 18:19:30 test bleu=36.34 loss=101.53 penalty=0.986 ratio=0.986
12/27 18:19:30 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 18:19:30 finished saving model
12/27 18:19:30 new best model
12/27 18:22:30   decaying learning rate to: 0.0497
12/27 18:48:28 step 112000 epoch 46 learning rate 0.0497 step-time 0.867 loss 0.381
12/27 18:48:28 starting evaluation
12/27 18:53:39 test bleu=36.42 loss=101.61 penalty=0.987 ratio=0.987
12/27 18:53:40 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 18:53:40 finished saving model
12/27 18:53:40 new best model
12/27 19:02:59   decaying learning rate to: 0.0472
12/27 19:22:34 step 114000 epoch 47 learning rate 0.0472 step-time 0.865 loss 0.371
12/27 19:22:34 starting evaluation
12/27 19:27:43 test bleu=36.27 loss=102.83 penalty=0.985 ratio=0.985
12/27 19:27:43 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 19:27:44 finished saving model
12/27 19:43:37   decaying learning rate to: 0.0449
12/27 19:56:41 step 116000 epoch 48 learning rate 0.0449 step-time 0.867 loss 0.351
12/27 19:56:41 starting evaluation
12/27 20:01:53 test bleu=36.55 loss=102.33 penalty=0.983 ratio=0.983
12/27 20:01:53 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 20:01:53 finished saving model
12/27 20:01:53 new best model
12/27 20:24:19   decaying learning rate to: 0.0426
12/27 20:30:54 step 118000 epoch 49 learning rate 0.0426 step-time 0.868 loss 0.336
12/27 20:30:54 starting evaluation
12/27 20:36:03 test bleu=36.61 loss=103.08 penalty=0.984 ratio=0.985
12/27 20:36:03 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 20:36:03 finished saving model
12/27 20:36:03 new best model
12/27 21:04:59 step 120000 epoch 50 learning rate 0.0426 step-time 0.865 loss 0.322
12/27 21:04:59 starting evaluation
12/27 21:10:11 test bleu=36.59 loss=102.65 penalty=0.984 ratio=0.984
12/27 21:10:11 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 21:10:11 finished saving model
12/27 21:10:12   decaying learning rate to: 0.0405
12/27 21:39:05 step 122000 epoch 50 learning rate 0.0405 step-time 0.865 loss 0.286
12/27 21:39:05 starting evaluation
12/27 21:44:13 test bleu=36.62 loss=102.96 penalty=0.984 ratio=0.984
12/27 21:44:13 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 21:44:14 finished saving model
12/27 21:44:14 new best model
12/27 21:50:51   decaying learning rate to: 0.0385
12/27 22:13:11 step 124000 epoch 51 learning rate 0.0385 step-time 0.867 loss 0.266
12/27 22:13:11 starting evaluation
12/27 22:18:20 test bleu=36.74 loss=103.41 penalty=0.979 ratio=0.980
12/27 22:18:20 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 22:18:21 finished saving model
12/27 22:18:21 new best model
12/27 22:31:15   decaying learning rate to: 0.0365
12/27 22:47:20 step 126000 epoch 52 learning rate 0.0365 step-time 0.868 loss 0.263
12/27 22:47:20 starting evaluation
12/27 22:52:30 test bleu=36.87 loss=103.32 penalty=0.984 ratio=0.984
12/27 22:52:30 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 22:52:30 finished saving model
12/27 22:52:30 new best model
12/27 23:11:51   decaying learning rate to: 0.0347
12/27 23:21:27 step 128000 epoch 53 learning rate 0.0347 step-time 0.866 loss 0.245
12/27 23:21:27 starting evaluation
12/27 23:26:36 test bleu=36.76 loss=102.95 penalty=0.982 ratio=0.982
12/27 23:26:36 saving model to models/5_fold_hybrid_pnl/checkpoints
12/27 23:26:36 finished saving model
12/27 23:52:35   decaying learning rate to: 0.033
12/27 23:55:32 step 130000 epoch 54 learning rate 0.033 step-time 0.866 loss 0.235
12/27 23:55:32 starting evaluation
12/28 00:00:39 test bleu=36.81 loss=103.40 penalty=0.988 ratio=0.988
12/28 00:00:39 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 00:00:39 finished saving model
12/28 00:29:36 step 132000 epoch 54 learning rate 0.033 step-time 0.866 loss 0.219
12/28 00:29:36 starting evaluation
12/28 00:34:46 test bleu=36.93 loss=102.64 penalty=0.987 ratio=0.987
12/28 00:34:46 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 00:34:47 finished saving model
12/28 00:34:47 new best model
12/28 00:38:23   decaying learning rate to: 0.0313
12/28 01:03:42 step 134000 epoch 55 learning rate 0.0313 step-time 0.865 loss 0.206
12/28 01:03:42 starting evaluation
12/28 01:08:48 test bleu=36.89 loss=103.17 penalty=0.956 ratio=0.957
12/28 01:08:48 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 01:08:48 finished saving model
12/28 01:18:42   decaying learning rate to: 0.0298
12/28 01:37:44 step 136000 epoch 56 learning rate 0.0298 step-time 0.866 loss 0.195
12/28 01:37:44 starting evaluation
12/28 01:42:53 test bleu=36.79 loss=103.24 penalty=0.994 ratio=0.994
12/28 01:42:53 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 01:42:54 finished saving model
12/28 01:59:31   decaying learning rate to: 0.0283
12/28 02:12:27 step 138000 epoch 57 learning rate 0.0283 step-time 0.885 loss 0.190
12/28 02:12:27 starting evaluation
12/28 02:17:43 test bleu=36.91 loss=103.32 penalty=0.993 ratio=0.993
12/28 02:17:43 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 02:17:43 finished saving model
12/28 02:41:41   decaying learning rate to: 0.0269
12/28 02:47:55 step 140000 epoch 58 learning rate 0.0269 step-time 0.904 loss 0.184
12/28 02:47:55 starting evaluation
12/28 02:53:10 test bleu=36.96 loss=102.99 penalty=0.990 ratio=0.990
12/28 02:53:10 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 02:53:10 finished saving model
12/28 02:53:10 new best model
12/28 03:23:24 step 142000 epoch 58 learning rate 0.0269 step-time 0.904 loss 0.177
12/28 03:23:24 starting evaluation
12/28 03:28:41 test bleu=36.81 loss=103.59 penalty=0.995 ratio=0.995
12/28 03:28:41 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 03:28:41 finished saving model
12/28 03:29:20   decaying learning rate to: 0.0255
12/28 03:59:07 step 144000 epoch 59 learning rate 0.0255 step-time 0.910 loss 0.160
12/28 03:59:07 starting evaluation
12/28 04:04:19 test bleu=36.86 loss=103.61 penalty=0.986 ratio=0.987
12/28 04:04:19 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 04:04:19 finished saving model
12/28 04:11:51   decaying learning rate to: 0.0242
12/28 04:34:33 step 146000 epoch 60 learning rate 0.0242 step-time 0.905 loss 0.154
12/28 04:34:33 starting evaluation
12/28 04:39:47 test bleu=36.90 loss=103.97 penalty=0.994 ratio=0.994
12/28 04:39:47 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 04:39:47 finished saving model
12/28 04:53:17   decaying learning rate to: 0.023
12/28 05:08:45 step 148000 epoch 61 learning rate 0.023 step-time 0.867 loss 0.154
12/28 05:08:45 starting evaluation
12/28 05:13:53 test bleu=37.02 loss=104.21 penalty=0.969 ratio=0.969
12/28 05:13:53 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 05:13:53 finished saving model
12/28 05:13:53 new best model
12/28 05:33:51   decaying learning rate to: 0.0219
12/28 05:42:50 step 150000 epoch 62 learning rate 0.0219 step-time 0.866 loss 0.147
12/28 05:42:50 starting evaluation
12/28 05:48:00 test bleu=36.94 loss=104.08 penalty=0.987 ratio=0.987
12/28 05:48:00 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 05:48:00 finished saving model
12/28 06:14:38   decaying learning rate to: 0.0208
12/28 06:17:01 step 152000 epoch 63 learning rate 0.0208 step-time 0.868 loss 0.146
12/28 06:17:01 starting evaluation
12/28 06:22:11 test bleu=36.97 loss=103.97 penalty=0.988 ratio=0.988
12/28 06:22:11 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 06:22:11 finished saving model
12/28 06:51:38 step 154000 epoch 63 learning rate 0.0208 step-time 0.882 loss 0.134
12/28 06:51:38 starting evaluation
12/28 06:56:51 test bleu=36.91 loss=103.82 penalty=0.995 ratio=0.995
12/28 06:56:51 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 06:56:51 finished saving model
12/28 07:01:06   decaying learning rate to: 0.0197
12/28 07:26:15 step 156000 epoch 64 learning rate 0.0197 step-time 0.880 loss 0.128
12/28 07:26:15 starting evaluation
12/28 07:31:24 test bleu=36.87 loss=104.12 penalty=0.971 ratio=0.971
12/28 07:31:24 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 07:31:24 finished saving model
12/28 07:42:05   decaying learning rate to: 0.0188
12/28 08:00:51 step 158000 epoch 65 learning rate 0.0188 step-time 0.882 loss 0.127
12/28 08:00:51 starting evaluation
12/28 08:06:02 test bleu=37.04 loss=104.22 penalty=0.985 ratio=0.985
12/28 08:06:02 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 08:06:03 finished saving model
12/28 08:06:03 new best model
12/28 08:23:25   decaying learning rate to: 0.0178
12/28 08:35:27 step 160000 epoch 66 learning rate 0.0178 step-time 0.880 loss 0.123
12/28 08:35:27 starting evaluation
12/28 08:40:37 test bleu=37.01 loss=104.66 penalty=0.982 ratio=0.983
12/28 08:40:37 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 08:40:37 finished saving model
12/28 09:04:38   decaying learning rate to: 0.0169
12/28 09:10:06 step 162000 epoch 67 learning rate 0.0169 step-time 0.882 loss 0.122
12/28 09:10:06 starting evaluation
12/28 09:15:17 test bleu=36.97 loss=104.95 penalty=0.992 ratio=0.992
12/28 09:15:17 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 09:15:18 finished saving model
12/28 09:44:41 step 164000 epoch 67 learning rate 0.0169 step-time 0.880 loss 0.119
12/28 09:44:41 starting evaluation
12/28 09:49:53 test bleu=37.03 loss=104.90 penalty=0.987 ratio=0.987
12/28 09:49:53 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 09:49:53 finished saving model
12/28 09:51:07   decaying learning rate to: 0.0161
12/28 10:19:15 step 166000 epoch 68 learning rate 0.0161 step-time 0.879 loss 0.109
12/28 10:19:15 starting evaluation
12/28 10:24:25 test bleu=37.07 loss=104.96 penalty=0.975 ratio=0.975
12/28 10:24:25 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 10:24:25 finished saving model
12/28 10:24:25 new best model
12/28 10:32:15   decaying learning rate to: 0.0153
12/28 10:53:51 step 168000 epoch 69 learning rate 0.0153 step-time 0.881 loss 0.108
12/28 10:53:51 starting evaluation
12/28 10:59:01 test bleu=37.00 loss=105.17 penalty=0.978 ratio=0.979
12/28 10:59:01 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 10:59:01 finished saving model
12/28 11:13:23   decaying learning rate to: 0.0145
12/28 11:28:29 step 170000 epoch 70 learning rate 0.0145 step-time 0.882 loss 0.106
12/28 11:28:29 starting evaluation
12/28 11:33:41 test bleu=37.06 loss=105.49 penalty=0.991 ratio=0.991
12/28 11:33:41 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 11:33:42 finished saving model
12/28 11:54:38   decaying learning rate to: 0.0138
12/28 12:03:01 step 172000 epoch 71 learning rate 0.0138 step-time 0.878 loss 0.105
12/28 12:03:01 starting evaluation
12/28 12:08:10 test bleu=37.03 loss=105.60 penalty=0.996 ratio=0.996
12/28 12:08:10 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 12:08:11 finished saving model
12/28 12:35:02   decaying learning rate to: 0.0131
12/28 12:36:46 step 174000 epoch 72 learning rate 0.0131 step-time 0.856 loss 0.105
12/28 12:36:46 starting evaluation
12/28 12:41:54 test bleu=37.04 loss=105.51 penalty=0.996 ratio=0.996
12/28 12:41:54 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 12:41:54 finished saving model
12/28 13:10:27 step 176000 epoch 72 learning rate 0.0131 step-time 0.854 loss 0.098
12/28 13:10:27 starting evaluation
12/28 13:15:36 test bleu=37.03 loss=105.51 penalty=0.981 ratio=0.981
12/28 13:15:36 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 13:15:36 finished saving model
12/28 13:20:21   decaying learning rate to: 0.0124
12/28 13:44:07 step 178000 epoch 73 learning rate 0.0124 step-time 0.854 loss 0.093
12/28 13:44:07 starting evaluation
12/28 13:49:15 test bleu=37.04 loss=105.94 penalty=0.980 ratio=0.980
12/28 13:49:15 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 13:49:15 finished saving model
12/28 14:00:07   decaying learning rate to: 0.0118
12/28 14:17:46 step 180000 epoch 74 learning rate 0.0118 step-time 0.853 loss 0.094
12/28 14:17:46 starting evaluation
12/28 14:22:53 test bleu=37.06 loss=106.28 penalty=0.971 ratio=0.971
12/28 14:22:53 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 14:22:53 finished saving model
12/28 14:40:12   decaying learning rate to: 0.0112
12/28 14:51:21 step 182000 epoch 75 learning rate 0.0112 step-time 0.852 loss 0.094
12/28 14:51:21 starting evaluation
12/28 14:56:29 test bleu=37.08 loss=106.12 penalty=0.981 ratio=0.981
12/28 14:56:29 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 14:56:29 finished saving model
12/28 14:56:29 new best model
12/28 15:20:18   decaying learning rate to: 0.0107
12/28 15:24:59 step 184000 epoch 76 learning rate 0.0107 step-time 0.853 loss 0.091
12/28 15:24:59 starting evaluation
12/28 15:30:07 test bleu=37.02 loss=106.10 penalty=0.987 ratio=0.987
12/28 15:30:07 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 15:30:07 finished saving model
12/28 15:58:38 step 186000 epoch 76 learning rate 0.0107 step-time 0.853 loss 0.090
12/28 15:58:38 starting evaluation
12/28 16:03:43 test bleu=37.02 loss=106.20 penalty=0.969 ratio=0.969
12/28 16:03:43 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 16:03:43 finished saving model
12/28 16:05:31   decaying learning rate to: 0.0101
12/28 16:32:32 step 188000 epoch 77 learning rate 0.0101 step-time 0.863 loss 0.085
12/28 16:32:32 starting evaluation
12/28 16:37:42 test bleu=37.17 loss=106.35 penalty=0.976 ratio=0.976
12/28 16:37:42 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 16:37:42 finished saving model
12/28 16:37:42 new best model
12/28 16:45:52   decaying learning rate to: 0.00963
12/28 17:06:39 step 190000 epoch 78 learning rate 0.00963 step-time 0.866 loss 0.084
12/28 17:06:39 starting evaluation
12/28 17:11:46 test bleu=37.00 loss=106.79 penalty=0.976 ratio=0.976
12/28 17:11:46 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 17:11:46 finished saving model
12/28 17:26:28   decaying learning rate to: 0.00915
12/28 17:40:46 step 192000 epoch 79 learning rate 0.00915 step-time 0.868 loss 0.084
12/28 17:40:46 starting evaluation
12/28 17:45:56 test bleu=37.11 loss=106.58 penalty=0.992 ratio=0.992
12/28 17:45:56 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 17:45:56 finished saving model
12/28 18:06:44   decaying learning rate to: 0.00869
12/28 18:14:12 step 194000 epoch 80 learning rate 0.00869 step-time 0.846 loss 0.083
12/28 18:14:12 starting evaluation
12/28 18:19:16 test bleu=37.04 loss=107.00 penalty=0.987 ratio=0.987
12/28 18:19:16 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 18:19:16 finished saving model
12/28 18:45:54   decaying learning rate to: 0.00826
12/28 18:47:01 step 196000 epoch 81 learning rate 0.00826 step-time 0.831 loss 0.083
12/28 18:47:01 starting evaluation
12/28 18:52:06 test bleu=37.03 loss=106.97 penalty=0.983 ratio=0.983
12/28 18:52:06 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 18:52:06 finished saving model
12/28 19:19:54 step 198000 epoch 81 learning rate 0.00826 step-time 0.832 loss 0.079
12/28 19:19:54 starting evaluation
12/28 19:25:00 test bleu=37.02 loss=106.75 penalty=0.999 ratio=0.999
12/28 19:25:00 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 19:25:00 finished saving model
12/28 19:30:11   decaying learning rate to: 0.00784
12/28 19:52:45 step 200000 epoch 82 learning rate 0.00784 step-time 0.831 loss 0.077
12/28 19:52:45 starting evaluation
12/28 19:57:51 test bleu=37.09 loss=107.20 penalty=0.986 ratio=0.986
12/28 19:57:51 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 19:57:51 finished saving model
12/28 20:09:00   decaying learning rate to: 0.00745
12/28 20:25:35 step 202000 epoch 83 learning rate 0.00745 step-time 0.830 loss 0.076
12/28 20:25:35 starting evaluation
12/28 20:30:39 test bleu=37.05 loss=107.55 penalty=0.985 ratio=0.985
12/28 20:30:39 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 20:30:39 finished saving model
12/28 20:48:07   decaying learning rate to: 0.00708
12/28 20:58:23 step 204000 epoch 84 learning rate 0.00708 step-time 0.830 loss 0.077
12/28 20:58:23 starting evaluation
12/28 21:03:28 test bleu=36.97 loss=107.52 penalty=0.990 ratio=0.990
12/28 21:03:28 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 21:03:28 finished saving model
12/28 21:27:15   decaying learning rate to: 0.00673
12/28 21:31:15 step 206000 epoch 85 learning rate 0.00673 step-time 0.831 loss 0.075
12/28 21:31:15 starting evaluation
12/28 21:36:19 test bleu=37.07 loss=107.54 penalty=0.986 ratio=0.986
12/28 21:36:19 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 21:36:19 finished saving model
12/28 22:04:05 step 208000 epoch 85 learning rate 0.00673 step-time 0.831 loss 0.075
12/28 22:04:05 starting evaluation
12/28 22:09:09 test bleu=37.07 loss=107.84 penalty=0.973 ratio=0.973
12/28 22:09:09 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 22:09:09 finished saving model
12/28 22:11:28   decaying learning rate to: 0.00639
12/28 22:36:53 step 210000 epoch 86 learning rate 0.00639 step-time 0.830 loss 0.072
12/28 22:36:53 starting evaluation
12/28 22:41:57 test bleu=37.06 loss=107.64 penalty=0.982 ratio=0.982
12/28 22:41:57 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 22:41:58 finished saving model
12/28 22:50:22   decaying learning rate to: 0.00607
12/28 23:09:43 step 212000 epoch 87 learning rate 0.00607 step-time 0.831 loss 0.070
12/28 23:09:43 starting evaluation
12/28 23:14:48 test bleu=37.10 loss=108.06 penalty=0.983 ratio=0.983
12/28 23:14:48 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 23:14:48 finished saving model
12/28 23:29:26   decaying learning rate to: 0.00577
12/28 23:42:33 step 214000 epoch 88 learning rate 0.00577 step-time 0.830 loss 0.071
12/28 23:42:33 starting evaluation
12/28 23:47:37 test bleu=37.03 loss=108.16 penalty=0.980 ratio=0.980
12/28 23:47:37 saving model to models/5_fold_hybrid_pnl/checkpoints
12/28 23:47:37 finished saving model
12/29 00:08:32   decaying learning rate to: 0.00548
12/29 00:15:23 step 216000 epoch 89 learning rate 0.00548 step-time 0.831 loss 0.073
12/29 00:15:23 starting evaluation
12/29 00:20:28 test bleu=36.99 loss=108.11 penalty=0.988 ratio=0.988
12/29 00:20:28 saving model to models/5_fold_hybrid_pnl/checkpoints
12/29 00:20:28 finished saving model
12/29 00:47:42   decaying learning rate to: 0.0052
12/29 00:48:14 step 218000 epoch 90 learning rate 0.0052 step-time 0.831 loss 0.071
12/29 00:48:14 starting evaluation
12/29 00:53:18 test bleu=37.04 loss=107.99 penalty=0.979 ratio=0.979
12/29 00:53:18 saving model to models/5_fold_hybrid_pnl/checkpoints
12/29 00:53:19 finished saving model
12/29 01:21:04 step 220000 epoch 90 learning rate 0.0052 step-time 0.831 loss 0.067
12/29 01:21:04 starting evaluation
12/29 01:26:10 test bleu=37.04 loss=108.31 penalty=0.991 ratio=0.991
12/29 01:26:10 saving model to models/5_fold_hybrid_pnl/checkpoints
12/29 01:26:10 finished saving model
12/29 01:31:51   decaying learning rate to: 0.00494
12/29 01:53:54 step 222000 epoch 91 learning rate 0.00494 step-time 0.830 loss 0.067
12/29 01:53:54 starting evaluation
12/29 01:58:58 test bleu=37.02 loss=108.23 penalty=0.982 ratio=0.982
12/29 01:58:58 saving model to models/5_fold_hybrid_pnl/checkpoints
12/29 01:58:58 finished saving model
12/29 02:11:31   decaying learning rate to: 0.0047
12/29 02:28:31 step 224000 epoch 92 learning rate 0.0047 step-time 0.884 loss 0.068
12/29 02:28:31 starting evaluation
12/29 02:33:47 test bleu=37.05 loss=108.59 penalty=0.983 ratio=0.983
12/29 02:33:47 saving model to models/5_fold_hybrid_pnl/checkpoints
12/29 02:33:48 finished saving model
12/29 02:53:39   decaying learning rate to: 0.00446
12/29 03:03:59 step 226000 epoch 93 learning rate 0.00446 step-time 0.903 loss 0.067
12/29 03:03:59 starting evaluation
12/29 03:09:11 test bleu=37.00 loss=108.56 penalty=0.987 ratio=0.987
12/29 03:09:11 saving model to models/5_fold_hybrid_pnl/checkpoints
12/29 03:09:11 finished saving model
12/29 03:35:09   decaying learning rate to: 0.00424
12/29 03:38:35 step 228000 epoch 94 learning rate 0.00424 step-time 0.880 loss 0.067
12/29 03:38:35 starting evaluation
12/29 03:43:40 test bleu=37.07 loss=108.59 penalty=0.983 ratio=0.983
12/29 03:43:40 saving model to models/5_fold_hybrid_pnl/checkpoints
12/29 03:43:40 finished saving model
12/29 04:11:24 step 230000 epoch 94 learning rate 0.00424 step-time 0.830 loss 0.067
12/29 04:11:24 starting evaluation
12/29 04:16:28 test bleu=37.02 loss=108.67 penalty=0.987 ratio=0.987
12/29 04:16:28 saving model to models/5_fold_hybrid_pnl/checkpoints
12/29 04:16:28 finished saving model
12/29 04:19:22   decaying learning rate to: 0.00403
12/29 04:44:59 step 232000 epoch 95 learning rate 0.00403 step-time 0.854 loss 0.064
12/29 04:44:59 starting evaluation
12/29 04:50:09 test bleu=36.96 loss=108.76 penalty=0.987 ratio=0.988
12/29 04:50:09 saving model to models/5_fold_hybrid_pnl/checkpoints
12/29 04:50:09 finished saving model
12/29 04:59:31   decaying learning rate to: 0.00383
12/29 05:19:15 step 234000 epoch 96 learning rate 0.00383 step-time 0.871 loss 0.065
12/29 05:19:15 starting evaluation
12/29 05:24:25 test bleu=37.08 loss=108.86 penalty=0.984 ratio=0.984
12/29 05:24:25 saving model to models/5_fold_hybrid_pnl/checkpoints
12/29 05:24:25 finished saving model
12/29 05:40:19   decaying learning rate to: 0.00363
12/29 05:53:26 step 236000 epoch 97 learning rate 0.00363 step-time 0.869 loss 0.064
12/29 05:53:26 starting evaluation
12/29 05:58:36 test bleu=37.04 loss=109.02 penalty=0.984 ratio=0.984
12/29 05:58:36 saving model to models/5_fold_hybrid_pnl/checkpoints
12/29 05:58:36 finished saving model
12/29 06:21:07   decaying learning rate to: 0.00345
12/29 06:27:39 step 238000 epoch 98 learning rate 0.00345 step-time 0.870 loss 0.065
12/29 06:27:39 starting evaluation
12/29 06:32:50 test bleu=36.99 loss=108.79 penalty=0.989 ratio=0.989
12/29 06:32:50 saving model to models/5_fold_hybrid_pnl/checkpoints
12/29 06:32:50 finished saving model
12/29 07:01:53 step 240000 epoch 99 learning rate 0.00345 step-time 0.869 loss 0.064
12/29 07:01:53 starting evaluation
12/29 07:07:00 test bleu=37.03 loss=109.05 penalty=0.981 ratio=0.981
12/29 07:07:00 saving model to models/5_fold_hybrid_pnl/checkpoints
12/29 07:07:00 finished saving model
12/29 07:07:02   decaying learning rate to: 0.00328
12/29 07:35:37 step 242000 epoch 99 learning rate 0.00328 step-time 0.857 loss 0.062
12/29 07:35:37 starting evaluation
12/29 07:40:44 test bleu=37.04 loss=109.10 penalty=0.988 ratio=0.988
12/29 07:40:44 saving model to models/5_fold_hybrid_pnl/checkpoints
12/29 07:40:44 finished saving model
12/29 07:47:06   decaying learning rate to: 0.00312
12/29 08:08:12 step 244000 epoch 100 learning rate 0.00312 step-time 0.822 loss 0.063
12/29 08:08:12 starting evaluation
12/29 08:11:38 test bleu=37.07 loss=109.21 penalty=0.983 ratio=0.983
12/29 08:11:38 saving model to models/5_fold_hybrid_pnl/checkpoints
12/29 08:11:38 finished saving model
12/29 08:21:48 finished training
12/29 08:21:48 exiting...
12/29 08:21:48 saving model to models/5_fold_hybrid_pnl/checkpoints
12/29 08:21:48 finished saving model
