nohup: ignoring input
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:107: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:30: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

12/23 14:06:02 label: default
12/23 14:06:02 description:
  default configuration
  next line of description
  last line
12/23 14:06:02 /root/icpc/icpc/translate/__main__.py config/10-folds/1_fold/hybrid_pnl/config.yaml --train -v
12/23 14:06:02 commit hash 74e0554cb3eb5df835cef993ad570ff8de651f71
12/23 14:06:02 tensorflow version: 1.14.0
12/23 14:06:02 program arguments
12/23 14:06:02   aggregation_method   'sum'
12/23 14:06:02   align_encoder_id     0
12/23 14:06:02   allow_growth         True
12/23 14:06:02   attention_type       'global'
12/23 14:06:02   attn_filter_length   0
12/23 14:06:02   attn_filters         0
12/23 14:06:02   attn_prev_word       False
12/23 14:06:02   attn_size            128
12/23 14:06:02   attn_temperature     1.0
12/23 14:06:02   attn_window_size     0
12/23 14:06:02   average              False
12/23 14:06:02   baseline_activation  None
12/23 14:06:02   baseline_learning_rate 0.001
12/23 14:06:02   baseline_optimizer   'adam'
12/23 14:06:02   baseline_steps       0
12/23 14:06:02   batch_mode           'standard'
12/23 14:06:02   batch_size           64
12/23 14:06:02   beam_size            5
12/23 14:06:02   bidir                True
12/23 14:06:02   bidir_projection     False
12/23 14:06:02   binary               False
12/23 14:06:02   cell_size            256
12/23 14:06:02   cell_type            'GRU'
12/23 14:06:02   character_level      False
12/23 14:06:02   checkpoints          []
12/23 14:06:02   conditional_rnn      False
12/23 14:06:02   config               'config/10-folds/1_fold/hybrid_pnl/config.yaml'
12/23 14:06:02   convolutions         None
12/23 14:06:02   data_dir             'data/gooddata/1_fold'
12/23 14:06:02   debug                False
12/23 14:06:02   decay_after_n_epoch  1
12/23 14:06:02   decay_every_n_epoch  1
12/23 14:06:02   decay_if_no_progress None
12/23 14:06:02   decoders             [{'max_len': 40, 'name': 'nl'}]
12/23 14:06:02   description          'default configuration\nnext line of description\nlast line\n'
12/23 14:06:02   dev_prefix           'test'
12/23 14:06:02   early_stopping       True
12/23 14:06:02   embedding_dropout    0.0
12/23 14:06:02   embedding_initializer None
12/23 14:06:02   embedding_size       256
12/23 14:06:02   embedding_weight_scale None
12/23 14:06:02   embeddings_on_cpu    True
12/23 14:06:02   encoders             [{'attention_type': 'global', 'max_len': 200, 'name': 'code'},
 {'attention_type': 'global', 'max_len': 80, 'name': 'pnl'}]
12/23 14:06:02   ensemble             False
12/23 14:06:02   eval_burn_in         0
12/23 14:06:02   feed_previous        0.0
12/23 14:06:02   final_state          'last'
12/23 14:06:02   freeze_variables     []
12/23 14:06:02   generate_first       True
12/23 14:06:02   gpu_id               0
12/23 14:06:02   highway_layers       0
12/23 14:06:02   initial_state_dropout 0.0
12/23 14:06:02   initializer          None
12/23 14:06:02   input_layer_dropout  0.0
12/23 14:06:02   input_layers         None
12/23 14:06:02   keep_best            5
12/23 14:06:02   keep_every_n_hours   0
12/23 14:06:02   label                'default'
12/23 14:06:02   layer_norm           False
12/23 14:06:02   layers               1
12/23 14:06:02   learning_rate        0.5
12/23 14:06:02   learning_rate_decay_factor 0.95
12/23 14:06:02   len_normalization    1.0
12/23 14:06:02   log_file             'log.txt'
12/23 14:06:02   loss_function        'xent'
12/23 14:06:02   max_dev_size         0
12/23 14:06:02   max_epochs           100
12/23 14:06:02   max_gradient_norm    5.0
12/23 14:06:02   max_len              50
12/23 14:06:02   max_steps            600000
12/23 14:06:02   max_test_size        0
12/23 14:06:02   max_to_keep          1
12/23 14:06:02   max_train_size       0
12/23 14:06:02   maxout_stride        None
12/23 14:06:02   mem_fraction         1.0
12/23 14:06:02   min_learning_rate    1e-06
12/23 14:06:02   model_dir            'models/1_fold_hybrid_pnl'
12/23 14:06:02   moving_average       None
12/23 14:06:02   no_gpu               False
12/23 14:06:02   optimizer            'sgd'
12/23 14:06:02   orthogonal_init      False
12/23 14:06:02   output               None
12/23 14:06:02   output_dropout       0.0
12/23 14:06:02   parallel_iterations  16
12/23 14:06:02   pervasive_dropout    False
12/23 14:06:02   pooling_avg          True
12/23 14:06:02   post_process_script  None
12/23 14:06:02   pred_deep_layer      False
12/23 14:06:02   pred_edits           False
12/23 14:06:02   pred_embed_proj      True
12/23 14:06:02   pred_maxout_layer    True
12/23 14:06:02   purge                False
12/23 14:06:02   raw_output           False
12/23 14:06:02   read_ahead           1
12/23 14:06:02   reconstruction_attn_weight 0.05
12/23 14:06:02   reconstruction_decoders False
12/23 14:06:02   reconstruction_weight 1.0
12/23 14:06:02   reinforce_after_n_epoch None
12/23 14:06:02   remove_unk           False
12/23 14:06:02   reverse              False
12/23 14:06:02   reverse_input        True
12/23 14:06:02   reward_function      'sentence_bleu'
12/23 14:06:02   rnn_feed_attn        True
12/23 14:06:02   rnn_input_dropout    0.0
12/23 14:06:02   rnn_output_dropout   0.0
12/23 14:06:02   rnn_state_dropout    0.0
12/23 14:06:02   save                 False
12/23 14:06:02   score_function       'corpus_bleu'
12/23 14:06:02   score_functions      ['bleu', 'loss']
12/23 14:06:02   script_dir           'scripts'
12/23 14:06:02   sgd_after_n_epoch    None
12/23 14:06:02   sgd_learning_rate    1.0
12/23 14:06:02   shuffle              True
12/23 14:06:02   softmax_temperature  1.0
12/23 14:06:02   steps_per_checkpoint 2000
12/23 14:06:02   steps_per_eval       2000
12/23 14:06:02   swap_memory          True
12/23 14:06:02   tie_embeddings       False
12/23 14:06:02   time_pooling         None
12/23 14:06:02   train                True
12/23 14:06:02   train_initial_states True
12/23 14:06:02   train_prefix         'train'
12/23 14:06:02   truncate_lines       True
12/23 14:06:02   update_first         False
12/23 14:06:02   use_baseline         False
12/23 14:06:02   use_dropout          False
12/23 14:06:02   use_lstm_full_state  False
12/23 14:06:02   use_previous_word    True
12/23 14:06:02   verbose              True
12/23 14:06:02   vocab_prefix         'vocab'
12/23 14:06:02   weight_scale         None
12/23 14:06:02   word_dropout         0.0
12/23 14:06:02 python random seed: 1493126031166102570
12/23 14:06:02 tf random seed:     2727930481703054088
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:203: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

12/23 14:06:02 creating model
12/23 14:06:02 using device: /gpu:0
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:230: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

12/23 14:06:02 copying vocab to models/1_fold_hybrid_pnl/data/vocab.code
12/23 14:06:02 copying vocab to models/1_fold_hybrid_pnl/data/vocab.pnl
12/23 14:06:02 copying vocab to models/1_fold_hybrid_pnl/data/vocab.nl
12/23 14:06:02 reading vocabularies
12/23 14:06:02 creating model
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:60: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:111: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:33: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f3b05fa2ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f3b05fa2ba8>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:226: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f3b05fa2ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f3b05fa2ef0>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f3b88aa5240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f3b88aa5240>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f3b88aa5710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f3b88aa5710>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b88a1bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b88a1bc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:838: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b88a02908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b88a02908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b8880da90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b8880da90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:432: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:435: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b886feeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b886feeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b886a9ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b886a9ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b8858deb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b8858deb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b8858df28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b8858df28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b863515c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b863515c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b8626bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b8626bc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b8625b860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b8625b860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b86281278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b86281278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b86281278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b86281278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:919: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f3b86149278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f3b86149278>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:131: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:223: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f3b29adce10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f3b29adce10>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b29a19a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b29a19a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b29a28da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b29a28da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b299e2208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b299e2208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b299b6c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b299b6c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b299b64e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b299b64e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b298e2898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b298e2898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b298e2898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3b298e2898>>: AssertionError: Bad argument number for Name: 3, expecting 4
12/23 14:06:09 model parameters (45)
12/23 14:06:09   baseline_step:0 ()
12/23 14:06:09   decoder_nl/attention_code/U_a/kernel:0 (512, 128)
12/23 14:06:09   decoder_nl/attention_code/W_a/bias:0 (128,)
12/23 14:06:09   decoder_nl/attention_code/W_a/kernel:0 (256, 128)
12/23 14:06:09   decoder_nl/attention_code/v_a:0 (128,)
12/23 14:06:09   decoder_nl/attention_pnl/U_a/kernel:0 (512, 128)
12/23 14:06:09   decoder_nl/attention_pnl/W_a/bias:0 (128,)
12/23 14:06:09   decoder_nl/attention_pnl/W_a/kernel:0 (256, 128)
12/23 14:06:09   decoder_nl/attention_pnl/v_a:0 (128,)
12/23 14:06:09   decoder_nl/code_pnl/initial_state_projection/bias:0 (256,)
12/23 14:06:09   decoder_nl/code_pnl/initial_state_projection/kernel:0 (512, 256)
12/23 14:06:09   decoder_nl/gru_cell/candidate/bias:0 (256,)
12/23 14:06:09   decoder_nl/gru_cell/candidate/kernel:0 (1024, 256)
12/23 14:06:09   decoder_nl/gru_cell/gates/bias:0 (512,)
12/23 14:06:09   decoder_nl/gru_cell/gates/kernel:0 (1024, 512)
12/23 14:06:09   decoder_nl/maxout/bias:0 (256,)
12/23 14:06:09   decoder_nl/maxout/kernel:0 (1024, 256)
12/23 14:06:09   decoder_nl/softmax0/kernel:0 (128, 256)
12/23 14:06:09   decoder_nl/softmax1/bias:0 (35942,)
12/23 14:06:09   decoder_nl/softmax1/kernel:0 (256, 35942)
12/23 14:06:09   embedding_code:0 (50000, 256)
12/23 14:06:09   embedding_nl:0 (35942, 256)
12/23 14:06:09   embedding_pnl:0 (35336, 256)
12/23 14:06:09   encoder_code/initial_state_bw:0 (256,)
12/23 14:06:09   encoder_code/initial_state_fw:0 (256,)
12/23 14:06:09   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/23 14:06:09   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/23 14:06:09   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/23 14:06:09   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/23 14:06:09   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/23 14:06:09   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/23 14:06:09   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/23 14:06:09   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/23 14:06:09   encoder_pnl/initial_state_bw:0 (256,)
12/23 14:06:09   encoder_pnl/initial_state_fw:0 (256,)
12/23 14:06:09   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/23 14:06:09   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/23 14:06:09   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/23 14:06:09   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/23 14:06:09   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/23 14:06:09   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/23 14:06:09   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/23 14:06:09   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/23 14:06:09   global_step:0 ()
12/23 14:06:09   learning_rate:0 ()
12/23 14:06:09 number of parameters: 43.27M
WARNING:tensorflow:From /root/icpc/icpc/translate/translation_model.py:666: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

12/23 14:06:10 global step: 0
12/23 14:06:11 baseline step: 0
12/23 14:06:11 reading training data
12/23 14:06:11 total line count: 147665
12/23 14:06:16   lines read: 100000
12/23 14:06:19 files: data/gooddata/1_fold/train.code data/gooddata/1_fold/train.pnl data/gooddata/1_fold/train.nl
12/23 14:06:19 lines reads: 147665
12/23 14:06:19 reading development data
12/23 14:06:19 files: data/gooddata/1_fold/test.code data/gooddata/1_fold/test.pnl data/gooddata/1_fold/test.nl
12/23 14:06:19 lines reads: 16407
12/23 14:06:20 starting training
12/23 14:34:40 step 2000 epoch 1 learning rate 0.5 step-time 0.847 loss 81.307
12/23 14:34:40 starting evaluation
12/23 14:38:26 test bleu=0.76 loss=64.21 penalty=0.713 ratio=0.748
12/23 14:38:26 saving model to models/1_fold_hybrid_pnl/checkpoints
12/23 14:38:26 finished saving model
12/23 14:38:26 new best model
12/23 14:42:45   decaying learning rate to: 0.475
12/23 15:06:24 step 4000 epoch 2 learning rate 0.475 step-time 0.835 loss 59.698
12/23 15:06:24 starting evaluation
12/23 15:10:34 test bleu=4.53 loss=54.72 penalty=1.000 ratio=1.124
12/23 15:10:34 saving model to models/1_fold_hybrid_pnl/checkpoints
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
12/23 15:10:34 finished saving model
12/23 15:10:34 new best model
12/23 15:19:13   decaying learning rate to: 0.451
12/23 15:38:30 step 6000 epoch 3 learning rate 0.451 step-time 0.835 loss 51.724
12/23 15:38:30 starting evaluation
12/23 15:42:35 test bleu=8.40 loss=49.33 penalty=0.765 ratio=0.789
12/23 15:42:35 saving model to models/1_fold_hybrid_pnl/checkpoints
12/23 15:42:35 finished saving model
12/23 15:42:35 new best model
12/23 15:55:39   decaying learning rate to: 0.429
12/23 16:10:59 step 8000 epoch 4 learning rate 0.429 step-time 0.848 loss 45.947
12/23 16:10:59 starting evaluation
12/23 16:15:14 test bleu=13.01 loss=45.46 penalty=1.000 ratio=1.065
12/23 16:15:14 saving model to models/1_fold_hybrid_pnl/checkpoints
12/23 16:15:14 finished saving model
12/23 16:15:14 new best model
12/23 16:32:49   decaying learning rate to: 0.407
12/23 16:43:43 step 10000 epoch 5 learning rate 0.407 step-time 0.851 loss 41.849
12/23 16:43:43 starting evaluation
12/23 16:47:58 test bleu=14.50 loss=42.62 penalty=1.000 ratio=1.170
12/23 16:47:58 saving model to models/1_fold_hybrid_pnl/checkpoints
12/23 16:47:58 finished saving model
12/23 16:47:58 new best model
12/23 17:09:55   decaying learning rate to: 0.387
12/23 17:16:27 step 12000 epoch 6 learning rate 0.387 step-time 0.851 loss 38.375
12/23 17:16:27 starting evaluation
12/23 17:20:49 test bleu=17.91 loss=40.37 penalty=1.000 ratio=1.085
12/23 17:20:49 saving model to models/1_fold_hybrid_pnl/checkpoints
12/23 17:20:50 finished saving model
12/23 17:20:50 new best model
12/23 17:47:06   decaying learning rate to: 0.368
12/23 17:49:15 step 14000 epoch 7 learning rate 0.368 step-time 0.849 loss 35.505
12/23 17:49:15 starting evaluation
12/23 17:53:28 test bleu=21.50 loss=38.94 penalty=0.926 ratio=0.929
12/23 17:53:28 saving model to models/1_fold_hybrid_pnl/checkpoints
12/23 17:53:28 finished saving model
12/23 17:53:28 new best model
12/23 18:21:57 step 16000 epoch 7 learning rate 0.368 step-time 0.851 loss 32.844
12/23 18:21:57 starting evaluation
12/23 18:26:09 test bleu=23.16 loss=37.53 penalty=0.908 ratio=0.912
12/23 18:26:09 saving model to models/1_fold_hybrid_pnl/checkpoints
12/23 18:26:09 finished saving model
12/23 18:26:09 new best model
12/23 18:28:23   decaying learning rate to: 0.349
12/23 18:54:40 step 18000 epoch 8 learning rate 0.349 step-time 0.852 loss 30.150
12/23 18:54:40 starting evaluation
12/23 18:58:48 test bleu=24.87 loss=36.72 penalty=0.892 ratio=0.897
12/23 18:58:48 saving model to models/1_fold_hybrid_pnl/checkpoints
12/23 18:58:49 finished saving model
12/23 18:58:49 new best model
12/23 19:05:25   decaying learning rate to: 0.332
12/23 19:27:15 step 20000 epoch 9 learning rate 0.332 step-time 0.850 loss 27.895
12/23 19:27:15 starting evaluation
12/23 19:31:30 test bleu=21.76 loss=36.82 penalty=1.000 ratio=1.200
12/23 19:31:30 saving model to models/1_fold_hybrid_pnl/checkpoints
12/23 19:31:30 finished saving model
12/23 19:42:29   decaying learning rate to: 0.315
12/23 19:59:58 step 22000 epoch 10 learning rate 0.315 step-time 0.851 loss 25.740
12/23 19:59:58 starting evaluation
12/23 20:04:06 test bleu=27.09 loss=36.69 penalty=0.919 ratio=0.922
12/23 20:04:06 saving model to models/1_fold_hybrid_pnl/checkpoints
12/23 20:04:07 finished saving model
12/23 20:04:07 new best model
12/23 20:19:31   decaying learning rate to: 0.299
12/23 20:32:34 step 24000 epoch 11 learning rate 0.299 step-time 0.850 loss 23.833
12/23 20:32:34 starting evaluation
12/23 20:36:48 test bleu=28.53 loss=36.67 penalty=0.896 ratio=0.901
12/23 20:36:48 saving model to models/1_fold_hybrid_pnl/checkpoints
12/23 20:36:48 finished saving model
12/23 20:36:48 new best model
12/23 20:56:32   decaying learning rate to: 0.284
12/23 21:05:19 step 26000 epoch 12 learning rate 0.284 step-time 0.852 loss 21.989
12/23 21:05:19 starting evaluation
12/23 21:09:24 test bleu=29.30 loss=37.43 penalty=0.867 ratio=0.875
12/23 21:09:24 saving model to models/1_fold_hybrid_pnl/checkpoints
12/23 21:09:25 finished saving model
12/23 21:09:25 new best model
12/23 21:33:38   decaying learning rate to: 0.27
12/23 21:37:59 step 28000 epoch 13 learning rate 0.27 step-time 0.854 loss 20.116
12/23 21:37:59 starting evaluation
12/23 21:42:00 test bleu=29.59 loss=38.33 penalty=0.874 ratio=0.881
12/23 21:42:00 saving model to models/1_fold_hybrid_pnl/checkpoints
12/23 21:42:00 finished saving model
12/23 21:42:00 new best model
12/23 22:10:32 step 30000 epoch 14 learning rate 0.27 step-time 0.853 loss 18.470
12/23 22:10:32 starting evaluation
12/23 22:14:43 test bleu=30.71 loss=37.32 penalty=0.917 ratio=0.920
12/23 22:14:43 saving model to models/1_fold_hybrid_pnl/checkpoints
12/23 22:14:43 finished saving model
12/23 22:14:43 new best model
12/23 22:14:47   decaying learning rate to: 0.257
12/23 22:43:12 step 32000 epoch 14 learning rate 0.257 step-time 0.851 loss 16.261
12/23 22:43:12 starting evaluation
12/23 22:47:25 test bleu=31.70 loss=38.86 penalty=0.948 ratio=0.949
12/23 22:47:25 saving model to models/1_fold_hybrid_pnl/checkpoints
12/23 22:47:25 finished saving model
12/23 22:47:25 new best model
12/23 22:51:51   decaying learning rate to: 0.244
12/23 23:15:55 step 34000 epoch 15 learning rate 0.244 step-time 0.851 loss 14.752
12/23 23:15:55 starting evaluation
12/23 23:19:58 test bleu=31.70 loss=39.84 penalty=0.868 ratio=0.876
12/23 23:19:58 saving model to models/1_fold_hybrid_pnl/checkpoints
12/23 23:19:58 finished saving model
12/23 23:28:49   decaying learning rate to: 0.232
12/23 23:48:27 step 36000 epoch 16 learning rate 0.232 step-time 0.851 loss 13.462
12/23 23:48:27 starting evaluation
12/23 23:52:38 test bleu=32.91 loss=40.95 penalty=0.930 ratio=0.932
12/23 23:52:38 saving model to models/1_fold_hybrid_pnl/checkpoints
12/23 23:52:39 finished saving model
12/23 23:52:39 new best model
12/24 00:05:53   decaying learning rate to: 0.22
12/24 00:21:08 step 38000 epoch 17 learning rate 0.22 step-time 0.851 loss 12.043
12/24 00:21:08 starting evaluation
12/24 00:25:17 test bleu=32.53 loss=43.03 penalty=0.924 ratio=0.927
12/24 00:25:17 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 00:25:17 finished saving model
12/24 00:42:54   decaying learning rate to: 0.209
12/24 00:53:47 step 40000 epoch 18 learning rate 0.209 step-time 0.852 loss 10.914
12/24 00:53:47 starting evaluation
12/24 00:57:59 test bleu=33.59 loss=45.08 penalty=0.952 ratio=0.953
12/24 00:57:59 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 00:58:00 finished saving model
12/24 00:58:00 new best model
12/24 01:20:00   decaying learning rate to: 0.199
12/24 01:26:29 step 42000 epoch 19 learning rate 0.199 step-time 0.851 loss 9.818
12/24 01:26:29 starting evaluation
12/24 01:30:38 test bleu=33.56 loss=47.17 penalty=0.951 ratio=0.953
12/24 01:30:38 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 01:30:39 finished saving model
12/24 01:57:10   decaying learning rate to: 0.189
12/24 01:59:16 step 44000 epoch 20 learning rate 0.189 step-time 0.856 loss 8.775
12/24 01:59:16 starting evaluation
12/24 02:03:24 test bleu=34.18 loss=48.89 penalty=0.924 ratio=0.927
12/24 02:03:24 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 02:03:24 finished saving model
12/24 02:03:24 new best model
12/24 02:31:27 step 46000 epoch 20 learning rate 0.189 step-time 0.838 loss 7.642
12/24 02:31:27 starting evaluation
12/24 02:35:30 test bleu=34.77 loss=48.53 penalty=0.993 ratio=0.993
12/24 02:35:30 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 02:35:30 finished saving model
12/24 02:35:30 new best model
12/24 02:37:43   decaying learning rate to: 0.179
12/24 03:02:24 step 48000 epoch 21 learning rate 0.179 step-time 0.804 loss 6.641
12/24 03:02:24 starting evaluation
12/24 03:06:24 test bleu=34.52 loss=51.26 penalty=0.986 ratio=0.986
12/24 03:06:24 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 03:06:24 finished saving model
12/24 03:12:40   decaying learning rate to: 0.17
12/24 03:33:19 step 50000 epoch 22 learning rate 0.17 step-time 0.804 loss 5.907
12/24 03:33:19 starting evaluation
12/24 03:37:21 test bleu=34.83 loss=54.34 penalty=1.000 ratio=1.003
12/24 03:37:21 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 03:37:22 finished saving model
12/24 03:37:22 new best model
12/24 03:47:47   decaying learning rate to: 0.162
12/24 04:04:13 step 52000 epoch 23 learning rate 0.162 step-time 0.803 loss 5.279
12/24 04:04:13 starting evaluation
12/24 04:08:15 test bleu=34.86 loss=56.38 penalty=1.000 ratio=1.005
12/24 04:08:15 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 04:08:15 finished saving model
12/24 04:08:15 new best model
12/24 04:22:50   decaying learning rate to: 0.154
12/24 04:35:11 step 54000 epoch 24 learning rate 0.154 step-time 0.805 loss 4.649
12/24 04:35:11 starting evaluation
12/24 04:39:10 test bleu=34.74 loss=59.90 penalty=1.000 ratio=1.025
12/24 04:39:10 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 04:39:10 finished saving model
12/24 04:57:52   decaying learning rate to: 0.146
12/24 05:06:08 step 56000 epoch 25 learning rate 0.146 step-time 0.806 loss 4.176
12/24 05:06:08 starting evaluation
12/24 05:10:06 test bleu=35.65 loss=62.24 penalty=1.000 ratio=1.005
12/24 05:10:06 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 05:10:06 finished saving model
12/24 05:10:06 new best model
12/24 05:33:00   decaying learning rate to: 0.139
12/24 05:36:59 step 58000 epoch 26 learning rate 0.139 step-time 0.803 loss 3.674
12/24 05:36:59 starting evaluation
12/24 05:41:07 test bleu=35.79 loss=63.78 penalty=1.000 ratio=1.013
12/24 05:41:07 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 05:41:07 finished saving model
12/24 05:41:07 new best model
12/24 06:08:04 step 60000 epoch 27 learning rate 0.139 step-time 0.805 loss 3.267
12/24 06:08:04 starting evaluation
12/24 06:12:02 test bleu=36.66 loss=64.68 penalty=0.977 ratio=0.978
12/24 06:12:02 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 06:12:02 finished saving model
12/24 06:12:02 new best model
12/24 06:12:09   decaying learning rate to: 0.132
12/24 06:38:58 step 62000 epoch 27 learning rate 0.132 step-time 0.805 loss 2.721
12/24 06:38:58 starting evaluation
12/24 06:42:56 test bleu=36.36 loss=68.35 penalty=0.994 ratio=0.994
12/24 06:42:56 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 06:42:57 finished saving model
12/24 06:47:16   decaying learning rate to: 0.125
12/24 07:09:54 step 64000 epoch 28 learning rate 0.125 step-time 0.805 loss 2.440
12/24 07:09:54 starting evaluation
12/24 07:13:54 test bleu=36.32 loss=70.25 penalty=1.000 ratio=1.003
12/24 07:13:54 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 07:13:54 finished saving model
12/24 07:22:16   decaying learning rate to: 0.119
12/24 07:40:53 step 66000 epoch 29 learning rate 0.119 step-time 0.806 loss 2.172
12/24 07:40:53 starting evaluation
12/24 07:44:52 test bleu=36.66 loss=72.78 penalty=0.993 ratio=0.993
12/24 07:44:52 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 07:44:52 finished saving model
12/24 07:57:23   decaying learning rate to: 0.113
12/24 08:11:47 step 68000 epoch 30 learning rate 0.113 step-time 0.804 loss 1.943
12/24 08:11:47 starting evaluation
12/24 08:15:48 test bleu=36.41 loss=74.97 penalty=1.000 ratio=1.008
12/24 08:15:48 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 08:15:48 finished saving model
12/24 08:32:46   decaying learning rate to: 0.107
12/24 08:43:12 step 70000 epoch 31 learning rate 0.107 step-time 0.819 loss 1.730
12/24 08:43:12 starting evaluation
12/24 08:47:16 test bleu=36.34 loss=76.32 penalty=1.000 ratio=1.019
12/24 08:47:16 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 08:47:16 finished saving model
12/24 09:08:06   decaying learning rate to: 0.102
12/24 09:14:10 step 72000 epoch 32 learning rate 0.102 step-time 0.804 loss 1.565
12/24 09:14:10 starting evaluation
12/24 09:18:05 test bleu=37.00 loss=79.11 penalty=0.988 ratio=0.988
12/24 09:18:05 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 09:18:05 finished saving model
12/24 09:18:05 new best model
12/24 09:43:02   decaying learning rate to: 0.0969
12/24 09:44:59 step 74000 epoch 33 learning rate 0.0969 step-time 0.804 loss 1.399
12/24 09:44:59 starting evaluation
12/24 09:48:58 test bleu=36.71 loss=80.84 penalty=1.000 ratio=1.010
12/24 09:48:58 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 09:48:58 finished saving model
12/24 10:15:50 step 76000 epoch 33 learning rate 0.0969 step-time 0.803 loss 1.218
12/24 10:15:50 starting evaluation
12/24 10:19:48 test bleu=35.76 loss=82.93 penalty=1.000 ratio=1.039
12/24 10:19:48 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 10:19:49 finished saving model
12/24 10:22:02   decaying learning rate to: 0.092
12/24 10:45:07 step 78000 epoch 34 learning rate 0.092 step-time 0.756 loss 1.080
12/24 10:45:07 starting evaluation
12/24 10:48:08 test bleu=36.45 loss=85.00 penalty=1.000 ratio=1.022
12/24 10:48:08 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 10:48:09 finished saving model
12/24 10:53:16   decaying learning rate to: 0.0874
12/24 11:09:43 step 80000 epoch 35 learning rate 0.0874 step-time 0.645 loss 0.984
12/24 11:09:43 starting evaluation
12/24 11:12:43 test bleu=35.78 loss=85.11 penalty=1.000 ratio=1.042
12/24 11:12:43 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 11:12:44 finished saving model
12/24 11:21:12   decaying learning rate to: 0.083
12/24 11:34:20 step 82000 epoch 36 learning rate 0.083 step-time 0.646 loss 0.906
12/24 11:34:20 starting evaluation
12/24 11:37:23 test bleu=36.69 loss=87.44 penalty=1.000 ratio=1.022
12/24 11:37:23 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 11:37:23 finished saving model
12/24 11:49:04   decaying learning rate to: 0.0789
12/24 11:58:52 step 84000 epoch 37 learning rate 0.0789 step-time 0.642 loss 0.819
12/24 11:58:52 starting evaluation
12/24 12:01:53 test bleu=36.53 loss=89.41 penalty=1.000 ratio=1.022
12/24 12:01:53 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 12:01:53 finished saving model
12/24 12:16:57   decaying learning rate to: 0.0749
12/24 12:23:27 step 86000 epoch 38 learning rate 0.0749 step-time 0.645 loss 0.768
12/24 12:23:27 starting evaluation
12/24 12:26:26 test bleu=37.35 loss=89.76 penalty=1.000 ratio=1.012
12/24 12:26:26 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 12:26:27 finished saving model
12/24 12:26:27 new best model
12/24 12:44:45   decaying learning rate to: 0.0712
12/24 12:47:56 step 88000 epoch 39 learning rate 0.0712 step-time 0.643 loss 0.707
12/24 12:47:56 starting evaluation
12/24 12:50:55 test bleu=36.85 loss=91.85 penalty=1.000 ratio=1.029
12/24 12:50:55 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 12:50:55 finished saving model
12/24 13:12:29 step 90000 epoch 40 learning rate 0.0712 step-time 0.645 loss 0.659
12/24 13:12:29 starting evaluation
12/24 13:15:29 test bleu=37.13 loss=92.17 penalty=1.000 ratio=1.025
12/24 13:15:29 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 13:15:29 finished saving model
12/24 13:15:36   decaying learning rate to: 0.0676
12/24 13:37:02 step 92000 epoch 40 learning rate 0.0676 step-time 0.645 loss 0.577
12/24 13:37:02 starting evaluation
12/24 13:40:03 test bleu=37.28 loss=93.70 penalty=1.000 ratio=1.019
12/24 13:40:03 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 13:40:03 finished saving model
12/24 13:43:33   decaying learning rate to: 0.0643
12/24 14:01:42 step 94000 epoch 41 learning rate 0.0643 step-time 0.647 loss 0.548
12/24 14:01:42 starting evaluation
12/24 14:04:44 test bleu=37.22 loss=93.96 penalty=1.000 ratio=1.022
12/24 14:04:44 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 14:04:44 finished saving model
12/24 14:11:38   decaying learning rate to: 0.061
12/24 14:26:28 step 96000 epoch 42 learning rate 0.061 step-time 0.650 loss 0.510
12/24 14:26:28 starting evaluation
12/24 14:29:28 test bleu=37.50 loss=95.53 penalty=1.000 ratio=1.012
12/24 14:29:28 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 14:29:28 finished saving model
12/24 14:29:28 new best model
12/24 14:39:40   decaying learning rate to: 0.058
12/24 14:51:10 step 98000 epoch 43 learning rate 0.058 step-time 0.649 loss 0.484
12/24 14:51:10 starting evaluation
12/24 14:54:14 test bleu=37.38 loss=95.63 penalty=1.000 ratio=1.018
12/24 14:54:14 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 14:54:14 finished saving model
12/24 15:07:53   decaying learning rate to: 0.0551
12/24 15:16:34 step 100000 epoch 44 learning rate 0.0551 step-time 0.668 loss 0.453
12/24 15:16:34 starting evaluation
12/24 15:19:50 test bleu=36.64 loss=96.79 penalty=1.000 ratio=1.040
12/24 15:19:50 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 15:19:50 finished saving model
12/24 15:37:47   decaying learning rate to: 0.0523
12/24 15:42:59 step 102000 epoch 45 learning rate 0.0523 step-time 0.692 loss 0.433
12/24 15:42:59 starting evaluation
12/24 15:46:14 test bleu=37.62 loss=97.46 penalty=1.000 ratio=1.018
12/24 15:46:14 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 15:46:14 finished saving model
12/24 15:46:14 new best model
12/24 16:07:37   decaying learning rate to: 0.0497
12/24 16:09:12 step 104000 epoch 46 learning rate 0.0497 step-time 0.687 loss 0.409
12/24 16:09:12 starting evaluation
12/24 16:12:29 test bleu=36.93 loss=97.27 penalty=1.000 ratio=1.033
12/24 16:12:29 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 16:12:29 finished saving model
12/24 16:35:31 step 106000 epoch 46 learning rate 0.0497 step-time 0.689 loss 0.378
12/24 16:35:31 starting evaluation
12/24 16:38:46 test bleu=37.62 loss=98.12 penalty=1.000 ratio=1.019
12/24 16:38:46 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 16:38:46 finished saving model
12/24 16:38:46 new best model
12/24 16:40:44   decaying learning rate to: 0.0472
12/24 17:01:48 step 108000 epoch 47 learning rate 0.0472 step-time 0.689 loss 0.344
12/24 17:01:48 starting evaluation
12/24 17:05:04 test bleu=37.76 loss=98.41 penalty=1.000 ratio=1.016
12/24 17:05:04 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 17:05:04 finished saving model
12/24 17:05:04 new best model
12/24 17:10:34   decaying learning rate to: 0.0449
12/24 17:28:10 step 110000 epoch 48 learning rate 0.0449 step-time 0.691 loss 0.325
12/24 17:28:10 starting evaluation
12/24 17:31:25 test bleu=37.36 loss=98.61 penalty=1.000 ratio=1.030
12/24 17:31:25 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 17:31:25 finished saving model
12/24 17:40:28   decaying learning rate to: 0.0426
12/24 17:54:29 step 112000 epoch 49 learning rate 0.0426 step-time 0.690 loss 0.309
12/24 17:54:29 starting evaluation
12/24 17:57:42 test bleu=38.58 loss=98.53 penalty=1.000 ratio=1.002
12/24 17:57:42 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 17:57:42 finished saving model
12/24 17:57:42 new best model
12/24 18:10:20   decaying learning rate to: 0.0405
12/24 18:20:43 step 114000 epoch 50 learning rate 0.0405 step-time 0.688 loss 0.290
12/24 18:20:43 starting evaluation
12/24 18:23:58 test bleu=37.48 loss=98.68 penalty=1.000 ratio=1.023
12/24 18:23:58 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 18:23:58 finished saving model
12/24 18:40:12   decaying learning rate to: 0.0385
12/24 18:47:05 step 116000 epoch 51 learning rate 0.0385 step-time 0.691 loss 0.278
12/24 18:47:05 starting evaluation
12/24 18:50:19 test bleu=38.02 loss=98.70 penalty=1.000 ratio=1.017
12/24 18:50:19 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 18:50:19 finished saving model
12/24 19:10:03   decaying learning rate to: 0.0365
12/24 19:13:27 step 118000 epoch 52 learning rate 0.0365 step-time 0.692 loss 0.260
12/24 19:13:27 starting evaluation
12/24 19:16:42 test bleu=38.38 loss=98.45 penalty=1.000 ratio=1.003
12/24 19:16:42 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 19:16:42 finished saving model
12/24 19:39:46 step 120000 epoch 53 learning rate 0.0365 step-time 0.690 loss 0.243
12/24 19:39:46 starting evaluation
12/24 19:42:59 test bleu=38.21 loss=98.02 penalty=1.000 ratio=1.010
12/24 19:42:59 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 19:42:59 finished saving model
12/24 19:43:11   decaying learning rate to: 0.0347
12/24 20:06:06 step 122000 epoch 53 learning rate 0.0347 step-time 0.691 loss 0.217
12/24 20:06:06 starting evaluation
12/24 20:09:20 test bleu=38.03 loss=97.87 penalty=1.000 ratio=1.016
12/24 20:09:20 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 20:09:20 finished saving model
12/24 20:13:05   decaying learning rate to: 0.033
12/24 20:32:28 step 124000 epoch 54 learning rate 0.033 step-time 0.691 loss 0.205
12/24 20:32:28 starting evaluation
12/24 20:35:43 test bleu=38.64 loss=98.29 penalty=1.000 ratio=1.004
12/24 20:35:43 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 20:35:43 finished saving model
12/24 20:35:43 new best model
12/24 20:43:04   decaying learning rate to: 0.0313
12/24 20:58:52 step 126000 epoch 55 learning rate 0.0313 step-time 0.692 loss 0.201
12/24 20:58:52 starting evaluation
12/24 21:02:05 test bleu=38.61 loss=98.36 penalty=1.000 ratio=1.003
12/24 21:02:05 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 21:02:05 finished saving model
12/24 21:13:00   decaying learning rate to: 0.0298
12/24 21:25:10 step 128000 epoch 56 learning rate 0.0298 step-time 0.690 loss 0.189
12/24 21:25:10 starting evaluation
12/24 21:28:24 test bleu=37.81 loss=98.94 penalty=1.000 ratio=1.027
12/24 21:28:24 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 21:28:24 finished saving model
12/24 21:42:51   decaying learning rate to: 0.0283
12/24 21:51:28 step 130000 epoch 57 learning rate 0.0283 step-time 0.690 loss 0.177
12/24 21:51:28 starting evaluation
12/24 21:54:43 test bleu=38.01 loss=99.11 penalty=1.000 ratio=1.017
12/24 21:54:43 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 21:54:43 finished saving model
12/24 22:12:46   decaying learning rate to: 0.0269
12/24 22:17:53 step 132000 epoch 58 learning rate 0.0269 step-time 0.692 loss 0.172
12/24 22:17:53 starting evaluation
12/24 22:21:08 test bleu=37.70 loss=98.64 penalty=1.000 ratio=1.031
12/24 22:21:08 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 22:21:09 finished saving model
12/24 22:42:42   decaying learning rate to: 0.0255
12/24 22:44:14 step 134000 epoch 59 learning rate 0.0255 step-time 0.690 loss 0.164
12/24 22:44:14 starting evaluation
12/24 22:47:30 test bleu=38.53 loss=99.03 penalty=1.000 ratio=1.007
12/24 22:47:30 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 22:47:30 finished saving model
12/24 23:10:39 step 136000 epoch 59 learning rate 0.0255 step-time 0.692 loss 0.152
12/24 23:10:39 starting evaluation
12/24 23:13:54 test bleu=38.81 loss=98.74 penalty=0.998 ratio=0.998
12/24 23:13:54 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 23:13:54 finished saving model
12/24 23:13:54 new best model
12/24 23:15:53   decaying learning rate to: 0.0242
12/24 23:36:58 step 138000 epoch 60 learning rate 0.0242 step-time 0.690 loss 0.146
12/24 23:36:58 starting evaluation
12/24 23:40:15 test bleu=38.14 loss=99.03 penalty=1.000 ratio=1.018
12/24 23:40:15 saving model to models/1_fold_hybrid_pnl/checkpoints
12/24 23:40:15 finished saving model
12/24 23:45:48   decaying learning rate to: 0.023
12/25 00:03:26 step 140000 epoch 61 learning rate 0.023 step-time 0.693 loss 0.138
12/25 00:03:26 starting evaluation
12/25 00:06:41 test bleu=38.80 loss=98.95 penalty=1.000 ratio=1.001
12/25 00:06:42 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 00:06:42 finished saving model
12/25 00:15:50   decaying learning rate to: 0.0219
12/25 00:29:52 step 142000 epoch 62 learning rate 0.0219 step-time 0.693 loss 0.135
12/25 00:29:52 starting evaluation
12/25 00:33:07 test bleu=38.11 loss=99.60 penalty=1.000 ratio=1.016
12/25 00:33:07 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 00:33:07 finished saving model
12/25 00:45:49   decaying learning rate to: 0.0208
12/25 00:56:16 step 144000 epoch 63 learning rate 0.0208 step-time 0.692 loss 0.129
12/25 00:56:16 starting evaluation
12/25 00:59:34 test bleu=38.20 loss=99.25 penalty=1.000 ratio=1.018
12/25 00:59:34 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 00:59:34 finished saving model
12/25 01:15:47   decaying learning rate to: 0.0197
12/25 01:22:43 step 146000 epoch 64 learning rate 0.0197 step-time 0.692 loss 0.129
12/25 01:22:43 starting evaluation
12/25 01:25:56 test bleu=38.80 loss=99.79 penalty=1.000 ratio=1.004
12/25 01:25:56 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 01:25:56 finished saving model
12/25 01:45:50   decaying learning rate to: 0.0188
12/25 01:49:10 step 148000 epoch 65 learning rate 0.0188 step-time 0.695 loss 0.122
12/25 01:49:10 starting evaluation
12/25 01:52:23 test bleu=38.54 loss=99.38 penalty=1.000 ratio=1.010
12/25 01:52:23 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 01:52:23 finished saving model
12/25 02:15:25 step 150000 epoch 66 learning rate 0.0188 step-time 0.689 loss 0.119
12/25 02:15:25 starting evaluation
12/25 02:18:38 test bleu=38.85 loss=99.55 penalty=0.998 ratio=0.998
12/25 02:18:38 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 02:18:38 finished saving model
12/25 02:18:38 new best model
12/25 02:18:52   decaying learning rate to: 0.0178
12/25 02:41:46 step 152000 epoch 66 learning rate 0.0178 step-time 0.692 loss 0.109
12/25 02:41:46 starting evaluation
12/25 02:44:59 test bleu=38.74 loss=99.82 penalty=1.000 ratio=1.002
12/25 02:44:59 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 02:45:00 finished saving model
12/25 02:48:48   decaying learning rate to: 0.0169
12/25 03:08:12 step 154000 epoch 67 learning rate 0.0169 step-time 0.694 loss 0.107
12/25 03:08:12 starting evaluation
12/25 03:11:26 test bleu=38.48 loss=99.86 penalty=1.000 ratio=1.010
12/25 03:11:26 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 03:11:26 finished saving model
12/25 03:18:46   decaying learning rate to: 0.0161
12/25 03:34:35 step 156000 epoch 68 learning rate 0.0161 step-time 0.693 loss 0.104
12/25 03:34:35 starting evaluation
12/25 03:37:48 test bleu=38.60 loss=100.31 penalty=1.000 ratio=1.010
12/25 03:37:48 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 03:37:49 finished saving model
12/25 03:48:44   decaying learning rate to: 0.0153
12/25 04:00:58 step 158000 epoch 69 learning rate 0.0153 step-time 0.692 loss 0.103
12/25 04:00:58 starting evaluation
12/25 04:04:11 test bleu=38.53 loss=100.85 penalty=1.000 ratio=1.010
12/25 04:04:11 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 04:04:12 finished saving model
12/25 04:18:43   decaying learning rate to: 0.0145
12/25 04:27:21 step 160000 epoch 70 learning rate 0.0145 step-time 0.693 loss 0.099
12/25 04:27:21 starting evaluation
12/25 04:30:35 test bleu=38.09 loss=100.45 penalty=1.000 ratio=1.023
12/25 04:30:35 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 04:30:35 finished saving model
12/25 04:48:40   decaying learning rate to: 0.0138
12/25 04:53:43 step 162000 epoch 71 learning rate 0.0138 step-time 0.692 loss 0.097
12/25 04:53:43 starting evaluation
12/25 04:56:59 test bleu=38.43 loss=100.65 penalty=1.000 ratio=1.013
12/25 04:56:59 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 04:56:59 finished saving model
12/25 05:18:35   decaying learning rate to: 0.0131
12/25 05:20:07 step 164000 epoch 72 learning rate 0.0131 step-time 0.692 loss 0.095
12/25 05:20:07 starting evaluation
12/25 05:23:23 test bleu=38.22 loss=100.61 penalty=1.000 ratio=1.018
12/25 05:23:23 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 05:23:24 finished saving model
12/25 05:46:33 step 166000 epoch 72 learning rate 0.0131 step-time 0.693 loss 0.091
12/25 05:46:33 starting evaluation
12/25 05:49:48 test bleu=38.43 loss=100.98 penalty=1.000 ratio=1.016
12/25 05:49:48 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 05:49:48 finished saving model
12/25 05:51:47   decaying learning rate to: 0.0124
12/25 06:12:54 step 168000 epoch 73 learning rate 0.0124 step-time 0.691 loss 0.088
12/25 06:12:54 starting evaluation
12/25 06:16:09 test bleu=38.53 loss=101.23 penalty=1.000 ratio=1.009
12/25 06:16:09 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 06:16:09 finished saving model
12/25 06:21:42   decaying learning rate to: 0.0118
12/25 06:39:15 step 170000 epoch 74 learning rate 0.0118 step-time 0.691 loss 0.085
12/25 06:39:15 starting evaluation
12/25 06:42:31 test bleu=38.58 loss=100.99 penalty=1.000 ratio=1.008
12/25 06:42:31 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 06:42:31 finished saving model
12/25 06:51:40   decaying learning rate to: 0.0112
12/25 07:05:39 step 172000 epoch 75 learning rate 0.0112 step-time 0.692 loss 0.083
12/25 07:05:39 starting evaluation
12/25 07:08:53 test bleu=38.49 loss=101.28 penalty=1.000 ratio=1.009
12/25 07:08:53 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 07:08:53 finished saving model
12/25 07:21:33   decaying learning rate to: 0.0107
12/25 07:31:57 step 174000 epoch 76 learning rate 0.0107 step-time 0.689 loss 0.082
12/25 07:31:57 starting evaluation
12/25 07:35:11 test bleu=38.13 loss=101.52 penalty=1.000 ratio=1.019
12/25 07:35:11 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 07:35:11 finished saving model
12/25 07:51:24   decaying learning rate to: 0.0101
12/25 07:58:14 step 176000 epoch 77 learning rate 0.0101 step-time 0.689 loss 0.081
12/25 07:58:14 starting evaluation
12/25 08:01:29 test bleu=38.25 loss=101.71 penalty=1.000 ratio=1.019
12/25 08:01:29 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 08:01:29 finished saving model
12/25 08:21:20   decaying learning rate to: 0.00963
12/25 08:24:47 step 178000 epoch 78 learning rate 0.00963 step-time 0.697 loss 0.079
12/25 08:24:47 starting evaluation
12/25 08:28:21 test bleu=38.25 loss=101.66 penalty=1.000 ratio=1.015
12/25 08:28:21 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 08:28:21 finished saving model
12/25 08:53:18 step 180000 epoch 79 learning rate 0.00963 step-time 0.746 loss 0.078
12/25 08:53:18 starting evaluation
12/25 08:56:53 test bleu=38.29 loss=101.76 penalty=1.000 ratio=1.016
12/25 08:56:53 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 08:56:54 finished saving model
12/25 08:57:12   decaying learning rate to: 0.00915
12/25 09:21:56 step 182000 epoch 79 learning rate 0.00915 step-time 0.748 loss 0.074
12/25 09:21:56 starting evaluation
12/25 09:25:30 test bleu=38.39 loss=101.63 penalty=1.000 ratio=1.015
12/25 09:25:30 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 09:25:30 finished saving model
12/25 09:29:37   decaying learning rate to: 0.00869
12/25 09:50:30 step 184000 epoch 80 learning rate 0.00869 step-time 0.747 loss 0.072
12/25 09:50:30 starting evaluation
12/25 09:54:04 test bleu=38.50 loss=102.26 penalty=1.000 ratio=1.010
12/25 09:54:04 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 09:54:04 finished saving model
12/25 10:02:04   decaying learning rate to: 0.00826
12/25 10:19:04 step 186000 epoch 81 learning rate 0.00826 step-time 0.747 loss 0.072
12/25 10:19:04 starting evaluation
12/25 10:22:39 test bleu=38.59 loss=102.14 penalty=1.000 ratio=1.006
12/25 10:22:39 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 10:22:39 finished saving model
12/25 10:34:31   decaying learning rate to: 0.00784
12/25 10:47:43 step 188000 epoch 82 learning rate 0.00784 step-time 0.749 loss 0.071
12/25 10:47:43 starting evaluation
12/25 10:51:17 test bleu=38.49 loss=102.46 penalty=1.000 ratio=1.007
12/25 10:51:17 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 10:51:17 finished saving model
12/25 11:06:54   decaying learning rate to: 0.00745
12/25 11:16:12 step 190000 epoch 83 learning rate 0.00745 step-time 0.745 loss 0.070
12/25 11:16:12 starting evaluation
12/25 11:19:45 test bleu=38.51 loss=102.25 penalty=1.000 ratio=1.010
12/25 11:19:45 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 11:19:46 finished saving model
12/25 11:39:19   decaying learning rate to: 0.00708
12/25 11:44:50 step 192000 epoch 84 learning rate 0.00708 step-time 0.749 loss 0.069
12/25 11:44:50 starting evaluation
12/25 11:48:24 test bleu=38.37 loss=102.50 penalty=1.000 ratio=1.015
12/25 11:48:24 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 11:48:24 finished saving model
12/25 12:11:50   decaying learning rate to: 0.00673
12/25 12:13:26 step 194000 epoch 85 learning rate 0.00673 step-time 0.748 loss 0.070
12/25 12:13:26 starting evaluation
12/25 12:17:00 test bleu=38.47 loss=102.54 penalty=1.000 ratio=1.010
12/25 12:17:00 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 12:17:00 finished saving model
12/25 12:42:00 step 196000 epoch 85 learning rate 0.00673 step-time 0.747 loss 0.067
12/25 12:42:00 starting evaluation
12/25 12:45:35 test bleu=38.16 loss=102.53 penalty=1.000 ratio=1.020
12/25 12:45:35 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 12:45:35 finished saving model
12/25 12:47:49   decaying learning rate to: 0.00639
12/25 13:10:37 step 198000 epoch 86 learning rate 0.00639 step-time 0.748 loss 0.065
12/25 13:10:37 starting evaluation
12/25 13:14:12 test bleu=38.33 loss=102.84 penalty=1.000 ratio=1.015
12/25 13:14:12 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 13:14:12 finished saving model
12/25 13:20:19   decaying learning rate to: 0.00607
12/25 13:39:12 step 200000 epoch 87 learning rate 0.00607 step-time 0.747 loss 0.064
12/25 13:39:12 starting evaluation
12/25 13:42:46 test bleu=38.22 loss=102.91 penalty=1.000 ratio=1.018
12/25 13:42:46 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 13:42:46 finished saving model
12/25 13:52:39   decaying learning rate to: 0.00577
12/25 14:07:44 step 202000 epoch 88 learning rate 0.00577 step-time 0.746 loss 0.063
12/25 14:07:44 starting evaluation
12/25 14:11:19 test bleu=38.14 loss=103.05 penalty=1.000 ratio=1.019
12/25 14:11:19 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 14:11:19 finished saving model
12/25 14:25:06   decaying learning rate to: 0.00548
12/25 14:36:20 step 204000 epoch 89 learning rate 0.00548 step-time 0.748 loss 0.064
12/25 14:36:20 starting evaluation
12/25 14:39:54 test bleu=38.40 loss=102.96 penalty=1.000 ratio=1.013
12/25 14:39:54 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 14:39:54 finished saving model
12/25 14:57:32   decaying learning rate to: 0.0052
12/25 15:04:54 step 206000 epoch 90 learning rate 0.0052 step-time 0.747 loss 0.062
12/25 15:04:54 starting evaluation
12/25 15:08:28 test bleu=38.37 loss=103.11 penalty=1.000 ratio=1.014
12/25 15:08:28 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 15:08:28 finished saving model
12/25 15:29:56   decaying learning rate to: 0.00494
12/25 15:33:26 step 208000 epoch 91 learning rate 0.00494 step-time 0.746 loss 0.063
12/25 15:33:26 starting evaluation
12/25 15:36:59 test bleu=38.49 loss=103.06 penalty=1.000 ratio=1.011
12/25 15:36:59 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 15:37:00 finished saving model
12/25 16:01:57 step 210000 epoch 92 learning rate 0.00494 step-time 0.746 loss 0.062
12/25 16:01:57 starting evaluation
12/25 16:05:32 test bleu=38.07 loss=103.16 penalty=1.000 ratio=1.022
12/25 16:05:32 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 16:05:32 finished saving model
12/25 16:05:53   decaying learning rate to: 0.0047
12/25 16:30:24 step 212000 epoch 92 learning rate 0.0047 step-time 0.743 loss 0.060
12/25 16:30:24 starting evaluation
12/25 16:33:59 test bleu=38.31 loss=103.24 penalty=1.000 ratio=1.014
12/25 16:33:59 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 16:33:59 finished saving model
12/25 16:38:13   decaying learning rate to: 0.00446
12/25 16:59:01 step 214000 epoch 93 learning rate 0.00446 step-time 0.748 loss 0.059
12/25 16:59:01 starting evaluation
12/25 17:02:34 test bleu=38.22 loss=103.38 penalty=1.000 ratio=1.018
12/25 17:02:34 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 17:02:34 finished saving model
12/25 17:10:37   decaying learning rate to: 0.00424
12/25 17:27:29 step 216000 epoch 94 learning rate 0.00424 step-time 0.745 loss 0.059
12/25 17:27:29 starting evaluation
12/25 17:31:03 test bleu=38.39 loss=103.51 penalty=1.000 ratio=1.013
12/25 17:31:03 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 17:31:03 finished saving model
12/25 17:43:00   decaying learning rate to: 0.00403
12/25 17:56:02 step 218000 epoch 95 learning rate 0.00403 step-time 0.747 loss 0.057
12/25 17:56:02 starting evaluation
12/25 17:59:37 test bleu=38.41 loss=103.60 penalty=1.000 ratio=1.013
12/25 17:59:37 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 17:59:37 finished saving model
12/25 18:15:26   decaying learning rate to: 0.00383
12/25 18:24:44 step 220000 epoch 96 learning rate 0.00383 step-time 0.751 loss 0.059
12/25 18:24:44 starting evaluation
12/25 18:28:17 test bleu=38.33 loss=103.69 penalty=1.000 ratio=1.015
12/25 18:28:17 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 18:28:18 finished saving model
12/25 18:47:54   decaying learning rate to: 0.00363
12/25 18:53:18 step 222000 epoch 97 learning rate 0.00363 step-time 0.748 loss 0.059
12/25 18:53:18 starting evaluation
12/25 18:56:52 test bleu=38.55 loss=103.70 penalty=1.000 ratio=1.009
12/25 18:56:52 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 18:56:52 finished saving model
12/25 19:20:20   decaying learning rate to: 0.00345
12/25 19:21:56 step 224000 epoch 98 learning rate 0.00345 step-time 0.749 loss 0.058
12/25 19:21:56 starting evaluation
12/25 19:25:31 test bleu=38.48 loss=103.79 penalty=1.000 ratio=1.011
12/25 19:25:31 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 19:25:32 finished saving model
12/25 19:50:28 step 226000 epoch 98 learning rate 0.00345 step-time 0.745 loss 0.056
12/25 19:50:28 starting evaluation
12/25 19:54:03 test bleu=38.17 loss=103.82 penalty=1.000 ratio=1.018
12/25 19:54:03 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 19:54:03 finished saving model
12/25 19:56:24   decaying learning rate to: 0.00328
12/25 20:19:06 step 228000 epoch 99 learning rate 0.00328 step-time 0.748 loss 0.056
12/25 20:19:06 starting evaluation
12/25 20:22:40 test bleu=38.41 loss=104.02 penalty=1.000 ratio=1.012
12/25 20:22:40 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 20:22:40 finished saving model
12/25 20:28:49   decaying learning rate to: 0.00312
12/25 20:47:41 step 230000 epoch 100 learning rate 0.00312 step-time 0.748 loss 0.055
12/25 20:47:41 starting evaluation
12/25 20:51:15 test bleu=38.44 loss=103.95 penalty=1.000 ratio=1.014
12/25 20:51:15 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 20:51:15 finished saving model
12/25 21:00:20 finished training
12/25 21:00:20 exiting...
12/25 21:00:20 saving model to models/1_fold_hybrid_pnl/checkpoints
12/25 21:00:20 finished saving model
