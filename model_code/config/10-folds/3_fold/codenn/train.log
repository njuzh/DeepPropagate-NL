nohup: ignoring input
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:107: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:30: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

12/26 11:04:30 label: default
12/26 11:04:30 description:
  default configuration
  next line of description
  last line
12/26 11:04:30 /root/icpc/icpc/translate/__main__.py config/10-folds/3_fold/codenn/config.yaml --train -v
12/26 11:04:30 commit hash 74e0554cb3eb5df835cef993ad570ff8de651f71
12/26 11:04:30 tensorflow version: 1.14.0
12/26 11:04:30 program arguments
12/26 11:04:30   aggregation_method   'sum'
12/26 11:04:30   align_encoder_id     0
12/26 11:04:30   allow_growth         True
12/26 11:04:30   attention_type       'global'
12/26 11:04:30   attn_filter_length   0
12/26 11:04:30   attn_filters         0
12/26 11:04:30   attn_prev_word       False
12/26 11:04:30   attn_size            128
12/26 11:04:30   attn_temperature     1.0
12/26 11:04:30   attn_window_size     0
12/26 11:04:30   average              False
12/26 11:04:30   baseline_activation  None
12/26 11:04:30   baseline_learning_rate 0.001
12/26 11:04:30   baseline_optimizer   'adam'
12/26 11:04:30   baseline_steps       0
12/26 11:04:30   batch_mode           'standard'
12/26 11:04:30   batch_size           64
12/26 11:04:30   beam_size            5
12/26 11:04:30   bidir                True
12/26 11:04:30   bidir_projection     False
12/26 11:04:30   binary               False
12/26 11:04:30   cell_size            256
12/26 11:04:30   cell_type            'GRU'
12/26 11:04:30   character_level      False
12/26 11:04:30   checkpoints          []
12/26 11:04:30   conditional_rnn      False
12/26 11:04:30   config               'config/10-folds/3_fold/codenn/config.yaml'
12/26 11:04:30   convolutions         None
12/26 11:04:30   data_dir             'data/gooddata/3_fold'
12/26 11:04:30   debug                False
12/26 11:04:30   decay_after_n_epoch  1
12/26 11:04:30   decay_every_n_epoch  1
12/26 11:04:30   decay_if_no_progress None
12/26 11:04:30   decoders             [{'max_len': 40, 'name': 'nl'}]
12/26 11:04:30   description          'default configuration\nnext line of description\nlast line\n'
12/26 11:04:30   dev_prefix           'test'
12/26 11:04:30   early_stopping       True
12/26 11:04:30   embedding_dropout    0.0
12/26 11:04:30   embedding_initializer None
12/26 11:04:30   embedding_size       256
12/26 11:04:30   embedding_weight_scale None
12/26 11:04:30   embeddings_on_cpu    True
12/26 11:04:30   encoders             [{'attention_type': 'global', 'max_len': 200, 'name': 'code'}]
12/26 11:04:30   ensemble             False
12/26 11:04:30   eval_burn_in         0
12/26 11:04:30   feed_previous        0.0
12/26 11:04:30   final_state          'last'
12/26 11:04:30   freeze_variables     []
12/26 11:04:30   generate_first       True
12/26 11:04:30   gpu_id               1
12/26 11:04:30   highway_layers       0
12/26 11:04:30   initial_state_dropout 0.0
12/26 11:04:30   initializer          None
12/26 11:04:30   input_layer_dropout  0.0
12/26 11:04:30   input_layers         None
12/26 11:04:30   keep_best            5
12/26 11:04:30   keep_every_n_hours   0
12/26 11:04:30   label                'default'
12/26 11:04:30   layer_norm           False
12/26 11:04:30   layers               1
12/26 11:04:30   learning_rate        0.5
12/26 11:04:30   learning_rate_decay_factor 0.95
12/26 11:04:30   len_normalization    1.0
12/26 11:04:30   log_file             'log.txt'
12/26 11:04:30   loss_function        'xent'
12/26 11:04:30   max_dev_size         0
12/26 11:04:30   max_epochs           100
12/26 11:04:30   max_gradient_norm    5.0
12/26 11:04:30   max_len              50
12/26 11:04:30   max_steps            600000
12/26 11:04:30   max_test_size        0
12/26 11:04:30   max_to_keep          1
12/26 11:04:30   max_train_size       0
12/26 11:04:30   maxout_stride        None
12/26 11:04:30   mem_fraction         1.0
12/26 11:04:30   min_learning_rate    1e-06
12/26 11:04:30   model_dir            'models/3_fold_codenn'
12/26 11:04:30   moving_average       None
12/26 11:04:30   no_gpu               False
12/26 11:04:30   optimizer            'sgd'
12/26 11:04:30   orthogonal_init      False
12/26 11:04:30   output               None
12/26 11:04:30   output_dropout       0.0
12/26 11:04:30   parallel_iterations  16
12/26 11:04:30   pervasive_dropout    False
12/26 11:04:30   pooling_avg          True
12/26 11:04:30   post_process_script  None
12/26 11:04:30   pred_deep_layer      False
12/26 11:04:30   pred_edits           False
12/26 11:04:30   pred_embed_proj      True
12/26 11:04:30   pred_maxout_layer    True
12/26 11:04:30   purge                False
12/26 11:04:30   raw_output           False
12/26 11:04:30   read_ahead           1
12/26 11:04:30   reconstruction_attn_weight 0.05
12/26 11:04:30   reconstruction_decoders False
12/26 11:04:30   reconstruction_weight 1.0
12/26 11:04:30   reinforce_after_n_epoch None
12/26 11:04:30   remove_unk           False
12/26 11:04:30   reverse              False
12/26 11:04:30   reverse_input        True
12/26 11:04:30   reward_function      'sentence_bleu'
12/26 11:04:30   rnn_feed_attn        True
12/26 11:04:30   rnn_input_dropout    0.0
12/26 11:04:30   rnn_output_dropout   0.0
12/26 11:04:30   rnn_state_dropout    0.0
12/26 11:04:30   save                 False
12/26 11:04:30   score_function       'corpus_bleu'
12/26 11:04:30   score_functions      ['bleu', 'loss']
12/26 11:04:30   script_dir           'scripts'
12/26 11:04:30   sgd_after_n_epoch    None
12/26 11:04:30   sgd_learning_rate    1.0
12/26 11:04:30   shuffle              True
12/26 11:04:30   softmax_temperature  1.0
12/26 11:04:30   steps_per_checkpoint 2000
12/26 11:04:30   steps_per_eval       2000
12/26 11:04:30   swap_memory          True
12/26 11:04:30   tie_embeddings       False
12/26 11:04:30   time_pooling         None
12/26 11:04:30   train                True
12/26 11:04:30   train_initial_states True
12/26 11:04:30   train_prefix         'train'
12/26 11:04:30   truncate_lines       True
12/26 11:04:30   update_first         False
12/26 11:04:30   use_baseline         False
12/26 11:04:30   use_dropout          False
12/26 11:04:30   use_lstm_full_state  False
12/26 11:04:30   use_previous_word    True
12/26 11:04:30   verbose              True
12/26 11:04:30   vocab_prefix         'vocab'
12/26 11:04:30   weight_scale         None
12/26 11:04:30   word_dropout         0.0
12/26 11:04:30 python random seed: 3902113780510178982
12/26 11:04:30 tf random seed:     5811404809445459889
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:203: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

12/26 11:04:30 creating model
12/26 11:04:30 using device: /gpu:1
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:230: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

12/26 11:04:30 copying vocab to models/3_fold_codenn/data/vocab.code
12/26 11:04:30 copying vocab to models/3_fold_codenn/data/vocab.nl
12/26 11:04:30 reading vocabularies
12/26 11:04:30 creating model
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:60: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:111: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:33: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fe3a459c6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fe3a459c6d8>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:226: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fe3a459c978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fe3a459c978>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe3a4591d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe3a4591d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:838: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe4293bf860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe4293bf860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe429384a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe429384a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:432: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:435: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe4271c1e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe4271c1e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe4271c1eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe4271c1eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe3a4576c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe3a4576c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe42722d8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe42722d8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe42722d8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe42722d8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:919: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fe426ec5b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fe426ec5b70>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:131: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:223: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fe3cf9bfe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fe3cf9bfe10>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe3cf97aa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe3cf97aa20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe3cf909da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe3cf909da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe3cf909ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe3cf909ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe3cf909ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe3cf909ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe3cf909ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe3cf909ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
12/26 11:04:35 model parameters (30)
12/26 11:04:35   baseline_step:0 ()
12/26 11:04:35   decoder_nl/attention_code/U_a/kernel:0 (512, 128)
12/26 11:04:35   decoder_nl/attention_code/W_a/bias:0 (128,)
12/26 11:04:35   decoder_nl/attention_code/W_a/kernel:0 (256, 128)
12/26 11:04:35   decoder_nl/attention_code/v_a:0 (128,)
12/26 11:04:35   decoder_nl/code/initial_state_projection/bias:0 (256,)
12/26 11:04:35   decoder_nl/code/initial_state_projection/kernel:0 (256, 256)
12/26 11:04:35   decoder_nl/gru_cell/candidate/bias:0 (256,)
12/26 11:04:35   decoder_nl/gru_cell/candidate/kernel:0 (1024, 256)
12/26 11:04:35   decoder_nl/gru_cell/gates/bias:0 (512,)
12/26 11:04:35   decoder_nl/gru_cell/gates/kernel:0 (1024, 512)
12/26 11:04:35   decoder_nl/maxout/bias:0 (256,)
12/26 11:04:35   decoder_nl/maxout/kernel:0 (1024, 256)
12/26 11:04:35   decoder_nl/softmax0/kernel:0 (128, 256)
12/26 11:04:35   decoder_nl/softmax1/bias:0 (37948,)
12/26 11:04:35   decoder_nl/softmax1/kernel:0 (256, 37948)
12/26 11:04:35   embedding_code:0 (50000, 256)
12/26 11:04:35   embedding_nl:0 (37948, 256)
12/26 11:04:35   encoder_code/initial_state_bw:0 (256,)
12/26 11:04:35   encoder_code/initial_state_fw:0 (256,)
12/26 11:04:35   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/26 11:04:35   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/26 11:04:35   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/26 11:04:35   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/26 11:04:35   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/26 11:04:35   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/26 11:04:35   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/26 11:04:35   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/26 11:04:35   global_step:0 ()
12/26 11:04:35   learning_rate:0 ()
12/26 11:04:35 number of parameters: 34.30M
WARNING:tensorflow:From /root/icpc/icpc/translate/translation_model.py:666: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

12/26 11:04:36 global step: 0
12/26 11:04:36 baseline step: 0
12/26 11:04:36 reading training data
12/26 11:04:36 total line count: 156721
12/26 11:04:40   lines read: 100000
12/26 11:04:42 files: data/gooddata/3_fold/train.code data/gooddata/3_fold/train.nl
12/26 11:04:42 lines reads: 156721
12/26 11:04:42 reading development data
12/26 11:04:43 files: data/gooddata/3_fold/test.code data/gooddata/3_fold/test.nl
12/26 11:04:43 lines reads: 17413
12/26 11:04:43 starting training
12/26 11:22:27 step 2000 epoch 1 learning rate 0.5 step-time 0.530 loss 78.337
12/26 11:22:27 starting evaluation
12/26 11:26:37 test bleu=2.03 loss=63.64 penalty=0.992 ratio=0.992
12/26 11:26:37 saving model to models/3_fold_codenn/checkpoints
12/26 11:26:37 finished saving model
12/26 11:26:37 new best model
12/26 11:31:44   decaying learning rate to: 0.475
12/26 11:49:02 step 4000 epoch 2 learning rate 0.475 step-time 0.670 loss 58.926
12/26 11:49:02 starting evaluation
12/26 11:53:13 test bleu=3.29 loss=55.99 penalty=1.000 ratio=1.607
12/26 11:53:13 saving model to models/3_fold_codenn/checkpoints
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
12/26 11:53:13 finished saving model
12/26 11:53:13 new best model
12/26 12:03:28   decaying learning rate to: 0.451
12/26 12:15:38 step 6000 epoch 3 learning rate 0.451 step-time 0.670 loss 52.236
12/26 12:15:38 starting evaluation
12/26 12:19:29 test bleu=7.22 loss=50.56 penalty=0.801 ratio=0.818
12/26 12:19:29 saving model to models/3_fold_codenn/checkpoints
12/26 12:19:29 finished saving model
12/26 12:19:29 new best model
12/26 12:34:50   decaying learning rate to: 0.429
12/26 12:42:01 step 8000 epoch 4 learning rate 0.429 step-time 0.674 loss 47.834
12/26 12:42:01 starting evaluation
12/26 12:46:11 test bleu=10.06 loss=47.76 penalty=0.926 ratio=0.928
12/26 12:46:11 saving model to models/3_fold_codenn/checkpoints
12/26 12:46:12 finished saving model
12/26 12:46:12 new best model
12/26 13:06:27   decaying learning rate to: 0.407
12/26 13:08:40 step 10000 epoch 5 learning rate 0.407 step-time 0.672 loss 44.183
12/26 13:08:40 starting evaluation
12/26 13:12:50 test bleu=10.83 loss=45.25 penalty=1.000 ratio=1.095
12/26 13:12:50 saving model to models/3_fold_codenn/checkpoints
12/26 13:12:51 finished saving model
12/26 13:12:51 new best model
12/26 13:35:16 step 12000 epoch 5 learning rate 0.407 step-time 0.671 loss 40.885
12/26 13:35:16 starting evaluation
12/26 13:39:17 test bleu=12.65 loss=43.81 penalty=0.822 ratio=0.836
12/26 13:39:17 saving model to models/3_fold_codenn/checkpoints
12/26 13:39:17 finished saving model
12/26 13:39:17 new best model
12/26 13:42:05   decaying learning rate to: 0.387
12/26 14:01:45 step 14000 epoch 6 learning rate 0.387 step-time 0.672 loss 37.902
12/26 14:01:45 starting evaluation
12/26 14:05:47 test bleu=14.34 loss=42.07 penalty=0.843 ratio=0.854
12/26 14:05:47 saving model to models/3_fold_codenn/checkpoints
12/26 14:05:47 finished saving model
12/26 14:05:47 new best model
12/26 14:13:41   decaying learning rate to: 0.368
12/26 14:28:13 step 16000 epoch 7 learning rate 0.368 step-time 0.671 loss 35.344
12/26 14:28:13 starting evaluation
12/26 14:32:03 test bleu=16.67 loss=41.09 penalty=0.797 ratio=0.815
12/26 14:32:03 saving model to models/3_fold_codenn/checkpoints
12/26 14:32:03 finished saving model
12/26 14:32:03 new best model
12/26 14:45:04   decaying learning rate to: 0.349
12/26 14:54:33 step 18000 epoch 8 learning rate 0.349 step-time 0.673 loss 33.332
12/26 14:54:33 starting evaluation
12/26 14:58:37 test bleu=18.71 loss=40.43 penalty=0.883 ratio=0.890
12/26 14:58:37 saving model to models/3_fold_codenn/checkpoints
12/26 14:58:37 finished saving model
12/26 14:58:37 new best model
12/26 15:16:45   decaying learning rate to: 0.332
12/26 15:21:07 step 20000 epoch 9 learning rate 0.332 step-time 0.673 loss 31.242
12/26 15:21:07 starting evaluation
12/26 15:24:56 test bleu=19.48 loss=40.31 penalty=0.798 ratio=0.816
12/26 15:24:56 saving model to models/3_fold_codenn/checkpoints
12/26 15:24:56 finished saving model
12/26 15:24:56 new best model
12/26 15:47:23 step 22000 epoch 9 learning rate 0.332 step-time 0.671 loss 29.380
12/26 15:47:23 starting evaluation
12/26 15:51:23 test bleu=19.98 loss=39.66 penalty=0.848 ratio=0.858
12/26 15:51:23 saving model to models/3_fold_codenn/checkpoints
12/26 15:51:23 finished saving model
12/26 15:51:23 new best model
12/26 15:51:52   decaying learning rate to: 0.315
12/26 16:13:50 step 24000 epoch 10 learning rate 0.315 step-time 0.671 loss 26.744
12/26 16:13:50 starting evaluation
12/26 16:17:39 test bleu=21.50 loss=39.69 penalty=0.787 ratio=0.807
12/26 16:17:39 saving model to models/3_fold_codenn/checkpoints
12/26 16:17:39 finished saving model
12/26 16:17:39 new best model
12/26 16:23:14   decaying learning rate to: 0.299
12/26 16:40:09 step 26000 epoch 11 learning rate 0.299 step-time 0.673 loss 25.112
12/26 16:40:09 starting evaluation
12/26 16:44:09 test bleu=23.11 loss=40.07 penalty=0.904 ratio=0.908
12/26 16:44:09 saving model to models/3_fold_codenn/checkpoints
12/26 16:44:09 finished saving model
12/26 16:44:09 new best model
12/26 16:54:50   decaying learning rate to: 0.284
12/26 17:06:37 step 28000 epoch 12 learning rate 0.284 step-time 0.672 loss 23.481
12/26 17:06:37 starting evaluation
12/26 17:10:41 test bleu=24.60 loss=40.06 penalty=0.938 ratio=0.940
12/26 17:10:41 saving model to models/3_fold_codenn/checkpoints
12/26 17:10:42 finished saving model
12/26 17:10:42 new best model
12/26 17:26:30   decaying learning rate to: 0.27
12/26 17:33:13 step 30000 epoch 13 learning rate 0.27 step-time 0.674 loss 22.083
12/26 17:33:13 starting evaluation
12/26 17:37:16 test bleu=25.33 loss=40.95 penalty=0.900 ratio=0.904
12/26 17:37:16 saving model to models/3_fold_codenn/checkpoints
12/26 17:37:16 finished saving model
12/26 17:37:16 new best model
12/26 17:57:59   decaying learning rate to: 0.257
12/26 17:59:47 step 32000 epoch 14 learning rate 0.257 step-time 0.673 loss 20.687
12/26 17:59:47 starting evaluation
12/26 18:03:51 test bleu=25.97 loss=41.53 penalty=0.904 ratio=0.909
12/26 18:03:51 saving model to models/3_fold_codenn/checkpoints
12/26 18:03:51 finished saving model
12/26 18:03:51 new best model
12/26 18:26:19 step 34000 epoch 14 learning rate 0.257 step-time 0.672 loss 18.848
12/26 18:26:19 starting evaluation
12/26 18:30:21 test bleu=26.39 loss=40.91 penalty=0.896 ratio=0.901
12/26 18:30:21 saving model to models/3_fold_codenn/checkpoints
12/26 18:30:22 finished saving model
12/26 18:30:22 new best model
12/26 18:33:37   decaying learning rate to: 0.244
12/26 18:52:51 step 36000 epoch 15 learning rate 0.244 step-time 0.673 loss 17.286
12/26 18:52:51 starting evaluation
12/26 18:56:57 test bleu=27.21 loss=42.16 penalty=0.953 ratio=0.955
12/26 18:56:57 saving model to models/3_fold_codenn/checkpoints
12/26 18:56:57 finished saving model
12/26 18:56:57 new best model
12/26 19:05:18   decaying learning rate to: 0.232
12/26 19:19:29 step 38000 epoch 16 learning rate 0.232 step-time 0.674 loss 16.179
12/26 19:19:29 starting evaluation
12/26 19:23:31 test bleu=27.58 loss=43.49 penalty=0.949 ratio=0.950
12/26 19:23:31 saving model to models/3_fold_codenn/checkpoints
12/26 19:23:31 finished saving model
12/26 19:23:31 new best model
12/26 19:36:59   decaying learning rate to: 0.22
12/26 19:45:57 step 40000 epoch 17 learning rate 0.22 step-time 0.671 loss 15.107
12/26 19:45:57 starting evaluation
12/26 19:49:57 test bleu=27.77 loss=44.91 penalty=0.897 ratio=0.902
12/26 19:49:57 saving model to models/3_fold_codenn/checkpoints
12/26 19:49:57 finished saving model
12/26 19:49:57 new best model
12/26 20:08:30   decaying learning rate to: 0.209
12/26 20:12:28 step 42000 epoch 18 learning rate 0.209 step-time 0.674 loss 14.182
12/26 20:12:28 starting evaluation
12/26 20:16:33 test bleu=28.52 loss=47.00 penalty=0.937 ratio=0.939
12/26 20:16:33 saving model to models/3_fold_codenn/checkpoints
12/26 20:16:34 finished saving model
12/26 20:16:34 new best model
12/26 20:38:58 step 44000 epoch 18 learning rate 0.209 step-time 0.670 loss 13.067
12/26 20:38:58 starting evaluation
12/26 20:43:04 test bleu=29.47 loss=45.55 penalty=0.967 ratio=0.968
12/26 20:43:04 saving model to models/3_fold_codenn/checkpoints
12/26 20:43:04 finished saving model
12/26 20:43:04 new best model
12/26 20:44:01   decaying learning rate to: 0.199
12/26 21:05:37 step 46000 epoch 19 learning rate 0.199 step-time 0.674 loss 11.568
12/26 21:05:37 starting evaluation
12/26 21:09:41 test bleu=29.49 loss=47.60 penalty=0.955 ratio=0.956
12/26 21:09:41 saving model to models/3_fold_codenn/checkpoints
12/26 21:09:41 finished saving model
12/26 21:09:41 new best model
12/26 21:15:44   decaying learning rate to: 0.189
12/26 21:32:12 step 48000 epoch 20 learning rate 0.189 step-time 0.674 loss 10.792
12/26 21:32:12 starting evaluation
12/26 21:36:25 test bleu=28.92 loss=49.76 penalty=1.000 ratio=1.034
12/26 21:36:25 saving model to models/3_fold_codenn/checkpoints
12/26 21:36:25 finished saving model
12/26 21:47:31   decaying learning rate to: 0.179
12/26 21:58:53 step 50000 epoch 21 learning rate 0.179 step-time 0.672 loss 10.159
12/26 21:58:53 starting evaluation
12/26 22:02:55 test bleu=30.06 loss=51.48 penalty=0.930 ratio=0.933
12/26 22:02:55 saving model to models/3_fold_codenn/checkpoints
12/26 22:02:55 finished saving model
12/26 22:02:55 new best model
12/26 22:19:11   decaying learning rate to: 0.17
12/26 22:25:22 step 52000 epoch 22 learning rate 0.17 step-time 0.671 loss 9.325
12/26 22:25:22 starting evaluation
12/26 22:29:27 test bleu=30.86 loss=53.98 penalty=0.964 ratio=0.965
12/26 22:29:27 saving model to models/3_fold_codenn/checkpoints
12/26 22:29:27 finished saving model
12/26 22:29:27 new best model
12/26 22:50:37   decaying learning rate to: 0.162
12/26 22:51:56 step 54000 epoch 23 learning rate 0.162 step-time 0.672 loss 8.738
12/26 22:51:56 starting evaluation
12/26 22:56:01 test bleu=31.12 loss=54.83 penalty=0.970 ratio=0.971
12/26 22:56:01 saving model to models/3_fold_codenn/checkpoints
12/26 22:56:01 finished saving model
12/26 22:56:01 new best model
12/26 23:18:15 step 56000 epoch 23 learning rate 0.162 step-time 0.665 loss 7.739
12/26 23:18:15 starting evaluation
12/26 23:22:19 test bleu=31.40 loss=55.93 penalty=0.998 ratio=0.998
12/26 23:22:19 saving model to models/3_fold_codenn/checkpoints
12/26 23:22:20 finished saving model
12/26 23:22:20 new best model
12/26 23:26:01   decaying learning rate to: 0.154
12/26 23:44:27 step 58000 epoch 24 learning rate 0.154 step-time 0.662 loss 7.048
12/26 23:44:27 starting evaluation
12/26 23:48:34 test bleu=30.98 loss=58.15 penalty=1.000 ratio=1.028
12/26 23:48:34 saving model to models/3_fold_codenn/checkpoints
12/26 23:48:34 finished saving model
12/26 23:57:16   decaying learning rate to: 0.146
12/27 00:10:42 step 60000 epoch 25 learning rate 0.146 step-time 0.662 loss 6.603
12/27 00:10:42 starting evaluation
12/27 00:14:47 test bleu=31.23 loss=60.59 penalty=1.000 ratio=1.017
12/27 00:14:47 saving model to models/3_fold_codenn/checkpoints
12/27 00:14:48 finished saving model
12/27 00:28:31   decaying learning rate to: 0.139
12/27 00:36:51 step 62000 epoch 26 learning rate 0.139 step-time 0.660 loss 6.095
12/27 00:36:51 starting evaluation
12/27 00:40:55 test bleu=31.67 loss=62.86 penalty=0.976 ratio=0.976
12/27 00:40:55 saving model to models/3_fold_codenn/checkpoints
12/27 00:40:55 finished saving model
12/27 00:40:55 new best model
12/27 00:59:40   decaying learning rate to: 0.132
12/27 01:03:02 step 64000 epoch 27 learning rate 0.132 step-time 0.661 loss 5.640
12/27 01:03:02 starting evaluation
12/27 01:07:09 test bleu=31.03 loss=64.77 penalty=1.000 ratio=1.034
12/27 01:07:09 saving model to models/3_fold_codenn/checkpoints
12/27 01:07:09 finished saving model
12/27 01:29:16 step 66000 epoch 27 learning rate 0.132 step-time 0.662 loss 5.165
12/27 01:29:16 starting evaluation
12/27 01:33:23 test bleu=31.75 loss=64.32 penalty=1.000 ratio=1.023
12/27 01:33:23 saving model to models/3_fold_codenn/checkpoints
12/27 01:33:23 finished saving model
12/27 01:33:23 new best model
12/27 01:34:46   decaying learning rate to: 0.125
12/27 01:55:30 step 68000 epoch 28 learning rate 0.125 step-time 0.661 loss 4.555
12/27 01:55:30 starting evaluation
12/27 01:59:39 test bleu=29.96 loss=68.09 penalty=1.000 ratio=1.084
12/27 01:59:39 saving model to models/3_fold_codenn/checkpoints
12/27 01:59:39 finished saving model
12/27 02:06:04   decaying learning rate to: 0.119
12/27 02:21:46 step 70000 epoch 29 learning rate 0.119 step-time 0.661 loss 4.200
12/27 02:21:46 starting evaluation
12/27 02:25:49 test bleu=32.40 loss=69.97 penalty=1.000 ratio=1.011
12/27 02:25:49 saving model to models/3_fold_codenn/checkpoints
12/27 02:25:50 finished saving model
12/27 02:25:50 new best model
12/27 02:37:16   decaying learning rate to: 0.113
12/27 02:47:55 step 72000 epoch 30 learning rate 0.113 step-time 0.661 loss 3.916
12/27 02:47:55 starting evaluation
12/27 02:51:59 test bleu=32.93 loss=73.12 penalty=1.000 ratio=1.001
12/27 02:51:59 saving model to models/3_fold_codenn/checkpoints
12/27 02:51:59 finished saving model
12/27 02:51:59 new best model
12/27 03:08:26   decaying learning rate to: 0.107
12/27 03:14:01 step 74000 epoch 31 learning rate 0.107 step-time 0.659 loss 3.662
12/27 03:14:01 starting evaluation
12/27 03:18:10 test bleu=31.84 loss=73.92 penalty=1.000 ratio=1.037
12/27 03:18:10 saving model to models/3_fold_codenn/checkpoints
12/27 03:18:10 finished saving model
12/27 03:39:24   decaying learning rate to: 0.102
12/27 03:40:15 step 76000 epoch 32 learning rate 0.102 step-time 0.661 loss 3.402
12/27 03:40:15 starting evaluation
12/27 03:44:20 test bleu=32.86 loss=76.15 penalty=1.000 ratio=1.014
12/27 03:44:20 saving model to models/3_fold_codenn/checkpoints
12/27 03:44:20 finished saving model
12/27 04:07:07 step 78000 epoch 32 learning rate 0.102 step-time 0.682 loss 2.967
12/27 04:07:07 starting evaluation
12/27 04:11:17 test bleu=33.18 loss=78.09 penalty=1.000 ratio=1.004
12/27 04:11:17 saving model to models/3_fold_codenn/checkpoints
12/27 04:11:17 finished saving model
12/27 04:11:17 new best model
12/27 04:15:31   decaying learning rate to: 0.0969
12/27 04:34:04 step 80000 epoch 33 learning rate 0.0969 step-time 0.681 loss 2.721
12/27 04:34:04 starting evaluation
12/27 04:38:16 test bleu=32.06 loss=80.58 penalty=1.000 ratio=1.039
12/27 04:38:16 saving model to models/3_fold_codenn/checkpoints
12/27 04:38:16 finished saving model
12/27 04:47:41   decaying learning rate to: 0.092
12/27 05:01:03 step 82000 epoch 34 learning rate 0.092 step-time 0.681 loss 2.529
12/27 05:01:03 starting evaluation
12/27 05:05:12 test bleu=32.74 loss=83.51 penalty=1.000 ratio=1.014
12/27 05:05:12 saving model to models/3_fold_codenn/checkpoints
12/27 05:05:12 finished saving model
12/27 05:19:48   decaying learning rate to: 0.0874
12/27 05:27:57 step 84000 epoch 35 learning rate 0.0874 step-time 0.680 loss 2.392
12/27 05:27:57 starting evaluation
12/27 05:32:08 test bleu=31.79 loss=84.94 penalty=1.000 ratio=1.052
12/27 05:32:08 saving model to models/3_fold_codenn/checkpoints
12/27 05:32:08 finished saving model
12/27 05:51:52   decaying learning rate to: 0.083
12/27 05:55:00 step 86000 epoch 36 learning rate 0.083 step-time 0.684 loss 2.198
12/27 05:55:00 starting evaluation
12/27 05:59:10 test bleu=32.50 loss=87.06 penalty=1.000 ratio=1.035
12/27 05:59:10 saving model to models/3_fold_codenn/checkpoints
12/27 05:59:10 finished saving model
12/27 06:21:57 step 88000 epoch 36 learning rate 0.083 step-time 0.681 loss 2.015
12/27 06:21:57 starting evaluation
12/27 06:26:07 test bleu=32.45 loss=88.64 penalty=1.000 ratio=1.038
12/27 06:26:07 saving model to models/3_fold_codenn/checkpoints
12/27 06:26:07 finished saving model
12/27 06:28:00   decaying learning rate to: 0.0789
12/27 06:48:55 step 90000 epoch 37 learning rate 0.0789 step-time 0.682 loss 1.774
12/27 06:48:55 starting evaluation
12/27 06:53:06 test bleu=32.33 loss=90.58 penalty=1.000 ratio=1.044
12/27 06:53:06 saving model to models/3_fold_codenn/checkpoints
12/27 06:53:06 finished saving model
12/27 07:00:10   decaying learning rate to: 0.0749
12/27 07:15:55 step 92000 epoch 38 learning rate 0.0749 step-time 0.682 loss 1.654
12/27 07:15:55 starting evaluation
12/27 07:20:05 test bleu=32.83 loss=92.64 penalty=1.000 ratio=1.023
12/27 07:20:05 saving model to models/3_fold_codenn/checkpoints
12/27 07:20:05 finished saving model
12/27 07:32:20   decaying learning rate to: 0.0712
12/27 07:42:52 step 94000 epoch 39 learning rate 0.0712 step-time 0.682 loss 1.572
12/27 07:42:52 starting evaluation
12/27 07:47:01 test bleu=32.62 loss=93.73 penalty=1.000 ratio=1.037
12/27 07:47:01 saving model to models/3_fold_codenn/checkpoints
12/27 07:47:01 finished saving model
12/27 08:04:30   decaying learning rate to: 0.0676
12/27 08:09:52 step 96000 epoch 40 learning rate 0.0676 step-time 0.683 loss 1.458
12/27 08:09:52 starting evaluation
12/27 08:14:02 test bleu=32.51 loss=96.83 penalty=1.000 ratio=1.037
12/27 08:14:02 saving model to models/3_fold_codenn/checkpoints
12/27 08:14:02 finished saving model
12/27 08:36:23   decaying learning rate to: 0.0643
12/27 08:36:49 step 98000 epoch 41 learning rate 0.0643 step-time 0.682 loss 1.369
12/27 08:36:49 starting evaluation
12/27 08:41:00 test bleu=32.49 loss=97.78 penalty=1.000 ratio=1.046
12/27 08:41:00 saving model to models/3_fold_codenn/checkpoints
12/27 08:41:00 finished saving model
12/27 09:03:47 step 100000 epoch 41 learning rate 0.0643 step-time 0.682 loss 1.204
12/27 09:03:47 starting evaluation
12/27 09:07:59 test bleu=32.65 loss=99.28 penalty=1.000 ratio=1.042
12/27 09:07:59 saving model to models/3_fold_codenn/checkpoints
12/27 09:07:59 finished saving model
12/27 09:12:41   decaying learning rate to: 0.061
12/27 09:30:48 step 102000 epoch 42 learning rate 0.061 step-time 0.683 loss 1.117
12/27 09:30:48 starting evaluation
12/27 09:34:59 test bleu=32.43 loss=101.86 penalty=1.000 ratio=1.047
12/27 09:34:59 saving model to models/3_fold_codenn/checkpoints
12/27 09:34:59 finished saving model
12/27 09:44:54   decaying learning rate to: 0.058
12/27 09:57:49 step 104000 epoch 43 learning rate 0.058 step-time 0.683 loss 1.061
12/27 09:57:49 starting evaluation
12/27 10:01:59 test bleu=32.10 loss=103.01 penalty=1.000 ratio=1.055
12/27 10:01:59 saving model to models/3_fold_codenn/checkpoints
12/27 10:01:59 finished saving model
12/27 10:17:05   decaying learning rate to: 0.0551
12/27 10:24:46 step 106000 epoch 44 learning rate 0.0551 step-time 0.682 loss 1.001
12/27 10:24:46 starting evaluation
12/27 10:28:55 test bleu=33.19 loss=104.35 penalty=1.000 ratio=1.029
12/27 10:28:55 saving model to models/3_fold_codenn/checkpoints
12/27 10:28:55 finished saving model
12/27 10:28:55 new best model
12/27 10:49:04   decaying learning rate to: 0.0523
12/27 10:51:47 step 108000 epoch 45 learning rate 0.0523 step-time 0.684 loss 0.951
12/27 10:51:47 starting evaluation
12/27 10:55:58 test bleu=32.30 loss=105.87 penalty=1.000 ratio=1.058
12/27 10:55:58 saving model to models/3_fold_codenn/checkpoints
12/27 10:55:58 finished saving model
12/27 11:18:45 step 110000 epoch 45 learning rate 0.0523 step-time 0.682 loss 0.875
12/27 11:18:45 starting evaluation
12/27 11:22:56 test bleu=32.05 loss=107.33 penalty=1.000 ratio=1.060
12/27 11:22:56 saving model to models/3_fold_codenn/checkpoints
12/27 11:22:56 finished saving model
12/27 11:25:18   decaying learning rate to: 0.0497
12/27 11:45:42 step 112000 epoch 46 learning rate 0.0497 step-time 0.681 loss 0.795
12/27 11:45:42 starting evaluation
12/27 11:49:51 test bleu=32.83 loss=108.52 penalty=1.000 ratio=1.041
12/27 11:49:51 saving model to models/3_fold_codenn/checkpoints
12/27 11:49:51 finished saving model
12/27 11:57:25   decaying learning rate to: 0.0472
12/27 12:12:42 step 114000 epoch 47 learning rate 0.0472 step-time 0.683 loss 0.758
12/27 12:12:42 starting evaluation
12/27 12:16:51 test bleu=32.51 loss=109.46 penalty=1.000 ratio=1.049
12/27 12:16:51 saving model to models/3_fold_codenn/checkpoints
12/27 12:16:51 finished saving model
12/27 12:29:37   decaying learning rate to: 0.0449
12/27 12:39:42 step 116000 epoch 48 learning rate 0.0449 step-time 0.684 loss 0.723
12/27 12:39:42 starting evaluation
12/27 12:43:52 test bleu=32.64 loss=111.38 penalty=1.000 ratio=1.041
12/27 12:43:52 saving model to models/3_fold_codenn/checkpoints
12/27 12:43:52 finished saving model
12/27 13:01:46   decaying learning rate to: 0.0426
12/27 13:06:41 step 118000 epoch 49 learning rate 0.0426 step-time 0.682 loss 0.691
12/27 13:06:41 starting evaluation
12/27 13:10:50 test bleu=32.56 loss=112.13 penalty=1.000 ratio=1.045
12/27 13:10:50 saving model to models/3_fold_codenn/checkpoints
12/27 13:10:50 finished saving model
12/27 13:33:37 step 120000 epoch 50 learning rate 0.0426 step-time 0.682 loss 0.659
12/27 13:33:37 starting evaluation
12/27 13:37:49 test bleu=32.41 loss=113.05 penalty=1.000 ratio=1.053
12/27 13:37:49 saving model to models/3_fold_codenn/checkpoints
12/27 13:37:49 finished saving model
12/27 13:37:49   decaying learning rate to: 0.0405
12/27 14:00:35 step 122000 epoch 50 learning rate 0.0405 step-time 0.681 loss 0.594
12/27 14:00:35 starting evaluation
12/27 14:04:44 test bleu=32.71 loss=113.90 penalty=1.000 ratio=1.045
12/27 14:04:44 saving model to models/3_fold_codenn/checkpoints
12/27 14:04:44 finished saving model
12/27 14:09:57   decaying learning rate to: 0.0385
12/27 14:27:36 step 124000 epoch 51 learning rate 0.0385 step-time 0.684 loss 0.572
12/27 14:27:36 starting evaluation
12/27 14:31:45 test bleu=32.67 loss=115.00 penalty=1.000 ratio=1.045
12/27 14:31:45 saving model to models/3_fold_codenn/checkpoints
12/27 14:31:45 finished saving model
12/27 14:42:07   decaying learning rate to: 0.0365
12/27 14:54:31 step 126000 epoch 52 learning rate 0.0365 step-time 0.681 loss 0.550
12/27 14:54:31 starting evaluation
12/27 14:58:40 test bleu=32.98 loss=116.41 penalty=1.000 ratio=1.037
12/27 14:58:40 saving model to models/3_fold_codenn/checkpoints
12/27 14:58:40 finished saving model
12/27 15:14:11   decaying learning rate to: 0.0347
12/27 15:21:25 step 128000 epoch 53 learning rate 0.0347 step-time 0.680 loss 0.529
12/27 15:21:25 starting evaluation
12/27 15:25:35 test bleu=33.02 loss=117.05 penalty=1.000 ratio=1.037
12/27 15:25:35 saving model to models/3_fold_codenn/checkpoints
12/27 15:25:36 finished saving model
12/27 15:46:09   decaying learning rate to: 0.033
12/27 15:48:23 step 130000 epoch 54 learning rate 0.033 step-time 0.682 loss 0.513
12/27 15:48:23 starting evaluation
12/27 15:52:34 test bleu=32.51 loss=117.58 penalty=1.000 ratio=1.050
12/27 15:52:34 saving model to models/3_fold_codenn/checkpoints
12/27 15:52:34 finished saving model
12/27 16:15:20 step 132000 epoch 54 learning rate 0.033 step-time 0.681 loss 0.479
12/27 16:15:20 starting evaluation
12/27 16:19:29 test bleu=33.35 loss=118.31 penalty=1.000 ratio=1.024
12/27 16:19:29 saving model to models/3_fold_codenn/checkpoints
12/27 16:19:29 finished saving model
12/27 16:19:29 new best model
12/27 16:22:21   decaying learning rate to: 0.0313
12/27 16:42:23 step 134000 epoch 55 learning rate 0.0313 step-time 0.685 loss 0.442
12/27 16:42:23 starting evaluation
12/27 16:46:34 test bleu=32.91 loss=118.51 penalty=1.000 ratio=1.039
12/27 16:46:34 saving model to models/3_fold_codenn/checkpoints
12/27 16:46:34 finished saving model
12/27 16:54:34   decaying learning rate to: 0.0298
12/27 17:09:24 step 136000 epoch 56 learning rate 0.0298 step-time 0.683 loss 0.441
12/27 17:09:24 starting evaluation
12/27 17:13:35 test bleu=32.62 loss=119.05 penalty=1.000 ratio=1.049
12/27 17:13:35 saving model to models/3_fold_codenn/checkpoints
12/27 17:13:35 finished saving model
12/27 17:26:44   decaying learning rate to: 0.0283
12/27 17:36:21 step 138000 epoch 57 learning rate 0.0283 step-time 0.681 loss 0.419
12/27 17:36:21 starting evaluation
12/27 17:40:32 test bleu=32.68 loss=118.85 penalty=1.000 ratio=1.052
12/27 17:40:32 saving model to models/3_fold_codenn/checkpoints
12/27 17:40:32 finished saving model
12/27 17:58:55   decaying learning rate to: 0.0269
12/27 18:03:18 step 140000 epoch 58 learning rate 0.0269 step-time 0.681 loss 0.415
12/27 18:03:18 starting evaluation
12/27 18:07:28 test bleu=33.39 loss=120.03 penalty=1.000 ratio=1.027
12/27 18:07:28 saving model to models/3_fold_codenn/checkpoints
12/27 18:07:28 finished saving model
12/27 18:07:28 new best model
12/27 18:30:16 step 142000 epoch 58 learning rate 0.0269 step-time 0.682 loss 0.395
12/27 18:30:16 starting evaluation
12/27 18:34:25 test bleu=32.24 loss=120.35 penalty=1.000 ratio=1.064
12/27 18:34:25 saving model to models/3_fold_codenn/checkpoints
12/27 18:34:26 finished saving model
12/27 18:34:55   decaying learning rate to: 0.0255
12/27 18:57:15 step 144000 epoch 59 learning rate 0.0255 step-time 0.683 loss 0.366
12/27 18:57:15 starting evaluation
12/27 19:01:26 test bleu=32.90 loss=119.82 penalty=1.000 ratio=1.045
12/27 19:01:26 saving model to models/3_fold_codenn/checkpoints
12/27 19:01:26 finished saving model
12/27 19:07:04   decaying learning rate to: 0.0242
12/27 19:24:14 step 146000 epoch 60 learning rate 0.0242 step-time 0.682 loss 0.352
12/27 19:24:14 starting evaluation
12/27 19:28:25 test bleu=32.89 loss=121.13 penalty=1.000 ratio=1.044
12/27 19:28:25 saving model to models/3_fold_codenn/checkpoints
12/27 19:28:25 finished saving model
12/27 19:39:16   decaying learning rate to: 0.023
12/27 19:51:13 step 148000 epoch 61 learning rate 0.023 step-time 0.682 loss 0.347
12/27 19:51:13 starting evaluation
12/27 19:55:24 test bleu=32.97 loss=121.69 penalty=1.000 ratio=1.042
12/27 19:55:24 saving model to models/3_fold_codenn/checkpoints
12/27 19:55:24 finished saving model
12/27 20:11:26   decaying learning rate to: 0.0219
12/27 20:18:08 step 150000 epoch 62 learning rate 0.0219 step-time 0.680 loss 0.336
12/27 20:18:08 starting evaluation
12/27 20:22:18 test bleu=33.05 loss=122.34 penalty=1.000 ratio=1.039
12/27 20:22:18 saving model to models/3_fold_codenn/checkpoints
12/27 20:22:18 finished saving model
12/27 20:43:21   decaying learning rate to: 0.0208
12/27 20:45:07 step 152000 epoch 63 learning rate 0.0208 step-time 0.683 loss 0.334
12/27 20:45:07 starting evaluation
12/27 20:49:16 test bleu=33.00 loss=122.50 penalty=1.000 ratio=1.044
12/27 20:49:16 saving model to models/3_fold_codenn/checkpoints
12/27 20:49:16 finished saving model
12/27 21:12:05 step 154000 epoch 63 learning rate 0.0208 step-time 0.682 loss 0.309
12/27 21:12:05 starting evaluation
12/27 21:16:16 test bleu=32.96 loss=122.17 penalty=1.000 ratio=1.042
12/27 21:16:16 saving model to models/3_fold_codenn/checkpoints
12/27 21:16:16 finished saving model
12/27 21:19:34   decaying learning rate to: 0.0197
12/27 21:39:03 step 156000 epoch 64 learning rate 0.0197 step-time 0.681 loss 0.292
12/27 21:39:03 starting evaluation
12/27 21:43:14 test bleu=32.66 loss=122.92 penalty=1.000 ratio=1.054
12/27 21:43:14 saving model to models/3_fold_codenn/checkpoints
12/27 21:43:15 finished saving model
12/27 21:51:46   decaying learning rate to: 0.0188
12/27 22:05:59 step 158000 epoch 65 learning rate 0.0188 step-time 0.680 loss 0.291
12/27 22:05:59 starting evaluation
12/27 22:10:11 test bleu=32.39 loss=122.95 penalty=1.000 ratio=1.060
12/27 22:10:11 saving model to models/3_fold_codenn/checkpoints
12/27 22:10:12 finished saving model
12/27 22:23:53   decaying learning rate to: 0.0178
12/27 22:32:58 step 160000 epoch 66 learning rate 0.0178 step-time 0.681 loss 0.287
12/27 22:32:58 starting evaluation
12/27 22:37:08 test bleu=33.32 loss=123.17 penalty=1.000 ratio=1.030
12/27 22:37:08 saving model to models/3_fold_codenn/checkpoints
12/27 22:37:08 finished saving model
12/27 22:56:03   decaying learning rate to: 0.0169
12/27 23:00:01 step 162000 epoch 67 learning rate 0.0169 step-time 0.684 loss 0.279
12/27 23:00:01 starting evaluation
12/27 23:04:11 test bleu=33.00 loss=123.57 penalty=1.000 ratio=1.043
12/27 23:04:11 saving model to models/3_fold_codenn/checkpoints
12/27 23:04:11 finished saving model
12/27 23:27:00 step 164000 epoch 67 learning rate 0.0169 step-time 0.683 loss 0.271
12/27 23:27:00 starting evaluation
12/27 23:31:12 test bleu=32.93 loss=123.78 penalty=1.000 ratio=1.046
12/27 23:31:12 saving model to models/3_fold_codenn/checkpoints
12/27 23:31:12 finished saving model
12/27 23:32:09   decaying learning rate to: 0.0161
12/27 23:53:58 step 166000 epoch 68 learning rate 0.0161 step-time 0.681 loss 0.252
12/27 23:53:58 starting evaluation
12/27 23:58:10 test bleu=32.82 loss=124.17 penalty=1.000 ratio=1.049
12/27 23:58:10 saving model to models/3_fold_codenn/checkpoints
12/27 23:58:11 finished saving model
12/28 00:04:19   decaying learning rate to: 0.0153
12/28 00:20:55 step 168000 epoch 69 learning rate 0.0153 step-time 0.680 loss 0.244
12/28 00:20:55 starting evaluation
12/28 00:25:07 test bleu=33.41 loss=124.39 penalty=1.000 ratio=1.031
12/28 00:25:07 saving model to models/3_fold_codenn/checkpoints
12/28 00:25:07 finished saving model
12/28 00:25:07 new best model
12/28 00:36:29   decaying learning rate to: 0.0145
12/28 00:47:54 step 170000 epoch 70 learning rate 0.0145 step-time 0.681 loss 0.245
12/28 00:47:54 starting evaluation
12/28 00:52:05 test bleu=33.31 loss=124.63 penalty=1.000 ratio=1.033
12/28 00:52:05 saving model to models/3_fold_codenn/checkpoints
12/28 00:52:05 finished saving model
12/28 01:08:37   decaying learning rate to: 0.0138
12/28 01:14:56 step 172000 epoch 71 learning rate 0.0138 step-time 0.684 loss 0.237
12/28 01:14:56 starting evaluation
12/28 01:19:06 test bleu=33.61 loss=125.08 penalty=1.000 ratio=1.026
12/28 01:19:06 saving model to models/3_fold_codenn/checkpoints
12/28 01:19:07 finished saving model
12/28 01:19:07 new best model
12/28 01:40:35   decaying learning rate to: 0.0131
12/28 01:41:54 step 174000 epoch 72 learning rate 0.0131 step-time 0.682 loss 0.240
12/28 01:41:54 starting evaluation
12/28 01:46:06 test bleu=32.88 loss=125.04 penalty=1.000 ratio=1.049
12/28 01:46:06 saving model to models/3_fold_codenn/checkpoints
12/28 01:46:06 finished saving model
12/28 02:09:24 step 176000 epoch 72 learning rate 0.0131 step-time 0.697 loss 0.225
12/28 02:09:24 starting evaluation
12/28 02:13:41 test bleu=33.13 loss=125.24 penalty=1.000 ratio=1.037
12/28 02:13:41 saving model to models/3_fold_codenn/checkpoints
12/28 02:13:41 finished saving model
12/28 02:17:35   decaying learning rate to: 0.0124
12/28 02:37:15 step 178000 epoch 73 learning rate 0.0124 step-time 0.705 loss 0.217
12/28 02:37:15 starting evaluation
12/28 02:41:34 test bleu=32.56 loss=125.51 penalty=1.000 ratio=1.054
12/28 02:41:34 saving model to models/3_fold_codenn/checkpoints
12/28 02:41:35 finished saving model
12/28 02:50:54   decaying learning rate to: 0.0118
12/28 03:05:17 step 180000 epoch 74 learning rate 0.0118 step-time 0.709 loss 0.215
12/28 03:05:17 starting evaluation
12/28 03:09:36 test bleu=32.83 loss=125.67 penalty=1.000 ratio=1.044
12/28 03:09:36 saving model to models/3_fold_codenn/checkpoints
12/28 03:09:36 finished saving model
12/28 03:24:13   decaying learning rate to: 0.0112
12/28 03:33:16 step 182000 epoch 75 learning rate 0.0112 step-time 0.708 loss 0.208
12/28 03:33:16 starting evaluation
12/28 03:37:35 test bleu=32.98 loss=126.23 penalty=1.000 ratio=1.041
12/28 03:37:35 saving model to models/3_fold_codenn/checkpoints
12/28 03:37:35 finished saving model
12/28 03:57:38   decaying learning rate to: 0.0107
12/28 04:01:18 step 184000 epoch 76 learning rate 0.0107 step-time 0.710 loss 0.210
12/28 04:01:18 starting evaluation
12/28 04:05:37 test bleu=32.61 loss=126.28 penalty=1.000 ratio=1.055
12/28 04:05:37 saving model to models/3_fold_codenn/checkpoints
12/28 04:05:37 finished saving model
12/28 04:29:16 step 186000 epoch 76 learning rate 0.0107 step-time 0.707 loss 0.205
12/28 04:29:16 starting evaluation
12/28 04:33:35 test bleu=32.68 loss=126.18 penalty=1.000 ratio=1.048
12/28 04:33:35 saving model to models/3_fold_codenn/checkpoints
12/28 04:33:35 finished saving model
12/28 04:35:04   decaying learning rate to: 0.0101
12/28 04:56:31 step 188000 epoch 77 learning rate 0.0101 step-time 0.686 loss 0.193
12/28 04:56:31 starting evaluation
12/28 05:00:42 test bleu=32.97 loss=126.47 penalty=1.000 ratio=1.046
12/28 05:00:42 saving model to models/3_fold_codenn/checkpoints
12/28 05:00:42 finished saving model
12/28 05:07:20   decaying learning rate to: 0.00963
12/28 05:23:31 step 190000 epoch 78 learning rate 0.00963 step-time 0.682 loss 0.189
12/28 05:23:31 starting evaluation
12/28 05:27:42 test bleu=32.90 loss=127.07 penalty=1.000 ratio=1.044
12/28 05:27:42 saving model to models/3_fold_codenn/checkpoints
12/28 05:27:43 finished saving model
12/28 05:39:31   decaying learning rate to: 0.00915
12/28 05:50:30 step 192000 epoch 79 learning rate 0.00915 step-time 0.682 loss 0.188
12/28 05:50:30 starting evaluation
12/28 05:54:42 test bleu=33.45 loss=127.22 penalty=1.000 ratio=1.029
12/28 05:54:42 saving model to models/3_fold_codenn/checkpoints
12/28 05:54:42 finished saving model
12/28 06:11:42   decaying learning rate to: 0.00869
12/28 06:17:34 step 194000 epoch 80 learning rate 0.00869 step-time 0.684 loss 0.189
12/28 06:17:34 starting evaluation
12/28 06:21:47 test bleu=32.86 loss=127.45 penalty=1.000 ratio=1.042
12/28 06:21:47 saving model to models/3_fold_codenn/checkpoints
12/28 06:21:48 finished saving model
12/28 06:44:00   decaying learning rate to: 0.00826
12/28 06:44:56 step 196000 epoch 81 learning rate 0.00826 step-time 0.692 loss 0.187
12/28 06:44:56 starting evaluation
12/28 06:49:10 test bleu=32.91 loss=127.55 penalty=1.000 ratio=1.047
12/28 06:49:10 saving model to models/3_fold_codenn/checkpoints
12/28 06:49:10 finished saving model
12/28 07:12:18 step 198000 epoch 81 learning rate 0.00826 step-time 0.692 loss 0.174
12/28 07:12:18 starting evaluation
12/28 07:16:31 test bleu=33.02 loss=127.58 penalty=1.000 ratio=1.039
12/28 07:16:31 saving model to models/3_fold_codenn/checkpoints
12/28 07:16:31 finished saving model
12/28 07:20:51   decaying learning rate to: 0.00784
12/28 07:39:41 step 200000 epoch 82 learning rate 0.00784 step-time 0.693 loss 0.173
12/28 07:39:41 starting evaluation
12/28 07:43:54 test bleu=33.07 loss=127.85 penalty=1.000 ratio=1.043
12/28 07:43:54 saving model to models/3_fold_codenn/checkpoints
12/28 07:43:54 finished saving model
12/28 07:53:29   decaying learning rate to: 0.00745
12/28 08:07:01 step 202000 epoch 83 learning rate 0.00745 step-time 0.691 loss 0.171
12/28 08:07:01 starting evaluation
12/28 08:11:15 test bleu=33.14 loss=128.15 penalty=1.000 ratio=1.040
12/28 08:11:15 saving model to models/3_fold_codenn/checkpoints
12/28 08:11:15 finished saving model
12/28 08:26:04   decaying learning rate to: 0.00708
12/28 08:34:23 step 204000 epoch 84 learning rate 0.00708 step-time 0.692 loss 0.168
12/28 08:34:23 starting evaluation
12/28 08:38:37 test bleu=32.90 loss=128.42 penalty=1.000 ratio=1.045
12/28 08:38:37 saving model to models/3_fold_codenn/checkpoints
12/28 08:38:37 finished saving model
12/28 08:58:38   decaying learning rate to: 0.00673
12/28 09:01:48 step 206000 epoch 85 learning rate 0.00673 step-time 0.693 loss 0.172
12/28 09:01:48 starting evaluation
12/28 09:06:03 test bleu=32.76 loss=128.55 penalty=1.000 ratio=1.050
12/28 09:06:03 saving model to models/3_fold_codenn/checkpoints
12/28 09:06:04 finished saving model
12/28 09:29:12 step 208000 epoch 85 learning rate 0.00673 step-time 0.692 loss 0.164
12/28 09:29:12 starting evaluation
12/28 09:33:26 test bleu=32.79 loss=128.40 penalty=1.000 ratio=1.047
12/28 09:33:26 saving model to models/3_fold_codenn/checkpoints
12/28 09:33:26 finished saving model
12/28 09:35:22   decaying learning rate to: 0.00639
12/28 09:56:36 step 210000 epoch 86 learning rate 0.00639 step-time 0.693 loss 0.158
12/28 09:56:36 starting evaluation
12/28 10:00:48 test bleu=32.76 loss=128.53 penalty=1.000 ratio=1.051
12/28 10:00:48 saving model to models/3_fold_codenn/checkpoints
12/28 10:00:49 finished saving model
12/28 10:07:59   decaying learning rate to: 0.00607
12/28 10:23:57 step 212000 epoch 87 learning rate 0.00607 step-time 0.692 loss 0.157
12/28 10:23:57 starting evaluation
12/28 10:28:11 test bleu=33.20 loss=128.89 penalty=1.000 ratio=1.036
12/28 10:28:11 saving model to models/3_fold_codenn/checkpoints
12/28 10:28:11 finished saving model
12/28 10:40:38   decaying learning rate to: 0.00577
12/28 10:51:21 step 214000 epoch 88 learning rate 0.00577 step-time 0.693 loss 0.157
12/28 10:51:21 starting evaluation
12/28 10:55:35 test bleu=32.78 loss=129.08 penalty=1.000 ratio=1.049
12/28 10:55:35 saving model to models/3_fold_codenn/checkpoints
12/28 10:55:35 finished saving model
12/28 11:13:17   decaying learning rate to: 0.00548
12/28 11:18:44 step 216000 epoch 89 learning rate 0.00548 step-time 0.693 loss 0.156
12/28 11:18:44 starting evaluation
12/28 11:22:58 test bleu=33.05 loss=129.28 penalty=1.000 ratio=1.042
12/28 11:22:58 saving model to models/3_fold_codenn/checkpoints
12/28 11:22:58 finished saving model
12/28 11:45:40   decaying learning rate to: 0.0052
12/28 11:46:07 step 218000 epoch 90 learning rate 0.0052 step-time 0.692 loss 0.156
12/28 11:46:07 starting evaluation
12/28 11:50:20 test bleu=32.86 loss=129.26 penalty=1.000 ratio=1.046
12/28 11:50:20 saving model to models/3_fold_codenn/checkpoints
12/28 11:50:20 finished saving model
12/28 12:13:14 step 220000 epoch 90 learning rate 0.0052 step-time 0.685 loss 0.147
12/28 12:13:14 starting evaluation
12/28 12:17:24 test bleu=32.95 loss=129.64 penalty=1.000 ratio=1.044
12/28 12:17:24 saving model to models/3_fold_codenn/checkpoints
12/28 12:17:24 finished saving model
12/28 12:22:03   decaying learning rate to: 0.00494
12/28 12:39:48 step 222000 epoch 91 learning rate 0.00494 step-time 0.670 loss 0.146
12/28 12:39:48 starting evaluation
12/28 12:43:58 test bleu=32.86 loss=129.78 penalty=1.000 ratio=1.045
12/28 12:43:58 saving model to models/3_fold_codenn/checkpoints
12/28 12:43:58 finished saving model
12/28 12:53:45   decaying learning rate to: 0.0047
12/28 13:06:26 step 224000 epoch 92 learning rate 0.0047 step-time 0.672 loss 0.147
12/28 13:06:26 starting evaluation
12/28 13:10:37 test bleu=32.88 loss=129.92 penalty=1.000 ratio=1.045
12/28 13:10:37 saving model to models/3_fold_codenn/checkpoints
12/28 13:10:37 finished saving model
12/28 13:25:26   decaying learning rate to: 0.00446
12/28 13:33:03 step 226000 epoch 93 learning rate 0.00446 step-time 0.671 loss 0.146
12/28 13:33:03 starting evaluation
12/28 13:37:11 test bleu=32.97 loss=130.01 penalty=1.000 ratio=1.042
12/28 13:37:11 saving model to models/3_fold_codenn/checkpoints
12/28 13:37:11 finished saving model
12/28 13:57:01   decaying learning rate to: 0.00424
12/28 13:59:37 step 228000 epoch 94 learning rate 0.00424 step-time 0.671 loss 0.145
12/28 13:59:37 starting evaluation
12/28 14:03:46 test bleu=32.63 loss=130.07 penalty=1.000 ratio=1.050
12/28 14:03:46 saving model to models/3_fold_codenn/checkpoints
12/28 14:03:46 finished saving model
12/28 14:26:16 step 230000 epoch 94 learning rate 0.00424 step-time 0.673 loss 0.142
12/28 14:26:16 starting evaluation
12/28 14:30:25 test bleu=32.86 loss=130.18 penalty=1.000 ratio=1.047
12/28 14:30:25 saving model to models/3_fold_codenn/checkpoints
12/28 14:30:25 finished saving model
12/28 14:32:45   decaying learning rate to: 0.00403
12/28 14:52:52 step 232000 epoch 95 learning rate 0.00403 step-time 0.671 loss 0.139
12/28 14:52:52 starting evaluation
12/28 14:57:00 test bleu=32.86 loss=130.32 penalty=1.000 ratio=1.047
12/28 14:57:00 saving model to models/3_fold_codenn/checkpoints
12/28 14:57:00 finished saving model
12/28 15:04:29   decaying learning rate to: 0.00383
12/28 15:19:30 step 234000 epoch 96 learning rate 0.00383 step-time 0.673 loss 0.138
12/28 15:19:30 starting evaluation
12/28 15:23:39 test bleu=32.86 loss=130.38 penalty=1.000 ratio=1.049
12/28 15:23:39 saving model to models/3_fold_codenn/checkpoints
12/28 15:23:39 finished saving model
12/28 15:36:11   decaying learning rate to: 0.00363
12/28 15:46:04 step 236000 epoch 97 learning rate 0.00363 step-time 0.671 loss 0.137
12/28 15:46:04 starting evaluation
12/28 15:50:14 test bleu=33.03 loss=130.56 penalty=1.000 ratio=1.041
12/28 15:50:14 saving model to models/3_fold_codenn/checkpoints
12/28 15:50:14 finished saving model
12/28 16:07:55   decaying learning rate to: 0.00345
12/28 16:12:41 step 238000 epoch 98 learning rate 0.00345 step-time 0.672 loss 0.139
12/28 16:12:41 starting evaluation
12/28 16:16:51 test bleu=32.82 loss=130.92 penalty=1.000 ratio=1.046
12/28 16:16:51 saving model to models/3_fold_codenn/checkpoints
12/28 16:16:51 finished saving model
12/28 16:39:42 step 240000 epoch 99 learning rate 0.00345 step-time 0.684 loss 0.137
12/28 16:39:42 starting evaluation
12/28 16:43:52 test bleu=33.01 loss=130.92 penalty=1.000 ratio=1.043
12/28 16:43:52 saving model to models/3_fold_codenn/checkpoints
12/28 16:43:52 finished saving model
12/28 16:43:54   decaying learning rate to: 0.00328
12/28 17:06:42 step 242000 epoch 99 learning rate 0.00328 step-time 0.683 loss 0.132
12/28 17:06:42 starting evaluation
12/28 17:10:52 test bleu=32.96 loss=130.88 penalty=1.000 ratio=1.044
12/28 17:10:52 saving model to models/3_fold_codenn/checkpoints
12/28 17:10:52 finished saving model
12/28 17:16:05   decaying learning rate to: 0.00312
12/28 17:33:39 step 244000 epoch 100 learning rate 0.00312 step-time 0.681 loss 0.131
12/28 17:33:39 starting evaluation
12/28 17:37:50 test bleu=33.10 loss=130.97 penalty=1.000 ratio=1.038
12/28 17:37:50 saving model to models/3_fold_codenn/checkpoints
12/28 17:37:50 finished saving model
12/28 17:47:58 finished training
12/28 17:47:58 exiting...
12/28 17:47:58 saving model to models/3_fold_codenn/checkpoints
12/28 17:47:58 finished saving model
