nohup: ignoring input
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:107: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:30: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

01/13 14:01:18 label: default
01/13 14:01:18 description:
  default configuration
  next line of description
  last line
01/13 14:01:18 /root/icpc/icpc/translate/__main__.py config/10-folds/10_fold/seq2seq/config.yaml --train -v
01/13 14:01:18 commit hash 74e0554cb3eb5df835cef993ad570ff8de651f71
01/13 14:01:18 tensorflow version: 1.14.0
01/13 14:01:18 program arguments
01/13 14:01:18   aggregation_method   'sum'
01/13 14:01:18   align_encoder_id     0
01/13 14:01:18   allow_growth         True
01/13 14:01:18   attention_type       'global'
01/13 14:01:18   attn_filter_length   0
01/13 14:01:18   attn_filters         0
01/13 14:01:18   attn_prev_word       False
01/13 14:01:18   attn_size            128
01/13 14:01:18   attn_temperature     1.0
01/13 14:01:18   attn_window_size     0
01/13 14:01:18   average              False
01/13 14:01:18   baseline_activation  None
01/13 14:01:18   baseline_learning_rate 0.001
01/13 14:01:18   baseline_optimizer   'adam'
01/13 14:01:18   baseline_steps       0
01/13 14:01:18   batch_mode           'standard'
01/13 14:01:18   batch_size           64
01/13 14:01:18   beam_size            5
01/13 14:01:18   bidir                True
01/13 14:01:18   bidir_projection     False
01/13 14:01:18   binary               False
01/13 14:01:18   cell_size            256
01/13 14:01:18   cell_type            'GRU'
01/13 14:01:18   character_level      False
01/13 14:01:18   checkpoints          []
01/13 14:01:18   conditional_rnn      False
01/13 14:01:18   config               'config/10-folds/10_fold/seq2seq/config.yaml'
01/13 14:01:18   convolutions         None
01/13 14:01:18   data_dir             'data/gooddata/10_fold'
01/13 14:01:18   debug                False
01/13 14:01:18   decay_after_n_epoch  1
01/13 14:01:18   decay_every_n_epoch  1
01/13 14:01:18   decay_if_no_progress None
01/13 14:01:18   decoders             [{'max_len': 40, 'name': 'nl'}]
01/13 14:01:18   description          'default configuration\nnext line of description\nlast line\n'
01/13 14:01:18   dev_prefix           'test'
01/13 14:01:18   early_stopping       True
01/13 14:01:18   embedding_dropout    0.0
01/13 14:01:18   embedding_initializer None
01/13 14:01:18   embedding_size       256
01/13 14:01:18   embedding_weight_scale None
01/13 14:01:18   embeddings_on_cpu    True
01/13 14:01:18   encoders             [{'max_len': 200, 'name': 'code'}]
01/13 14:01:18   ensemble             False
01/13 14:01:18   eval_burn_in         0
01/13 14:01:18   feed_previous        0.0
01/13 14:01:18   final_state          'last'
01/13 14:01:18   freeze_variables     []
01/13 14:01:18   generate_first       True
01/13 14:01:18   gpu_id               0
01/13 14:01:18   highway_layers       0
01/13 14:01:18   initial_state_dropout 0.0
01/13 14:01:18   initializer          None
01/13 14:01:18   input_layer_dropout  0.0
01/13 14:01:18   input_layers         None
01/13 14:01:18   keep_best            5
01/13 14:01:18   keep_every_n_hours   0
01/13 14:01:18   label                'default'
01/13 14:01:18   layer_norm           False
01/13 14:01:18   layers               1
01/13 14:01:18   learning_rate        0.5
01/13 14:01:18   learning_rate_decay_factor 0.95
01/13 14:01:18   len_normalization    1.0
01/13 14:01:18   log_file             'log.txt'
01/13 14:01:18   loss_function        'xent'
01/13 14:01:18   max_dev_size         0
01/13 14:01:18   max_epochs           100
01/13 14:01:18   max_gradient_norm    5.0
01/13 14:01:18   max_len              50
01/13 14:01:18   max_steps            600000
01/13 14:01:18   max_test_size        0
01/13 14:01:18   max_to_keep          1
01/13 14:01:18   max_train_size       0
01/13 14:01:18   maxout_stride        None
01/13 14:01:18   mem_fraction         1.0
01/13 14:01:18   min_learning_rate    1e-06
01/13 14:01:18   model_dir            'models/10_fold_seq2seq'
01/13 14:01:18   moving_average       None
01/13 14:01:18   no_gpu               False
01/13 14:01:18   optimizer            'sgd'
01/13 14:01:18   orthogonal_init      False
01/13 14:01:18   output               None
01/13 14:01:18   output_dropout       0.0
01/13 14:01:18   parallel_iterations  16
01/13 14:01:18   pervasive_dropout    False
01/13 14:01:18   pooling_avg          True
01/13 14:01:18   post_process_script  None
01/13 14:01:18   pred_deep_layer      False
01/13 14:01:18   pred_edits           False
01/13 14:01:18   pred_embed_proj      True
01/13 14:01:18   pred_maxout_layer    True
01/13 14:01:18   purge                False
01/13 14:01:18   raw_output           False
01/13 14:01:18   read_ahead           1
01/13 14:01:18   reconstruction_attn_weight 0.05
01/13 14:01:18   reconstruction_decoders False
01/13 14:01:18   reconstruction_weight 1.0
01/13 14:01:18   reinforce_after_n_epoch None
01/13 14:01:18   remove_unk           False
01/13 14:01:18   reverse              False
01/13 14:01:18   reverse_input        True
01/13 14:01:18   reward_function      'sentence_bleu'
01/13 14:01:18   rnn_feed_attn        True
01/13 14:01:18   rnn_input_dropout    0.0
01/13 14:01:18   rnn_output_dropout   0.0
01/13 14:01:18   rnn_state_dropout    0.0
01/13 14:01:18   save                 False
01/13 14:01:18   score_function       'corpus_bleu'
01/13 14:01:18   score_functions      ['bleu', 'loss']
01/13 14:01:18   script_dir           'scripts'
01/13 14:01:18   sgd_after_n_epoch    None
01/13 14:01:18   sgd_learning_rate    1.0
01/13 14:01:18   shuffle              True
01/13 14:01:18   softmax_temperature  1.0
01/13 14:01:18   steps_per_checkpoint 2000
01/13 14:01:18   steps_per_eval       2000
01/13 14:01:18   swap_memory          True
01/13 14:01:18   tie_embeddings       False
01/13 14:01:18   time_pooling         None
01/13 14:01:18   train                True
01/13 14:01:18   train_initial_states True
01/13 14:01:18   train_prefix         'train'
01/13 14:01:18   truncate_lines       True
01/13 14:01:18   update_first         False
01/13 14:01:18   use_baseline         False
01/13 14:01:18   use_dropout          False
01/13 14:01:18   use_lstm_full_state  False
01/13 14:01:18   use_previous_word    True
01/13 14:01:18   verbose              True
01/13 14:01:18   vocab_prefix         'vocab'
01/13 14:01:18   weight_scale         None
01/13 14:01:18   word_dropout         0.0
01/13 14:01:18 python random seed: 216593113495028857
01/13 14:01:18 tf random seed:     5918336156303126862
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:203: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

01/13 14:01:18 creating model
01/13 14:01:18 using device: /gpu:0
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:230: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

01/13 14:01:18 copying vocab to models/10_fold_seq2seq/data/vocab.code
01/13 14:01:18 copying vocab to models/10_fold_seq2seq/data/vocab.nl
01/13 14:01:18 reading vocabularies
01/13 14:01:18 creating model
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:60: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:111: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:33: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f66388d7908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f66388d7908>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:226: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f66388d7208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f66388d7208>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66388ccf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66388ccf60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:838: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66bd6ac9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66bd6ac9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66bd66fa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66bd66fa20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:432: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:435: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66bb4d4e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66bb4d4e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66bb4d4eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66bb4d4eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66bb53e358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66bb53e358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66bd6f97f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66bd6f97f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66bd6f97f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66bd6f97f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:919: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f66bb1d7f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f66bb1d7f60>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:131: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:223: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f6665abbf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f6665abbf98>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6665a78f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6665a78f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6665a06d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6665a06d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6665a06e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6665a06e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f666597c630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f666597c630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f666597c630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f666597c630>>: AssertionError: Bad argument number for Name: 3, expecting 4
01/13 14:01:22 model parameters (30)
01/13 14:01:22   baseline_step:0 ()
01/13 14:01:22   decoder_nl/attention_code/U_a/kernel:0 (512, 128)
01/13 14:01:22   decoder_nl/attention_code/W_a/bias:0 (128,)
01/13 14:01:22   decoder_nl/attention_code/W_a/kernel:0 (256, 128)
01/13 14:01:22   decoder_nl/attention_code/v_a:0 (128,)
01/13 14:01:22   decoder_nl/code/initial_state_projection/bias:0 (256,)
01/13 14:01:22   decoder_nl/code/initial_state_projection/kernel:0 (256, 256)
01/13 14:01:22   decoder_nl/gru_cell/candidate/bias:0 (256,)
01/13 14:01:22   decoder_nl/gru_cell/candidate/kernel:0 (1024, 256)
01/13 14:01:22   decoder_nl/gru_cell/gates/bias:0 (512,)
01/13 14:01:22   decoder_nl/gru_cell/gates/kernel:0 (1024, 512)
01/13 14:01:22   decoder_nl/maxout/bias:0 (256,)
01/13 14:01:22   decoder_nl/maxout/kernel:0 (1024, 256)
01/13 14:01:22   decoder_nl/softmax0/kernel:0 (128, 256)
01/13 14:01:22   decoder_nl/softmax1/bias:0 (37996,)
01/13 14:01:22   decoder_nl/softmax1/kernel:0 (256, 37996)
01/13 14:01:22   embedding_code:0 (50000, 256)
01/13 14:01:22   embedding_nl:0 (37996, 256)
01/13 14:01:22   encoder_code/initial_state_bw:0 (256,)
01/13 14:01:22   encoder_code/initial_state_fw:0 (256,)
01/13 14:01:22   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
01/13 14:01:22   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
01/13 14:01:22   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
01/13 14:01:22   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
01/13 14:01:22   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
01/13 14:01:22   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
01/13 14:01:22   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
01/13 14:01:22   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
01/13 14:01:22   global_step:0 ()
01/13 14:01:22   learning_rate:0 ()
01/13 14:01:22 number of parameters: 34.33M
WARNING:tensorflow:From /root/icpc/icpc/translate/translation_model.py:666: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

01/13 14:01:23 global step: 0
01/13 14:01:23 baseline step: 0
01/13 14:01:23 reading training data
01/13 14:01:23 total line count: 156717
01/13 14:01:27   lines read: 100000
01/13 14:01:30 files: data/gooddata/10_fold/train.code data/gooddata/10_fold/train.nl
01/13 14:01:30 lines reads: 156717
01/13 14:01:30 reading development data
01/13 14:01:31 files: data/gooddata/10_fold/test.code data/gooddata/10_fold/test.nl
01/13 14:01:31 lines reads: 17417
01/13 14:01:31 starting training
01/13 14:18:28 step 2000 epoch 1 learning rate 0.5 step-time 0.507 loss 77.990
01/13 14:18:28 starting evaluation
01/13 14:20:24 test bleu=1.67 loss=63.35 penalty=0.394 ratio=0.517
01/13 14:20:24 saving model to models/10_fold_seq2seq/checkpoints
01/13 14:20:24 finished saving model
01/13 14:20:24 new best model
01/13 14:24:10   decaying learning rate to: 0.475
01/13 14:37:05 step 4000 epoch 2 learning rate 0.475 step-time 0.498 loss 58.725
01/13 14:37:05 starting evaluation
01/13 14:39:35 test bleu=4.64 loss=55.41 penalty=0.820 ratio=0.834
01/13 14:39:35 saving model to models/10_fold_seq2seq/checkpoints
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
01/13 14:39:35 finished saving model
01/13 14:39:35 new best model
01/13 14:47:03   decaying learning rate to: 0.451
01/13 14:56:15 step 6000 epoch 3 learning rate 0.451 step-time 0.498 loss 52.224
01/13 14:56:15 starting evaluation
01/13 14:58:23 test bleu=6.93 loss=51.25 penalty=0.612 ratio=0.670
01/13 14:58:23 saving model to models/10_fold_seq2seq/checkpoints
01/13 14:58:23 finished saving model
01/13 14:58:23 new best model
01/13 15:09:39   decaying learning rate to: 0.429
01/13 15:15:06 step 8000 epoch 4 learning rate 0.429 step-time 0.499 loss 47.720
01/13 15:15:06 starting evaluation
01/13 15:17:40 test bleu=10.06 loss=46.81 penalty=0.833 ratio=0.845
01/13 15:17:40 saving model to models/10_fold_seq2seq/checkpoints
01/13 15:17:40 finished saving model
01/13 15:17:40 new best model
01/13 15:32:35   decaying learning rate to: 0.407
01/13 15:34:16 step 10000 epoch 5 learning rate 0.407 step-time 0.496 loss 44.143
01/13 15:34:16 starting evaluation
01/13 15:36:46 test bleu=11.27 loss=45.14 penalty=0.829 ratio=0.842
01/13 15:36:46 saving model to models/10_fold_seq2seq/checkpoints
01/13 15:36:46 finished saving model
01/13 15:36:46 new best model
01/13 15:53:24 step 12000 epoch 5 learning rate 0.407 step-time 0.497 loss 40.811
01/13 15:53:24 starting evaluation
01/13 15:56:02 test bleu=13.13 loss=42.99 penalty=1.000 ratio=1.037
01/13 15:56:02 saving model to models/10_fold_seq2seq/checkpoints
01/13 15:56:02 finished saving model
01/13 15:56:02 new best model
01/13 15:58:04   decaying learning rate to: 0.387
01/13 16:12:42 step 14000 epoch 6 learning rate 0.387 step-time 0.498 loss 37.878
01/13 16:12:42 starting evaluation
01/13 16:15:05 test bleu=14.25 loss=41.75 penalty=0.750 ratio=0.776
01/13 16:15:05 saving model to models/10_fold_seq2seq/checkpoints
01/13 16:15:05 finished saving model
01/13 16:15:05 new best model
01/13 16:20:53   decaying learning rate to: 0.368
01/13 16:31:47 step 16000 epoch 7 learning rate 0.368 step-time 0.499 loss 35.401
01/13 16:31:47 starting evaluation
01/13 16:34:09 test bleu=16.38 loss=41.04 penalty=0.761 ratio=0.785
01/13 16:34:09 saving model to models/10_fold_seq2seq/checkpoints
01/13 16:34:09 finished saving model
01/13 16:34:09 new best model
01/13 16:43:40   decaying learning rate to: 0.349
01/13 16:50:47 step 18000 epoch 8 learning rate 0.349 step-time 0.497 loss 33.196
01/13 16:50:47 starting evaluation
01/13 16:53:25 test bleu=18.23 loss=40.17 penalty=1.000 ratio=1.038
01/13 16:53:25 saving model to models/10_fold_seq2seq/checkpoints
01/13 16:53:25 finished saving model
01/13 16:53:25 new best model
01/13 17:06:37   decaying learning rate to: 0.332
01/13 17:10:02 step 20000 epoch 9 learning rate 0.332 step-time 0.496 loss 31.316
01/13 17:10:02 starting evaluation
01/13 17:12:31 test bleu=20.21 loss=39.66 penalty=0.878 ratio=0.885
01/13 17:12:31 saving model to models/10_fold_seq2seq/checkpoints
01/13 17:12:31 finished saving model
01/13 17:12:31 new best model
01/13 17:29:10 step 22000 epoch 9 learning rate 0.332 step-time 0.497 loss 29.424
01/13 17:29:10 starting evaluation
01/13 17:31:38 test bleu=20.83 loss=38.76 penalty=0.819 ratio=0.833
01/13 17:31:38 saving model to models/10_fold_seq2seq/checkpoints
01/13 17:31:38 finished saving model
01/13 17:31:38 new best model
01/13 17:31:59   decaying learning rate to: 0.315
01/13 17:48:15 step 24000 epoch 10 learning rate 0.315 step-time 0.496 loss 26.791
01/13 17:48:15 starting evaluation
01/13 17:50:47 test bleu=21.99 loss=38.96 penalty=0.873 ratio=0.880
01/13 17:50:47 saving model to models/10_fold_seq2seq/checkpoints
01/13 17:50:47 finished saving model
01/13 17:50:47 new best model
01/13 17:54:54   decaying learning rate to: 0.299
01/13 18:07:26 step 26000 epoch 11 learning rate 0.299 step-time 0.497 loss 25.130
01/13 18:07:26 starting evaluation
01/13 18:09:57 test bleu=23.51 loss=39.15 penalty=0.875 ratio=0.882
01/13 18:09:57 saving model to models/10_fold_seq2seq/checkpoints
01/13 18:09:57 finished saving model
01/13 18:09:57 new best model
01/13 18:17:45   decaying learning rate to: 0.284
01/13 18:26:34 step 28000 epoch 12 learning rate 0.284 step-time 0.496 loss 23.639
01/13 18:26:34 starting evaluation
01/13 18:29:04 test bleu=23.61 loss=39.80 penalty=0.857 ratio=0.866
01/13 18:29:04 saving model to models/10_fold_seq2seq/checkpoints
01/13 18:29:04 finished saving model
01/13 18:29:04 new best model
01/13 18:40:33   decaying learning rate to: 0.27
01/13 18:45:41 step 30000 epoch 13 learning rate 0.27 step-time 0.497 loss 22.167
01/13 18:45:41 starting evaluation
01/13 18:48:14 test bleu=25.24 loss=40.30 penalty=0.905 ratio=0.909
01/13 18:48:14 saving model to models/10_fold_seq2seq/checkpoints
01/13 18:48:14 finished saving model
01/13 18:48:14 new best model
01/13 19:03:37   decaying learning rate to: 0.257
01/13 19:04:58 step 32000 epoch 14 learning rate 0.257 step-time 0.500 loss 20.832
01/13 19:04:58 starting evaluation
01/13 19:07:33 test bleu=26.38 loss=41.14 penalty=0.934 ratio=0.936
01/13 19:07:33 saving model to models/10_fold_seq2seq/checkpoints
01/13 19:07:33 finished saving model
01/13 19:07:33 new best model
01/13 19:24:08 step 34000 epoch 14 learning rate 0.257 step-time 0.496 loss 19.007
01/13 19:24:08 starting evaluation
01/13 19:26:42 test bleu=26.54 loss=40.16 penalty=0.919 ratio=0.922
01/13 19:26:42 saving model to models/10_fold_seq2seq/checkpoints
01/13 19:26:42 finished saving model
01/13 19:26:42 new best model
01/13 19:29:05   decaying learning rate to: 0.244
01/13 19:43:20 step 36000 epoch 15 learning rate 0.244 step-time 0.497 loss 17.428
01/13 19:43:20 starting evaluation
01/13 19:45:56 test bleu=27.14 loss=41.84 penalty=0.942 ratio=0.943
01/13 19:45:56 saving model to models/10_fold_seq2seq/checkpoints
01/13 19:45:56 finished saving model
01/13 19:45:56 new best model
01/13 19:52:06   decaying learning rate to: 0.232
01/13 20:02:36 step 38000 epoch 16 learning rate 0.232 step-time 0.498 loss 16.345
01/13 20:02:36 starting evaluation
01/13 20:05:09 test bleu=27.78 loss=43.42 penalty=0.946 ratio=0.947
01/13 20:05:09 saving model to models/10_fold_seq2seq/checkpoints
01/13 20:05:09 finished saving model
01/13 20:05:09 new best model
01/13 20:14:57   decaying learning rate to: 0.22
01/13 20:21:45 step 40000 epoch 17 learning rate 0.22 step-time 0.496 loss 15.242
01/13 20:21:45 starting evaluation
01/13 20:24:20 test bleu=27.77 loss=44.89 penalty=0.952 ratio=0.953
01/13 20:24:20 saving model to models/10_fold_seq2seq/checkpoints
01/13 20:24:20 finished saving model
01/13 20:37:56   decaying learning rate to: 0.209
01/13 20:41:01 step 42000 epoch 18 learning rate 0.209 step-time 0.499 loss 14.259
01/13 20:41:01 starting evaluation
01/13 20:43:34 test bleu=28.40 loss=46.60 penalty=0.951 ratio=0.952
01/13 20:43:34 saving model to models/10_fold_seq2seq/checkpoints
01/13 20:43:34 finished saving model
01/13 20:43:34 new best model
01/13 21:00:09 step 44000 epoch 18 learning rate 0.209 step-time 0.496 loss 13.241
01/13 21:00:09 starting evaluation
01/13 21:02:44 test bleu=29.38 loss=45.05 penalty=0.959 ratio=0.960
01/13 21:02:44 saving model to models/10_fold_seq2seq/checkpoints
01/13 21:02:44 finished saving model
01/13 21:02:44 new best model
01/13 21:03:24   decaying learning rate to: 0.199
01/13 21:19:26 step 46000 epoch 19 learning rate 0.199 step-time 0.499 loss 11.675
01/13 21:19:26 starting evaluation
01/13 21:22:03 test bleu=29.63 loss=47.85 penalty=1.000 ratio=1.010
01/13 21:22:03 saving model to models/10_fold_seq2seq/checkpoints
01/13 21:22:03 finished saving model
01/13 21:22:03 new best model
01/13 21:26:26   decaying learning rate to: 0.189
01/13 21:38:39 step 48000 epoch 20 learning rate 0.189 step-time 0.496 loss 10.942
01/13 21:38:39 starting evaluation
01/13 21:41:14 test bleu=29.89 loss=49.34 penalty=1.000 ratio=1.000
01/13 21:41:14 saving model to models/10_fold_seq2seq/checkpoints
01/13 21:41:14 finished saving model
01/13 21:41:14 new best model
01/13 21:49:25   decaying learning rate to: 0.179
01/13 21:57:55 step 50000 epoch 21 learning rate 0.179 step-time 0.499 loss 10.171
01/13 21:57:55 starting evaluation
01/13 22:00:31 test bleu=30.34 loss=51.11 penalty=0.990 ratio=0.990
01/13 22:00:31 saving model to models/10_fold_seq2seq/checkpoints
01/13 22:00:31 finished saving model
01/13 22:00:31 new best model
01/13 22:12:25   decaying learning rate to: 0.17
01/13 22:17:11 step 52000 epoch 22 learning rate 0.17 step-time 0.498 loss 9.481
01/13 22:17:11 starting evaluation
01/13 22:19:45 test bleu=30.51 loss=53.57 penalty=0.969 ratio=0.969
01/13 22:19:45 saving model to models/10_fold_seq2seq/checkpoints
01/13 22:19:46 finished saving model
01/13 22:19:46 new best model
01/13 22:35:21   decaying learning rate to: 0.162
01/13 22:36:22 step 54000 epoch 23 learning rate 0.162 step-time 0.496 loss 8.866
01/13 22:36:22 starting evaluation
01/13 22:38:56 test bleu=30.50 loss=54.73 penalty=0.972 ratio=0.973
01/13 22:38:56 saving model to models/10_fold_seq2seq/checkpoints
01/13 22:38:56 finished saving model
01/13 22:55:33 step 56000 epoch 23 learning rate 0.162 step-time 0.497 loss 7.841
01/13 22:55:33 starting evaluation
01/13 22:58:04 test bleu=30.93 loss=55.19 penalty=0.894 ratio=0.899
01/13 22:58:04 saving model to models/10_fold_seq2seq/checkpoints
01/13 22:58:05 finished saving model
01/13 22:58:05 new best model
01/13 23:00:46   decaying learning rate to: 0.154
01/13 23:14:37 step 58000 epoch 24 learning rate 0.154 step-time 0.494 loss 7.124
01/13 23:14:37 starting evaluation
01/13 23:17:12 test bleu=31.30 loss=57.97 penalty=1.000 ratio=1.005
01/13 23:17:12 saving model to models/10_fold_seq2seq/checkpoints
01/13 23:17:12 finished saving model
01/13 23:17:12 new best model
01/13 23:23:41   decaying learning rate to: 0.146
01/13 23:33:56 step 60000 epoch 25 learning rate 0.146 step-time 0.500 loss 6.669
01/13 23:33:56 starting evaluation
01/13 23:36:32 test bleu=31.21 loss=60.64 penalty=1.000 ratio=1.011
01/13 23:36:32 saving model to models/10_fold_seq2seq/checkpoints
01/13 23:36:32 finished saving model
01/13 23:46:41   decaying learning rate to: 0.139
01/13 23:53:10 step 62000 epoch 26 learning rate 0.139 step-time 0.497 loss 6.168
01/13 23:53:10 starting evaluation
01/13 23:55:42 test bleu=31.79 loss=63.11 penalty=0.969 ratio=0.970
01/13 23:55:42 saving model to models/10_fold_seq2seq/checkpoints
01/13 23:55:42 finished saving model
01/13 23:55:42 new best model
01/14 00:09:33   decaying learning rate to: 0.132
01/14 00:12:16 step 64000 epoch 27 learning rate 0.132 step-time 0.495 loss 5.760
01/14 00:12:16 starting evaluation
01/14 00:14:53 test bleu=31.30 loss=63.86 penalty=1.000 ratio=1.016
01/14 00:14:53 saving model to models/10_fold_seq2seq/checkpoints
01/14 00:14:53 finished saving model
01/14 00:31:34 step 66000 epoch 27 learning rate 0.132 step-time 0.499 loss 5.269
01/14 00:31:34 starting evaluation
01/14 00:34:10 test bleu=32.24 loss=64.39 penalty=1.000 ratio=1.001
01/14 00:34:10 saving model to models/10_fold_seq2seq/checkpoints
01/14 00:34:10 finished saving model
01/14 00:34:10 new best model
01/14 00:35:11   decaying learning rate to: 0.125
01/14 00:50:42 step 68000 epoch 28 learning rate 0.125 step-time 0.494 loss 4.628
01/14 00:50:42 starting evaluation
01/14 00:53:19 test bleu=31.71 loss=67.68 penalty=1.000 ratio=1.011
01/14 00:53:19 saving model to models/10_fold_seq2seq/checkpoints
01/14 00:53:19 finished saving model
01/14 00:58:06   decaying learning rate to: 0.119
01/14 01:09:58 step 70000 epoch 29 learning rate 0.119 step-time 0.497 loss 4.301
01/14 01:09:58 starting evaluation
01/14 01:12:33 test bleu=31.59 loss=69.61 penalty=1.000 ratio=1.027
01/14 01:12:33 saving model to models/10_fold_seq2seq/checkpoints
01/14 01:12:33 finished saving model
01/14 01:20:59   decaying learning rate to: 0.113
01/14 01:29:07 step 72000 epoch 30 learning rate 0.113 step-time 0.495 loss 4.032
01/14 01:29:07 starting evaluation
01/14 01:31:41 test bleu=32.02 loss=71.73 penalty=1.000 ratio=1.018
01/14 01:31:41 saving model to models/10_fold_seq2seq/checkpoints
01/14 01:31:41 finished saving model
01/14 01:43:54   decaying learning rate to: 0.107
01/14 01:48:19 step 74000 epoch 31 learning rate 0.107 step-time 0.497 loss 3.733
01/14 01:48:19 starting evaluation
01/14 01:50:55 test bleu=31.17 loss=74.18 penalty=1.000 ratio=1.056
01/14 01:50:55 saving model to models/10_fold_seq2seq/checkpoints
01/14 01:50:56 finished saving model
01/14 02:06:54   decaying learning rate to: 0.102
01/14 02:07:34 step 76000 epoch 32 learning rate 0.102 step-time 0.498 loss 3.490
01/14 02:07:34 starting evaluation
01/14 02:10:11 test bleu=31.30 loss=75.81 penalty=1.000 ratio=1.046
01/14 02:10:11 saving model to models/10_fold_seq2seq/checkpoints
01/14 02:10:11 finished saving model
01/14 02:26:47 step 78000 epoch 32 learning rate 0.102 step-time 0.496 loss 3.071
01/14 02:26:47 starting evaluation
01/14 02:29:25 test bleu=31.49 loss=77.20 penalty=1.000 ratio=1.052
01/14 02:29:25 saving model to models/10_fold_seq2seq/checkpoints
01/14 02:29:25 finished saving model
01/14 02:32:29   decaying learning rate to: 0.0969
01/14 02:46:03 step 80000 epoch 33 learning rate 0.0969 step-time 0.497 loss 2.785
01/14 02:46:03 starting evaluation
01/14 02:48:38 test bleu=31.94 loss=79.77 penalty=1.000 ratio=1.039
01/14 02:48:38 saving model to models/10_fold_seq2seq/checkpoints
01/14 02:48:38 finished saving model
01/14 02:55:29   decaying learning rate to: 0.092
01/14 03:05:19 step 82000 epoch 34 learning rate 0.092 step-time 0.498 loss 2.626
01/14 03:05:19 starting evaluation
01/14 03:07:55 test bleu=32.21 loss=82.19 penalty=1.000 ratio=1.022
01/14 03:07:55 saving model to models/10_fold_seq2seq/checkpoints
01/14 03:07:55 finished saving model
01/14 03:18:25   decaying learning rate to: 0.0874
01/14 03:24:33 step 84000 epoch 35 learning rate 0.0874 step-time 0.497 loss 2.442
01/14 03:24:33 starting evaluation
01/14 03:27:09 test bleu=32.24 loss=84.44 penalty=1.000 ratio=1.031
01/14 03:27:09 saving model to models/10_fold_seq2seq/checkpoints
01/14 03:27:09 finished saving model
01/14 03:27:09 new best model
01/14 03:41:31   decaying learning rate to: 0.083
01/14 03:43:54 step 86000 epoch 36 learning rate 0.083 step-time 0.500 loss 2.287
01/14 03:43:54 starting evaluation
01/14 03:46:30 test bleu=31.83 loss=85.73 penalty=1.000 ratio=1.047
01/14 03:46:30 saving model to models/10_fold_seq2seq/checkpoints
01/14 03:46:30 finished saving model
01/14 04:03:06 step 88000 epoch 36 learning rate 0.083 step-time 0.496 loss 2.072
01/14 04:03:06 starting evaluation
01/14 04:05:42 test bleu=32.46 loss=87.63 penalty=1.000 ratio=1.029
01/14 04:05:42 saving model to models/10_fold_seq2seq/checkpoints
01/14 04:05:42 finished saving model
01/14 04:05:42 new best model
01/14 04:07:04   decaying learning rate to: 0.0789
01/14 04:22:22 step 90000 epoch 37 learning rate 0.0789 step-time 0.498 loss 1.852
01/14 04:22:22 starting evaluation
01/14 04:24:57 test bleu=31.80 loss=88.91 penalty=1.000 ratio=1.051
01/14 04:24:57 saving model to models/10_fold_seq2seq/checkpoints
01/14 04:24:57 finished saving model
01/14 04:30:03   decaying learning rate to: 0.0749
01/14 04:41:33 step 92000 epoch 38 learning rate 0.0749 step-time 0.496 loss 1.734
01/14 04:41:33 starting evaluation
01/14 04:44:08 test bleu=31.84 loss=91.65 penalty=1.000 ratio=1.048
01/14 04:44:08 saving model to models/10_fold_seq2seq/checkpoints
01/14 04:44:09 finished saving model
01/14 04:52:58   decaying learning rate to: 0.0712
01/14 05:00:45 step 94000 epoch 39 learning rate 0.0712 step-time 0.496 loss 1.614
01/14 05:00:45 starting evaluation
01/14 05:03:22 test bleu=32.68 loss=93.07 penalty=1.000 ratio=1.022
01/14 05:03:22 saving model to models/10_fold_seq2seq/checkpoints
01/14 05:03:22 finished saving model
01/14 05:03:22 new best model
01/14 05:16:00   decaying learning rate to: 0.0676
01/14 05:20:03 step 96000 epoch 40 learning rate 0.0676 step-time 0.498 loss 1.528
01/14 05:20:03 starting evaluation
01/14 05:22:38 test bleu=32.56 loss=95.44 penalty=1.000 ratio=1.033
01/14 05:22:38 saving model to models/10_fold_seq2seq/checkpoints
01/14 05:22:39 finished saving model
01/14 05:38:56   decaying learning rate to: 0.0643
01/14 05:39:16 step 98000 epoch 41 learning rate 0.0643 step-time 0.497 loss 1.426
01/14 05:39:16 starting evaluation
01/14 05:41:52 test bleu=32.28 loss=96.40 penalty=1.000 ratio=1.042
01/14 05:41:52 saving model to models/10_fold_seq2seq/checkpoints
01/14 05:41:53 finished saving model
01/14 05:58:35 step 100000 epoch 41 learning rate 0.0643 step-time 0.499 loss 1.253
01/14 05:58:35 starting evaluation
01/14 06:01:11 test bleu=32.19 loss=97.83 penalty=1.000 ratio=1.040
01/14 06:01:11 saving model to models/10_fold_seq2seq/checkpoints
01/14 06:01:12 finished saving model
01/14 06:04:34   decaying learning rate to: 0.061
01/14 06:17:45 step 102000 epoch 42 learning rate 0.061 step-time 0.495 loss 1.171
01/14 06:17:45 starting evaluation
01/14 06:20:20 test bleu=32.98 loss=100.00 penalty=1.000 ratio=1.023
01/14 06:20:20 saving model to models/10_fold_seq2seq/checkpoints
01/14 06:20:21 finished saving model
01/14 06:20:21 new best model
01/14 06:27:27   decaying learning rate to: 0.058
01/14 06:36:59 step 104000 epoch 43 learning rate 0.058 step-time 0.497 loss 1.112
01/14 06:36:59 starting evaluation
01/14 06:39:36 test bleu=32.41 loss=101.40 penalty=1.000 ratio=1.041
01/14 06:39:36 saving model to models/10_fold_seq2seq/checkpoints
01/14 06:39:36 finished saving model
01/14 06:50:28   decaying learning rate to: 0.0551
01/14 06:56:13 step 106000 epoch 44 learning rate 0.0551 step-time 0.496 loss 1.053
01/14 06:56:13 starting evaluation
01/14 06:58:49 test bleu=32.10 loss=103.62 penalty=1.000 ratio=1.044
01/14 06:58:49 saving model to models/10_fold_seq2seq/checkpoints
01/14 06:58:49 finished saving model
01/14 07:13:30   decaying learning rate to: 0.0523
01/14 07:15:30 step 108000 epoch 45 learning rate 0.0523 step-time 0.498 loss 0.988
01/14 07:15:30 starting evaluation
01/14 07:18:05 test bleu=32.49 loss=104.86 penalty=1.000 ratio=1.037
01/14 07:18:05 saving model to models/10_fold_seq2seq/checkpoints
01/14 07:18:06 finished saving model
01/14 07:34:50 step 110000 epoch 45 learning rate 0.0523 step-time 0.500 loss 0.915
01/14 07:34:50 starting evaluation
01/14 07:37:26 test bleu=32.24 loss=106.63 penalty=1.000 ratio=1.042
01/14 07:37:26 saving model to models/10_fold_seq2seq/checkpoints
01/14 07:37:26 finished saving model
01/14 07:39:09   decaying learning rate to: 0.0497
01/14 07:54:01 step 112000 epoch 46 learning rate 0.0497 step-time 0.495 loss 0.826
01/14 07:54:01 starting evaluation
01/14 07:56:37 test bleu=31.97 loss=108.09 penalty=1.000 ratio=1.054
01/14 07:56:37 saving model to models/10_fold_seq2seq/checkpoints
01/14 07:56:37 finished saving model
01/14 08:02:03   decaying learning rate to: 0.0472
01/14 08:13:14 step 114000 epoch 47 learning rate 0.0472 step-time 0.497 loss 0.791
01/14 08:13:14 starting evaluation
01/14 08:15:50 test bleu=31.90 loss=108.66 penalty=1.000 ratio=1.057
01/14 08:15:50 saving model to models/10_fold_seq2seq/checkpoints
01/14 08:15:51 finished saving model
01/14 08:25:03   decaying learning rate to: 0.0449
01/14 08:32:28 step 116000 epoch 48 learning rate 0.0449 step-time 0.497 loss 0.753
01/14 08:32:28 starting evaluation
01/14 08:35:03 test bleu=32.78 loss=110.75 penalty=1.000 ratio=1.031
01/14 08:35:03 saving model to models/10_fold_seq2seq/checkpoints
01/14 08:35:03 finished saving model
01/14 08:48:00   decaying learning rate to: 0.0426
01/14 08:51:46 step 118000 epoch 49 learning rate 0.0426 step-time 0.500 loss 0.724
01/14 08:51:46 starting evaluation
01/14 08:54:23 test bleu=31.60 loss=111.47 penalty=1.000 ratio=1.069
01/14 08:54:23 saving model to models/10_fold_seq2seq/checkpoints
01/14 08:54:23 finished saving model
01/14 09:11:00 step 120000 epoch 50 learning rate 0.0426 step-time 0.497 loss 0.693
01/14 09:11:00 starting evaluation
01/14 09:13:35 test bleu=32.36 loss=112.16 penalty=1.000 ratio=1.046
01/14 09:13:35 saving model to models/10_fold_seq2seq/checkpoints
01/14 09:13:35 finished saving model
01/14 09:13:35   decaying learning rate to: 0.0405
01/14 09:30:15 step 122000 epoch 50 learning rate 0.0405 step-time 0.498 loss 0.613
01/14 09:30:15 starting evaluation
01/14 09:32:50 test bleu=32.01 loss=112.93 penalty=1.000 ratio=1.055
01/14 09:32:50 saving model to models/10_fold_seq2seq/checkpoints
01/14 09:32:50 finished saving model
01/14 09:36:34   decaying learning rate to: 0.0385
01/14 09:49:32 step 124000 epoch 51 learning rate 0.0385 step-time 0.499 loss 0.593
01/14 09:49:32 starting evaluation
01/14 09:52:09 test bleu=32.59 loss=114.35 penalty=1.000 ratio=1.037
01/14 09:52:09 saving model to models/10_fold_seq2seq/checkpoints
01/14 09:52:09 finished saving model
01/14 09:59:35   decaying learning rate to: 0.0365
01/14 10:08:44 step 126000 epoch 52 learning rate 0.0365 step-time 0.496 loss 0.569
01/14 10:08:44 starting evaluation
01/14 10:11:20 test bleu=32.11 loss=115.12 penalty=1.000 ratio=1.051
01/14 10:11:20 saving model to models/10_fold_seq2seq/checkpoints
01/14 10:11:21 finished saving model
01/14 10:22:36   decaying learning rate to: 0.0347
01/14 10:28:02 step 128000 epoch 53 learning rate 0.0347 step-time 0.499 loss 0.547
01/14 10:28:02 starting evaluation
01/14 10:30:39 test bleu=32.84 loss=116.94 penalty=1.000 ratio=1.029
01/14 10:30:39 saving model to models/10_fold_seq2seq/checkpoints
01/14 10:30:39 finished saving model
01/14 10:45:36   decaying learning rate to: 0.033
01/14 10:47:15 step 130000 epoch 54 learning rate 0.033 step-time 0.496 loss 0.535
01/14 10:47:15 starting evaluation
01/14 10:49:49 test bleu=32.61 loss=117.15 penalty=1.000 ratio=1.039
01/14 10:49:49 saving model to models/10_fold_seq2seq/checkpoints
01/14 10:49:50 finished saving model
01/14 11:06:32 step 132000 epoch 54 learning rate 0.033 step-time 0.499 loss 0.491
01/14 11:06:32 starting evaluation
01/14 11:09:10 test bleu=32.29 loss=117.00 penalty=1.000 ratio=1.045
01/14 11:09:10 saving model to models/10_fold_seq2seq/checkpoints
01/14 11:09:10 finished saving model
01/14 11:11:13   decaying learning rate to: 0.0313
01/14 11:25:49 step 134000 epoch 55 learning rate 0.0313 step-time 0.497 loss 0.462
01/14 11:25:49 starting evaluation
01/14 11:28:24 test bleu=32.16 loss=118.50 penalty=1.000 ratio=1.050
01/14 11:28:24 saving model to models/10_fold_seq2seq/checkpoints
01/14 11:28:24 finished saving model
01/14 11:34:10   decaying learning rate to: 0.0298
01/14 11:45:01 step 136000 epoch 56 learning rate 0.0298 step-time 0.496 loss 0.446
01/14 11:45:01 starting evaluation
01/14 11:47:36 test bleu=32.54 loss=119.17 penalty=1.000 ratio=1.041
01/14 11:47:36 saving model to models/10_fold_seq2seq/checkpoints
01/14 11:47:36 finished saving model
01/14 11:56:54   decaying learning rate to: 0.0283
01/14 12:03:55 step 138000 epoch 57 learning rate 0.0283 step-time 0.488 loss 0.434
01/14 12:03:55 starting evaluation
01/14 12:06:32 test bleu=32.62 loss=119.70 penalty=1.000 ratio=1.038
01/14 12:06:32 saving model to models/10_fold_seq2seq/checkpoints
01/14 12:06:32 finished saving model
01/14 12:20:19   decaying learning rate to: 0.0269
01/14 12:23:48 step 140000 epoch 58 learning rate 0.0269 step-time 0.516 loss 0.420
01/14 12:23:48 starting evaluation
01/14 12:26:30 test bleu=32.60 loss=119.90 penalty=1.000 ratio=1.039
01/14 12:26:30 saving model to models/10_fold_seq2seq/checkpoints
01/14 12:26:30 finished saving model
01/14 12:43:49 step 142000 epoch 58 learning rate 0.0269 step-time 0.517 loss 0.408
01/14 12:43:49 starting evaluation
01/14 12:46:31 test bleu=32.99 loss=120.09 penalty=1.000 ratio=1.026
01/14 12:46:31 saving model to models/10_fold_seq2seq/checkpoints
01/14 12:46:32 finished saving model
01/14 12:46:32 new best model
01/14 12:46:54   decaying learning rate to: 0.0255
01/14 13:03:52 step 144000 epoch 59 learning rate 0.0255 step-time 0.518 loss 0.368
01/14 13:03:52 starting evaluation
01/14 13:06:29 test bleu=32.61 loss=120.96 penalty=1.000 ratio=1.039
01/14 13:06:29 saving model to models/10_fold_seq2seq/checkpoints
01/14 13:06:30 finished saving model
01/14 13:10:40   decaying learning rate to: 0.0242
01/14 13:23:47 step 146000 epoch 60 learning rate 0.0242 step-time 0.517 loss 0.361
01/14 13:23:47 starting evaluation
01/14 13:26:30 test bleu=32.35 loss=121.67 penalty=1.000 ratio=1.044
01/14 13:26:30 saving model to models/10_fold_seq2seq/checkpoints
01/14 13:26:30 finished saving model
01/14 13:34:39   decaying learning rate to: 0.023
01/14 13:43:48 step 148000 epoch 61 learning rate 0.023 step-time 0.517 loss 0.353
01/14 13:43:48 starting evaluation
01/14 13:46:31 test bleu=32.01 loss=121.73 penalty=1.000 ratio=1.058
01/14 13:46:31 saving model to models/10_fold_seq2seq/checkpoints
01/14 13:46:31 finished saving model
01/14 13:58:27   decaying learning rate to: 0.0219
01/14 14:03:46 step 150000 epoch 62 learning rate 0.0219 step-time 0.515 loss 0.343
01/14 14:03:46 starting evaluation
01/14 14:06:29 test bleu=32.29 loss=122.47 penalty=1.000 ratio=1.050
01/14 14:06:29 saving model to models/10_fold_seq2seq/checkpoints
01/14 14:06:29 finished saving model
01/14 14:22:22   decaying learning rate to: 0.0208
01/14 14:23:46 step 152000 epoch 63 learning rate 0.0208 step-time 0.517 loss 0.338
01/14 14:23:46 starting evaluation
01/14 14:26:30 test bleu=31.66 loss=122.85 penalty=1.000 ratio=1.068
01/14 14:26:30 saving model to models/10_fold_seq2seq/checkpoints
01/14 14:26:30 finished saving model
01/14 14:43:49 step 154000 epoch 63 learning rate 0.0208 step-time 0.518 loss 0.311
01/14 14:43:49 starting evaluation
01/14 14:46:29 test bleu=32.27 loss=122.93 penalty=1.000 ratio=1.048
01/14 14:46:29 saving model to models/10_fold_seq2seq/checkpoints
01/14 14:46:29 finished saving model
01/14 14:48:59   decaying learning rate to: 0.0197
01/14 15:03:48 step 156000 epoch 64 learning rate 0.0197 step-time 0.518 loss 0.303
01/14 15:03:48 starting evaluation
01/14 15:06:32 test bleu=32.33 loss=123.33 penalty=1.000 ratio=1.046
01/14 15:06:32 saving model to models/10_fold_seq2seq/checkpoints
01/14 15:06:32 finished saving model
01/14 15:12:55   decaying learning rate to: 0.0188
01/14 15:23:55 step 158000 epoch 65 learning rate 0.0188 step-time 0.520 loss 0.293
01/14 15:23:55 starting evaluation
01/14 15:26:38 test bleu=32.40 loss=123.11 penalty=1.000 ratio=1.043
01/14 15:26:38 saving model to models/10_fold_seq2seq/checkpoints
01/14 15:26:38 finished saving model
01/14 15:36:47   decaying learning rate to: 0.0178
01/14 15:43:51 step 160000 epoch 66 learning rate 0.0178 step-time 0.514 loss 0.285
01/14 15:43:51 starting evaluation
01/14 15:46:34 test bleu=32.58 loss=124.08 penalty=1.000 ratio=1.041
01/14 15:46:34 saving model to models/10_fold_seq2seq/checkpoints
01/14 15:46:34 finished saving model
01/14 16:00:46   decaying learning rate to: 0.0169
01/14 16:03:56 step 162000 epoch 67 learning rate 0.0169 step-time 0.519 loss 0.285
01/14 16:03:56 starting evaluation
01/14 16:06:36 test bleu=32.44 loss=124.56 penalty=1.000 ratio=1.044
01/14 16:06:36 saving model to models/10_fold_seq2seq/checkpoints
01/14 16:06:36 finished saving model
01/14 16:23:48 step 164000 epoch 67 learning rate 0.0169 step-time 0.514 loss 0.276
01/14 16:23:48 starting evaluation
01/14 16:26:29 test bleu=32.43 loss=124.21 penalty=1.000 ratio=1.044
01/14 16:26:29 saving model to models/10_fold_seq2seq/checkpoints
01/14 16:26:30 finished saving model
01/14 16:27:12   decaying learning rate to: 0.0161
01/14 16:43:48 step 166000 epoch 68 learning rate 0.0161 step-time 0.517 loss 0.257
01/14 16:43:48 starting evaluation
01/14 16:46:30 test bleu=32.21 loss=124.55 penalty=1.000 ratio=1.049
01/14 16:46:30 saving model to models/10_fold_seq2seq/checkpoints
01/14 16:46:31 finished saving model
01/14 16:51:08   decaying learning rate to: 0.0153
01/14 17:03:53 step 168000 epoch 69 learning rate 0.0153 step-time 0.519 loss 0.249
01/14 17:03:53 starting evaluation
01/14 17:06:37 test bleu=31.63 loss=124.91 penalty=1.000 ratio=1.067
01/14 17:06:37 saving model to models/10_fold_seq2seq/checkpoints
01/14 17:06:37 finished saving model
01/14 17:15:02   decaying learning rate to: 0.0145
01/14 17:23:50 step 170000 epoch 70 learning rate 0.0145 step-time 0.514 loss 0.253
01/14 17:23:50 starting evaluation
01/14 17:26:29 test bleu=32.11 loss=124.97 penalty=1.000 ratio=1.055
01/14 17:26:29 saving model to models/10_fold_seq2seq/checkpoints
01/14 17:26:30 finished saving model
01/14 17:38:56   decaying learning rate to: 0.0138
01/14 17:43:53 step 172000 epoch 71 learning rate 0.0138 step-time 0.520 loss 0.241
01/14 17:43:53 starting evaluation
01/14 17:46:36 test bleu=32.33 loss=125.70 penalty=1.000 ratio=1.048
01/14 17:46:36 saving model to models/10_fold_seq2seq/checkpoints
01/14 17:46:36 finished saving model
01/14 18:02:48   decaying learning rate to: 0.0131
01/14 18:03:56 step 174000 epoch 72 learning rate 0.0131 step-time 0.518 loss 0.241
01/14 18:03:56 starting evaluation
01/14 18:06:52 test bleu=32.18 loss=126.24 penalty=1.000 ratio=1.050
01/14 18:06:52 saving model to models/10_fold_seq2seq/checkpoints
01/14 18:06:52 finished saving model
01/14 18:25:42 step 176000 epoch 72 learning rate 0.0131 step-time 0.562 loss 0.227
01/14 18:25:42 starting evaluation
01/14 18:28:37 test bleu=32.13 loss=125.86 penalty=1.000 ratio=1.050
01/14 18:28:37 saving model to models/10_fold_seq2seq/checkpoints
01/14 18:28:37 finished saving model
01/14 18:31:42   decaying learning rate to: 0.0124
01/14 18:47:10 step 178000 epoch 73 learning rate 0.0124 step-time 0.554 loss 0.221
01/14 18:47:10 starting evaluation
01/14 18:50:02 test bleu=32.13 loss=126.20 penalty=1.000 ratio=1.052
01/14 18:50:02 saving model to models/10_fold_seq2seq/checkpoints
01/14 18:50:02 finished saving model
01/14 18:57:21   decaying learning rate to: 0.0118
01/14 19:08:51 step 180000 epoch 74 learning rate 0.0118 step-time 0.562 loss 0.215
01/14 19:08:51 starting evaluation
01/14 19:11:44 test bleu=31.93 loss=126.57 penalty=1.000 ratio=1.058
01/14 19:11:44 saving model to models/10_fold_seq2seq/checkpoints
01/14 19:11:44 finished saving model
01/14 19:23:13   decaying learning rate to: 0.0112
01/14 19:30:29 step 182000 epoch 75 learning rate 0.0112 step-time 0.560 loss 0.215
01/14 19:30:29 starting evaluation
01/14 19:33:25 test bleu=32.69 loss=127.07 penalty=1.000 ratio=1.037
01/14 19:33:25 saving model to models/10_fold_seq2seq/checkpoints
01/14 19:33:25 finished saving model
01/14 19:48:56   decaying learning rate to: 0.0107
01/14 19:51:53 step 184000 epoch 76 learning rate 0.0107 step-time 0.551 loss 0.211
01/14 19:51:53 starting evaluation
01/14 19:54:50 test bleu=32.42 loss=126.81 penalty=1.000 ratio=1.045
01/14 19:54:50 saving model to models/10_fold_seq2seq/checkpoints
01/14 19:54:50 finished saving model
01/14 20:13:39 step 186000 epoch 76 learning rate 0.0107 step-time 0.562 loss 0.208
01/14 20:13:39 starting evaluation
01/14 20:16:33 test bleu=32.09 loss=127.04 penalty=1.000 ratio=1.052
01/14 20:16:33 saving model to models/10_fold_seq2seq/checkpoints
01/14 20:16:33 finished saving model
01/14 20:17:43   decaying learning rate to: 0.0101
01/14 20:34:27 step 188000 epoch 77 learning rate 0.0101 step-time 0.534 loss 0.194
01/14 20:34:27 starting evaluation
01/14 20:37:16 test bleu=31.91 loss=127.69 penalty=1.000 ratio=1.058
01/14 20:37:16 saving model to models/10_fold_seq2seq/checkpoints
01/14 20:37:16 finished saving model
01/14 20:42:27   decaying learning rate to: 0.00963
01/14 20:55:30 step 190000 epoch 78 learning rate 0.00963 step-time 0.544 loss 0.194
01/14 20:55:30 starting evaluation
01/14 20:58:20 test bleu=32.01 loss=128.00 penalty=1.000 ratio=1.054
01/14 20:58:20 saving model to models/10_fold_seq2seq/checkpoints
01/14 20:58:20 finished saving model
01/14 21:07:29   decaying learning rate to: 0.00915
01/14 21:16:19 step 192000 epoch 79 learning rate 0.00915 step-time 0.537 loss 0.189
01/14 21:16:19 starting evaluation
01/14 21:19:04 test bleu=32.21 loss=128.06 penalty=1.000 ratio=1.047
01/14 21:19:04 saving model to models/10_fold_seq2seq/checkpoints
01/14 21:19:04 finished saving model
01/14 21:32:23   decaying learning rate to: 0.00869
01/14 21:37:08 step 194000 epoch 80 learning rate 0.00869 step-time 0.539 loss 0.191
01/14 21:37:08 starting evaluation
01/14 21:39:59 test bleu=31.99 loss=128.30 penalty=1.000 ratio=1.055
01/14 21:39:59 saving model to models/10_fold_seq2seq/checkpoints
01/14 21:39:59 finished saving model
01/14 21:57:20   decaying learning rate to: 0.00826
01/14 21:58:04 step 196000 epoch 81 learning rate 0.00826 step-time 0.540 loss 0.187
01/14 21:58:04 starting evaluation
01/14 22:00:53 test bleu=32.00 loss=128.20 penalty=1.000 ratio=1.054
01/14 22:00:53 saving model to models/10_fold_seq2seq/checkpoints
01/14 22:00:53 finished saving model
01/14 22:18:46 step 198000 epoch 81 learning rate 0.00826 step-time 0.534 loss 0.179
01/14 22:18:46 starting evaluation
01/14 22:21:33 test bleu=32.08 loss=128.57 penalty=1.000 ratio=1.053
01/14 22:21:33 saving model to models/10_fold_seq2seq/checkpoints
01/14 22:21:33 finished saving model
01/14 22:24:53   decaying learning rate to: 0.00784
01/14 22:39:43 step 200000 epoch 82 learning rate 0.00784 step-time 0.543 loss 0.172
01/14 22:39:43 starting evaluation
01/14 22:42:33 test bleu=32.49 loss=128.95 penalty=1.000 ratio=1.040
01/14 22:42:33 saving model to models/10_fold_seq2seq/checkpoints
01/14 22:42:34 finished saving model
01/14 22:50:01   decaying learning rate to: 0.00745
01/14 23:00:24 step 202000 epoch 83 learning rate 0.00745 step-time 0.533 loss 0.173
01/14 23:00:24 starting evaluation
01/14 23:03:14 test bleu=32.06 loss=129.07 penalty=1.000 ratio=1.054
01/14 23:03:14 saving model to models/10_fold_seq2seq/checkpoints
01/14 23:03:14 finished saving model
01/14 23:14:47   decaying learning rate to: 0.00708
01/14 23:21:27 step 204000 epoch 84 learning rate 0.00708 step-time 0.544 loss 0.170
01/14 23:21:27 starting evaluation
01/14 23:24:16 test bleu=31.89 loss=129.14 penalty=1.000 ratio=1.056
01/14 23:24:16 saving model to models/10_fold_seq2seq/checkpoints
01/14 23:24:16 finished saving model
01/14 23:39:46   decaying learning rate to: 0.00673
01/14 23:42:11 step 206000 epoch 85 learning rate 0.00673 step-time 0.535 loss 0.171
01/14 23:42:11 starting evaluation
01/14 23:45:01 test bleu=32.43 loss=129.28 penalty=1.000 ratio=1.042
01/14 23:45:01 saving model to models/10_fold_seq2seq/checkpoints
01/14 23:45:01 finished saving model
01/15 00:03:17 step 208000 epoch 85 learning rate 0.00673 step-time 0.545 loss 0.166
01/15 00:03:17 starting evaluation
01/15 00:06:06 test bleu=32.59 loss=129.20 penalty=1.000 ratio=1.040
01/15 00:06:06 saving model to models/10_fold_seq2seq/checkpoints
01/15 00:06:06 finished saving model
01/15 00:07:37   decaying learning rate to: 0.00639
01/15 00:24:12 step 210000 epoch 86 learning rate 0.00639 step-time 0.540 loss 0.159
01/15 00:24:12 starting evaluation
01/15 00:27:03 test bleu=32.11 loss=129.67 penalty=1.000 ratio=1.050
01/15 00:27:03 saving model to models/10_fold_seq2seq/checkpoints
01/15 00:27:03 finished saving model
01/15 00:32:29   decaying learning rate to: 0.00607
01/15 00:45:05 step 212000 epoch 87 learning rate 0.00607 step-time 0.538 loss 0.158
01/15 00:45:05 starting evaluation
01/15 00:47:49 test bleu=32.22 loss=130.05 penalty=1.000 ratio=1.048
01/15 00:47:49 saving model to models/10_fold_seq2seq/checkpoints
01/15 00:47:49 finished saving model
01/15 00:57:30   decaying learning rate to: 0.00577
01/15 01:06:01 step 214000 epoch 88 learning rate 0.00577 step-time 0.543 loss 0.160
01/15 01:06:01 starting evaluation
01/15 01:08:51 test bleu=32.12 loss=130.14 penalty=1.000 ratio=1.051
01/15 01:08:51 saving model to models/10_fold_seq2seq/checkpoints
01/15 01:08:51 finished saving model
01/15 01:22:22   decaying learning rate to: 0.00548
01/15 01:26:39 step 216000 epoch 89 learning rate 0.00548 step-time 0.531 loss 0.157
01/15 01:26:39 starting evaluation
01/15 01:29:29 test bleu=32.28 loss=130.21 penalty=1.000 ratio=1.046
01/15 01:29:29 saving model to models/10_fold_seq2seq/checkpoints
01/15 01:29:29 finished saving model
01/15 01:47:15   decaying learning rate to: 0.0052
01/15 01:47:37 step 218000 epoch 90 learning rate 0.0052 step-time 0.541 loss 0.157
01/15 01:47:37 starting evaluation
01/15 01:50:27 test bleu=32.43 loss=130.25 penalty=1.000 ratio=1.043
01/15 01:50:27 saving model to models/10_fold_seq2seq/checkpoints
01/15 01:50:27 finished saving model
01/15 02:08:15 step 220000 epoch 90 learning rate 0.0052 step-time 0.532 loss 0.150
01/15 02:08:15 starting evaluation
01/15 02:11:05 test bleu=31.98 loss=130.56 penalty=1.000 ratio=1.055
01/15 02:11:05 saving model to models/10_fold_seq2seq/checkpoints
01/15 02:11:05 finished saving model
01/15 02:14:49   decaying learning rate to: 0.00494
01/15 02:29:16 step 222000 epoch 91 learning rate 0.00494 step-time 0.543 loss 0.148
01/15 02:29:16 starting evaluation
01/15 02:32:05 test bleu=32.49 loss=130.65 penalty=1.000 ratio=1.040
01/15 02:32:05 saving model to models/10_fold_seq2seq/checkpoints
01/15 02:32:05 finished saving model
01/15 02:39:44   decaying learning rate to: 0.0047
01/15 02:50:04 step 224000 epoch 92 learning rate 0.0047 step-time 0.537 loss 0.146
01/15 02:50:04 starting evaluation
01/15 02:52:53 test bleu=32.05 loss=130.70 penalty=1.000 ratio=1.052
01/15 02:52:53 saving model to models/10_fold_seq2seq/checkpoints
01/15 02:52:54 finished saving model
01/15 03:04:31   decaying learning rate to: 0.00446
01/15 03:10:50 step 226000 epoch 93 learning rate 0.00446 step-time 0.536 loss 0.147
01/15 03:10:50 starting evaluation
01/15 03:13:38 test bleu=32.24 loss=130.84 penalty=1.000 ratio=1.050
01/15 03:13:38 saving model to models/10_fold_seq2seq/checkpoints
01/15 03:13:38 finished saving model
01/15 03:29:26   decaying learning rate to: 0.00424
01/15 03:31:38 step 228000 epoch 94 learning rate 0.00424 step-time 0.537 loss 0.147
01/15 03:31:38 starting evaluation
01/15 03:34:29 test bleu=32.21 loss=131.23 penalty=1.000 ratio=1.049
01/15 03:34:29 saving model to models/10_fold_seq2seq/checkpoints
01/15 03:34:29 finished saving model
01/15 03:52:17 step 230000 epoch 94 learning rate 0.00424 step-time 0.532 loss 0.144
01/15 03:52:17 starting evaluation
01/15 03:55:09 test bleu=32.17 loss=131.24 penalty=1.000 ratio=1.049
01/15 03:55:09 saving model to models/10_fold_seq2seq/checkpoints
01/15 03:55:09 finished saving model
01/15 03:57:01   decaying learning rate to: 0.00403
01/15 04:13:22 step 232000 epoch 95 learning rate 0.00403 step-time 0.544 loss 0.141
01/15 04:13:22 starting evaluation
01/15 04:16:12 test bleu=32.13 loss=131.26 penalty=1.000 ratio=1.047
01/15 04:16:12 saving model to models/10_fold_seq2seq/checkpoints
01/15 04:16:12 finished saving model
01/15 04:22:10   decaying learning rate to: 0.00383
01/15 04:34:07 step 234000 epoch 96 learning rate 0.00383 step-time 0.535 loss 0.138
01/15 04:34:07 starting evaluation
01/15 04:36:56 test bleu=32.35 loss=131.37 penalty=1.000 ratio=1.045
01/15 04:36:56 saving model to models/10_fold_seq2seq/checkpoints
01/15 04:36:57 finished saving model
01/15 04:46:58   decaying learning rate to: 0.00363
01/15 04:55:10 step 236000 epoch 97 learning rate 0.00363 step-time 0.544 loss 0.139
01/15 04:55:10 starting evaluation
01/15 04:58:01 test bleu=32.14 loss=131.58 penalty=1.000 ratio=1.049
01/15 04:58:01 saving model to models/10_fold_seq2seq/checkpoints
01/15 04:58:01 finished saving model
01/15 05:12:03   decaying learning rate to: 0.00345
01/15 05:16:04 step 238000 epoch 98 learning rate 0.00345 step-time 0.539 loss 0.139
01/15 05:16:04 starting evaluation
01/15 05:19:01 test bleu=32.12 loss=131.65 penalty=1.000 ratio=1.050
01/15 05:19:01 saving model to models/10_fold_seq2seq/checkpoints
01/15 05:19:02 finished saving model
01/15 05:38:32 step 240000 epoch 99 learning rate 0.00345 step-time 0.582 loss 0.139
01/15 05:38:32 starting evaluation
01/15 05:41:37 test bleu=32.19 loss=131.55 penalty=1.000 ratio=1.050
01/15 05:41:37 saving model to models/10_fold_seq2seq/checkpoints
01/15 05:41:38 finished saving model
01/15 05:41:39   decaying learning rate to: 0.00328
01/15 06:01:16 step 242000 epoch 99 learning rate 0.00328 step-time 0.586 loss 0.132
01/15 06:01:16 starting evaluation
01/15 06:04:22 test bleu=32.09 loss=131.80 penalty=1.000 ratio=1.051
01/15 06:04:22 saving model to models/10_fold_seq2seq/checkpoints
01/15 06:04:22 finished saving model
01/15 06:08:49   decaying learning rate to: 0.00312
01/15 06:23:57 step 244000 epoch 100 learning rate 0.00312 step-time 0.584 loss 0.132
01/15 06:23:57 starting evaluation
01/15 06:27:01 test bleu=32.32 loss=131.97 penalty=1.000 ratio=1.047
01/15 06:27:01 saving model to models/10_fold_seq2seq/checkpoints
01/15 06:27:02 finished saving model
01/15 06:35:37 finished training
01/15 06:35:37 exiting...
01/15 06:35:37 saving model to models/10_fold_seq2seq/checkpoints
01/15 06:35:38 finished saving model
