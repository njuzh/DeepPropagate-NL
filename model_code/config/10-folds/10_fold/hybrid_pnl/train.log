nohup: ignoring input
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:107: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:30: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

12/30 03:08:45 label: default
12/30 03:08:45 description:
  default configuration
  next line of description
  last line
12/30 03:08:45 /root/icpc/icpc/translate/__main__.py config/10-folds/10_fold/hybrid_pnl/config.yaml --train -v
12/30 03:08:45 commit hash 74e0554cb3eb5df835cef993ad570ff8de651f71
12/30 03:08:45 tensorflow version: 1.14.0
12/30 03:08:45 program arguments
12/30 03:08:45   aggregation_method   'sum'
12/30 03:08:45   align_encoder_id     0
12/30 03:08:45   allow_growth         True
12/30 03:08:45   attention_type       'global'
12/30 03:08:45   attn_filter_length   0
12/30 03:08:45   attn_filters         0
12/30 03:08:45   attn_prev_word       False
12/30 03:08:45   attn_size            128
12/30 03:08:45   attn_temperature     1.0
12/30 03:08:45   attn_window_size     0
12/30 03:08:45   average              False
12/30 03:08:45   baseline_activation  None
12/30 03:08:45   baseline_learning_rate 0.001
12/30 03:08:45   baseline_optimizer   'adam'
12/30 03:08:45   baseline_steps       0
12/30 03:08:45   batch_mode           'standard'
12/30 03:08:45   batch_size           64
12/30 03:08:45   beam_size            5
12/30 03:08:45   bidir                True
12/30 03:08:45   bidir_projection     False
12/30 03:08:45   binary               False
12/30 03:08:45   cell_size            256
12/30 03:08:45   cell_type            'GRU'
12/30 03:08:45   character_level      False
12/30 03:08:45   checkpoints          []
12/30 03:08:45   conditional_rnn      False
12/30 03:08:45   config               'config/10-folds/10_fold/hybrid_pnl/config.yaml'
12/30 03:08:45   convolutions         None
12/30 03:08:45   data_dir             'data/gooddata/10_fold'
12/30 03:08:45   debug                False
12/30 03:08:45   decay_after_n_epoch  1
12/30 03:08:45   decay_every_n_epoch  1
12/30 03:08:45   decay_if_no_progress None
12/30 03:08:45   decoders             [{'max_len': 40, 'name': 'nl'}]
12/30 03:08:45   description          'default configuration\nnext line of description\nlast line\n'
12/30 03:08:45   dev_prefix           'test'
12/30 03:08:45   early_stopping       True
12/30 03:08:45   embedding_dropout    0.0
12/30 03:08:45   embedding_initializer None
12/30 03:08:45   embedding_size       256
12/30 03:08:45   embedding_weight_scale None
12/30 03:08:45   embeddings_on_cpu    True
12/30 03:08:45   encoders             [{'attention_type': 'global', 'max_len': 200, 'name': 'code'},
 {'attention_type': 'global', 'max_len': 80, 'name': 'pnl'}]
12/30 03:08:45   ensemble             False
12/30 03:08:45   eval_burn_in         0
12/30 03:08:45   feed_previous        0.0
12/30 03:08:45   final_state          'last'
12/30 03:08:45   freeze_variables     []
12/30 03:08:45   generate_first       True
12/30 03:08:45   gpu_id               3
12/30 03:08:45   highway_layers       0
12/30 03:08:45   initial_state_dropout 0.0
12/30 03:08:45   initializer          None
12/30 03:08:45   input_layer_dropout  0.0
12/30 03:08:45   input_layers         None
12/30 03:08:45   keep_best            5
12/30 03:08:45   keep_every_n_hours   0
12/30 03:08:45   label                'default'
12/30 03:08:45   layer_norm           False
12/30 03:08:45   layers               1
12/30 03:08:45   learning_rate        0.5
12/30 03:08:45   learning_rate_decay_factor 0.95
12/30 03:08:45   len_normalization    1.0
12/30 03:08:45   log_file             'log.txt'
12/30 03:08:45   loss_function        'xent'
12/30 03:08:45   max_dev_size         0
12/30 03:08:45   max_epochs           100
12/30 03:08:45   max_gradient_norm    5.0
12/30 03:08:45   max_len              50
12/30 03:08:45   max_steps            600000
12/30 03:08:45   max_test_size        0
12/30 03:08:45   max_to_keep          1
12/30 03:08:45   max_train_size       0
12/30 03:08:45   maxout_stride        None
12/30 03:08:45   mem_fraction         1.0
12/30 03:08:45   min_learning_rate    1e-06
12/30 03:08:45   model_dir            'models/10_fold_hybrid_pnl'
12/30 03:08:45   moving_average       None
12/30 03:08:45   no_gpu               False
12/30 03:08:45   optimizer            'sgd'
12/30 03:08:45   orthogonal_init      False
12/30 03:08:45   output               None
12/30 03:08:45   output_dropout       0.0
12/30 03:08:45   parallel_iterations  16
12/30 03:08:45   pervasive_dropout    False
12/30 03:08:45   pooling_avg          True
12/30 03:08:45   post_process_script  None
12/30 03:08:45   pred_deep_layer      False
12/30 03:08:45   pred_edits           False
12/30 03:08:45   pred_embed_proj      True
12/30 03:08:45   pred_maxout_layer    True
12/30 03:08:45   purge                False
12/30 03:08:45   raw_output           False
12/30 03:08:45   read_ahead           1
12/30 03:08:45   reconstruction_attn_weight 0.05
12/30 03:08:45   reconstruction_decoders False
12/30 03:08:45   reconstruction_weight 1.0
12/30 03:08:45   reinforce_after_n_epoch None
12/30 03:08:45   remove_unk           False
12/30 03:08:45   reverse              False
12/30 03:08:45   reverse_input        True
12/30 03:08:45   reward_function      'sentence_bleu'
12/30 03:08:45   rnn_feed_attn        True
12/30 03:08:45   rnn_input_dropout    0.0
12/30 03:08:45   rnn_output_dropout   0.0
12/30 03:08:45   rnn_state_dropout    0.0
12/30 03:08:45   save                 False
12/30 03:08:45   score_function       'corpus_bleu'
12/30 03:08:45   score_functions      ['bleu', 'loss']
12/30 03:08:45   script_dir           'scripts'
12/30 03:08:45   sgd_after_n_epoch    None
12/30 03:08:45   sgd_learning_rate    1.0
12/30 03:08:45   shuffle              True
12/30 03:08:45   softmax_temperature  1.0
12/30 03:08:45   steps_per_checkpoint 2000
12/30 03:08:45   steps_per_eval       2000
12/30 03:08:45   swap_memory          True
12/30 03:08:45   tie_embeddings       False
12/30 03:08:45   time_pooling         None
12/30 03:08:45   train                True
12/30 03:08:45   train_initial_states True
12/30 03:08:45   train_prefix         'train'
12/30 03:08:45   truncate_lines       True
12/30 03:08:45   update_first         False
12/30 03:08:45   use_baseline         False
12/30 03:08:45   use_dropout          False
12/30 03:08:45   use_lstm_full_state  False
12/30 03:08:45   use_previous_word    True
12/30 03:08:45   verbose              True
12/30 03:08:45   vocab_prefix         'vocab'
12/30 03:08:45   weight_scale         None
12/30 03:08:45   word_dropout         0.0
12/30 03:08:45 python random seed: 635334500941110234
12/30 03:08:45 tf random seed:     1055521197827554740
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:203: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

12/30 03:08:45 creating model
12/30 03:08:45 using device: /gpu:3
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:230: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

12/30 03:08:45 copying vocab to models/10_fold_hybrid_pnl/data/vocab.code
12/30 03:08:45 copying vocab to models/10_fold_hybrid_pnl/data/vocab.pnl
12/30 03:08:45 copying vocab to models/10_fold_hybrid_pnl/data/vocab.nl
12/30 03:08:45 reading vocabularies
12/30 03:08:45 creating model
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:60: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:111: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:33: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f1d6931b828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f1d6931b828>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:226: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f1d6931bfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f1d6931bfd0>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f1df1d9be80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f1df1d9be80>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f1df1d9be48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f1df1d9be48>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1d6931e080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1d6931e080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:838: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df1a53f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df1a53f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df1a3af60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df1a3af60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:432: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:435: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df19976d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df19976d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df19976d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df19976d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df181cda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df181cda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df17d5e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df17d5e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df1501fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df1501fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df14b6ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df14b6ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df1511cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df1511cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df1511cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df1511cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df1511cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1df1511cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:919: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f1df13972e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f1df13972e8>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:131: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:223: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f1d8cfc4b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f1d8cfc4b38>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1d8cf60c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1d8cf60c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1d8cff8ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1d8cff8ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1d8cebe2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1d8cebe2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1d8cec52b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1d8cec52b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1d8cec5278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1d8cec5278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1d8ce0ba58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1d8ce0ba58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1d8ce0ba58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1d8ce0ba58>>: AssertionError: Bad argument number for Name: 3, expecting 4
12/30 03:08:53 model parameters (45)
12/30 03:08:53   baseline_step:0 ()
12/30 03:08:53   decoder_nl/attention_code/U_a/kernel:0 (512, 128)
12/30 03:08:53   decoder_nl/attention_code/W_a/bias:0 (128,)
12/30 03:08:53   decoder_nl/attention_code/W_a/kernel:0 (256, 128)
12/30 03:08:53   decoder_nl/attention_code/v_a:0 (128,)
12/30 03:08:53   decoder_nl/attention_pnl/U_a/kernel:0 (512, 128)
12/30 03:08:53   decoder_nl/attention_pnl/W_a/bias:0 (128,)
12/30 03:08:53   decoder_nl/attention_pnl/W_a/kernel:0 (256, 128)
12/30 03:08:53   decoder_nl/attention_pnl/v_a:0 (128,)
12/30 03:08:53   decoder_nl/code_pnl/initial_state_projection/bias:0 (256,)
12/30 03:08:53   decoder_nl/code_pnl/initial_state_projection/kernel:0 (512, 256)
12/30 03:08:53   decoder_nl/gru_cell/candidate/bias:0 (256,)
12/30 03:08:53   decoder_nl/gru_cell/candidate/kernel:0 (1024, 256)
12/30 03:08:53   decoder_nl/gru_cell/gates/bias:0 (512,)
12/30 03:08:53   decoder_nl/gru_cell/gates/kernel:0 (1024, 512)
12/30 03:08:53   decoder_nl/maxout/bias:0 (256,)
12/30 03:08:53   decoder_nl/maxout/kernel:0 (1024, 256)
12/30 03:08:53   decoder_nl/softmax0/kernel:0 (128, 256)
12/30 03:08:53   decoder_nl/softmax1/bias:0 (37996,)
12/30 03:08:53   decoder_nl/softmax1/kernel:0 (256, 37996)
12/30 03:08:53   embedding_code:0 (50000, 256)
12/30 03:08:53   embedding_nl:0 (37996, 256)
12/30 03:08:53   embedding_pnl:0 (37529, 256)
12/30 03:08:53   encoder_code/initial_state_bw:0 (256,)
12/30 03:08:53   encoder_code/initial_state_fw:0 (256,)
12/30 03:08:53   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/30 03:08:53   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/30 03:08:53   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/30 03:08:53   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/30 03:08:53   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/30 03:08:53   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/30 03:08:53   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/30 03:08:53   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/30 03:08:53   encoder_pnl/initial_state_bw:0 (256,)
12/30 03:08:53   encoder_pnl/initial_state_fw:0 (256,)
12/30 03:08:53   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/30 03:08:53   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/30 03:08:53   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/30 03:08:53   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/30 03:08:53   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/30 03:08:53   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/30 03:08:53   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/30 03:08:53   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/30 03:08:53   global_step:0 ()
12/30 03:08:53   learning_rate:0 ()
12/30 03:08:53 number of parameters: 44.89M
WARNING:tensorflow:From /root/icpc/icpc/translate/translation_model.py:666: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

12/30 03:08:54 global step: 0
12/30 03:08:54 baseline step: 0
12/30 03:08:54 reading training data
12/30 03:08:54 total line count: 156717
12/30 03:09:00   lines read: 100000
12/30 03:09:03 files: data/gooddata/10_fold/train.code data/gooddata/10_fold/train.pnl data/gooddata/10_fold/train.nl
12/30 03:09:03 lines reads: 156717
12/30 03:09:03 reading development data
12/30 03:09:04 files: data/gooddata/10_fold/test.code data/gooddata/10_fold/test.pnl data/gooddata/10_fold/test.nl
12/30 03:09:04 lines reads: 17417
12/30 03:09:04 starting training
12/30 03:36:25 step 2000 epoch 1 learning rate 0.5 step-time 0.819 loss 81.600
12/30 03:36:25 starting evaluation
12/30 03:40:05 test bleu=0.63 loss=66.16 penalty=0.491 ratio=0.584
12/30 03:40:05 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 03:40:05 finished saving model
12/30 03:40:05 new best model
12/30 03:46:14   decaying learning rate to: 0.475
12/30 04:07:31 step 4000 epoch 2 learning rate 0.475 step-time 0.821 loss 60.054
12/30 04:07:31 starting evaluation
12/30 04:12:28 test bleu=5.91 loss=55.13 penalty=0.962 ratio=0.963
12/30 04:12:28 saving model to models/10_fold_hybrid_pnl/checkpoints
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
12/30 04:12:28 finished saving model
12/30 04:12:28 new best model
12/30 04:24:41   decaying learning rate to: 0.451
12/30 04:39:47 step 6000 epoch 3 learning rate 0.451 step-time 0.817 loss 52.254
12/30 04:39:47 starting evaluation
12/30 04:44:46 test bleu=8.83 loss=50.04 penalty=0.813 ratio=0.828
12/30 04:44:46 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 04:44:46 finished saving model
12/30 04:44:46 new best model
12/30 05:03:14   decaying learning rate to: 0.429
12/30 05:12:01 step 8000 epoch 4 learning rate 0.429 step-time 0.815 loss 46.686
12/30 05:12:01 starting evaluation
12/30 05:16:52 test bleu=13.20 loss=45.28 penalty=0.898 ratio=0.903
12/30 05:16:52 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 05:16:52 finished saving model
12/30 05:16:52 new best model
12/30 05:41:29   decaying learning rate to: 0.407
12/30 05:44:15 step 10000 epoch 5 learning rate 0.407 step-time 0.819 loss 42.732
12/30 05:44:15 starting evaluation
12/30 05:49:06 test bleu=14.46 loss=43.16 penalty=0.745 ratio=0.772
12/30 05:49:06 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 05:49:06 finished saving model
12/30 05:49:06 new best model
12/30 06:16:26 step 12000 epoch 5 learning rate 0.407 step-time 0.818 loss 39.136
12/30 06:16:26 starting evaluation
12/30 06:21:08 test bleu=18.27 loss=40.83 penalty=0.791 ratio=0.810
12/30 06:21:08 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 06:21:09 finished saving model
12/30 06:21:09 new best model
12/30 06:24:39   decaying learning rate to: 0.387
12/30 06:48:25 step 14000 epoch 6 learning rate 0.387 step-time 0.816 loss 35.979
12/30 06:48:25 starting evaluation
12/30 06:53:21 test bleu=17.21 loss=39.56 penalty=1.000 ratio=1.194
12/30 06:53:21 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 06:53:21 finished saving model
12/30 07:02:48   decaying learning rate to: 0.368
12/30 07:20:50 step 16000 epoch 7 learning rate 0.368 step-time 0.823 loss 33.538
12/30 07:20:50 starting evaluation
12/30 07:25:43 test bleu=21.75 loss=38.18 penalty=0.809 ratio=0.825
12/30 07:25:43 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 07:25:43 finished saving model
12/30 07:25:43 new best model
12/30 07:43:13   decaying learning rate to: 0.349
12/30 07:56:50 step 18000 epoch 8 learning rate 0.349 step-time 0.931 loss 31.334
12/30 07:56:50 starting evaluation
12/30 08:01:57 test bleu=23.37 loss=37.57 penalty=0.835 ratio=0.847
12/30 08:01:57 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 08:01:57 finished saving model
12/30 08:01:57 new best model
12/30 08:27:27   decaying learning rate to: 0.332
12/30 08:33:58 step 20000 epoch 9 learning rate 0.332 step-time 0.958 loss 29.345
12/30 08:33:58 starting evaluation
12/30 08:39:18 test bleu=25.96 loss=37.13 penalty=0.948 ratio=0.949
12/30 08:39:18 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 08:39:18 finished saving model
12/30 08:39:18 new best model
12/30 09:08:57 step 22000 epoch 9 learning rate 0.332 step-time 0.887 loss 27.459
12/30 09:08:57 starting evaluation
12/30 09:13:53 test bleu=25.85 loss=36.25 penalty=0.881 ratio=0.888
12/30 09:13:53 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 09:13:53 finished saving model
12/30 09:14:28   decaying learning rate to: 0.315
12/30 09:41:14 step 24000 epoch 10 learning rate 0.315 step-time 0.818 loss 24.819
12/30 09:41:14 starting evaluation
12/30 09:45:54 test bleu=27.03 loss=36.33 penalty=0.842 ratio=0.853
12/30 09:45:54 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 09:45:54 finished saving model
12/30 09:45:54 new best model
12/30 09:52:38   decaying learning rate to: 0.299
12/30 10:13:16 step 26000 epoch 11 learning rate 0.299 step-time 0.819 loss 23.046
12/30 10:13:16 starting evaluation
12/30 10:18:13 test bleu=27.61 loss=36.45 penalty=0.807 ratio=0.823
12/30 10:18:13 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 10:18:13 finished saving model
12/30 10:18:13 new best model
12/30 10:30:59   decaying learning rate to: 0.284
12/30 10:45:43 step 28000 epoch 12 learning rate 0.284 step-time 0.823 loss 21.352
12/30 10:45:43 starting evaluation
12/30 10:50:40 test bleu=29.48 loss=37.03 penalty=0.943 ratio=0.944
12/30 10:50:40 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 10:50:41 finished saving model
12/30 10:50:41 new best model
12/30 11:09:41   decaying learning rate to: 0.27
12/30 11:18:08 step 30000 epoch 13 learning rate 0.27 step-time 0.822 loss 19.885
12/30 11:18:08 starting evaluation
12/30 11:23:02 test bleu=29.87 loss=38.15 penalty=0.886 ratio=0.892
12/30 11:23:02 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 11:23:02 finished saving model
12/30 11:23:02 new best model
12/30 11:48:19   decaying learning rate to: 0.257
12/30 11:50:29 step 32000 epoch 14 learning rate 0.257 step-time 0.821 loss 18.364
12/30 11:50:29 starting evaluation
12/30 11:55:21 test bleu=30.76 loss=38.80 penalty=0.873 ratio=0.880
12/30 11:55:21 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 11:55:21 finished saving model
12/30 11:55:21 new best model
12/30 12:22:49 step 34000 epoch 14 learning rate 0.257 step-time 0.822 loss 16.464
12/30 12:22:49 starting evaluation
12/30 12:27:48 test bleu=31.47 loss=38.78 penalty=0.967 ratio=0.968
12/30 12:27:48 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 12:27:49 finished saving model
12/30 12:27:49 new best model
12/30 12:31:45   decaying learning rate to: 0.244
12/30 12:55:19 step 36000 epoch 15 learning rate 0.244 step-time 0.823 loss 14.803
12/30 12:55:19 starting evaluation
12/30 13:00:15 test bleu=31.62 loss=40.23 penalty=0.905 ratio=0.909
12/30 13:00:15 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 13:00:16 finished saving model
12/30 13:00:16 new best model
12/30 13:10:30   decaying learning rate to: 0.232
12/30 13:27:41 step 38000 epoch 16 learning rate 0.232 step-time 0.821 loss 13.608
12/30 13:27:41 starting evaluation
12/30 13:32:26 test bleu=32.23 loss=41.73 penalty=0.918 ratio=0.922
12/30 13:32:26 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 13:32:26 finished saving model
12/30 13:32:26 new best model
12/30 13:48:35   decaying learning rate to: 0.22
12/30 13:59:44 step 40000 epoch 17 learning rate 0.22 step-time 0.817 loss 12.449
12/30 13:59:44 starting evaluation
12/30 14:04:44 test bleu=32.36 loss=43.86 penalty=0.923 ratio=0.926
12/30 14:04:44 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 14:04:44 finished saving model
12/30 14:04:44 new best model
12/30 14:27:03   decaying learning rate to: 0.209
12/30 14:32:09 step 42000 epoch 18 learning rate 0.209 step-time 0.820 loss 11.401
12/30 14:32:09 starting evaluation
12/30 14:37:06 test bleu=33.13 loss=45.40 penalty=0.955 ratio=0.956
12/30 14:37:06 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 14:37:06 finished saving model
12/30 14:37:06 new best model
12/30 15:04:25 step 44000 epoch 18 learning rate 0.209 step-time 0.817 loss 10.291
12/30 15:04:25 starting evaluation
12/30 15:09:19 test bleu=33.55 loss=44.95 penalty=0.918 ratio=0.921
12/30 15:09:19 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 15:09:20 finished saving model
12/30 15:09:20 new best model
12/30 15:10:27   decaying learning rate to: 0.199
12/30 15:36:40 step 46000 epoch 19 learning rate 0.199 step-time 0.818 loss 8.830
12/30 15:36:40 starting evaluation
12/30 15:41:37 test bleu=33.78 loss=47.36 penalty=0.955 ratio=0.956
12/30 15:41:37 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 15:41:37 finished saving model
12/30 15:41:37 new best model
12/30 15:48:56   decaying learning rate to: 0.189
12/30 16:09:00 step 48000 epoch 20 learning rate 0.189 step-time 0.819 loss 7.997
12/30 16:09:00 starting evaluation
12/30 16:14:00 test bleu=33.39 loss=50.42 penalty=1.000 ratio=1.006
12/30 16:14:00 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 16:14:00 finished saving model
12/30 16:27:36   decaying learning rate to: 0.179
12/30 16:41:26 step 50000 epoch 21 learning rate 0.179 step-time 0.821 loss 7.245
12/30 16:41:26 starting evaluation
12/30 16:46:26 test bleu=34.08 loss=53.17 penalty=1.000 ratio=1.006
12/30 16:46:26 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 16:46:26 finished saving model
12/30 16:46:26 new best model
12/30 17:05:57   decaying learning rate to: 0.17
12/30 17:13:43 step 52000 epoch 22 learning rate 0.17 step-time 0.816 loss 6.586
12/30 17:13:43 starting evaluation
12/30 17:18:24 test bleu=34.56 loss=54.61 penalty=0.931 ratio=0.934
12/30 17:18:24 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 17:18:25 finished saving model
12/30 17:18:25 new best model
12/30 17:44:04   decaying learning rate to: 0.162
12/30 17:45:43 step 54000 epoch 23 learning rate 0.162 step-time 0.817 loss 5.957
12/30 17:45:43 starting evaluation
12/30 17:50:44 test bleu=34.69 loss=56.36 penalty=0.997 ratio=0.997
12/30 17:50:44 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 17:50:44 finished saving model
12/30 17:50:44 new best model
12/30 18:18:08 step 56000 epoch 23 learning rate 0.162 step-time 0.820 loss 5.050
12/30 18:18:08 starting evaluation
12/30 18:23:04 test bleu=34.72 loss=57.85 penalty=0.940 ratio=0.941
12/30 18:23:04 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 18:23:04 finished saving model
12/30 18:23:04 new best model
12/30 18:27:33   decaying learning rate to: 0.154
12/30 18:50:22 step 58000 epoch 24 learning rate 0.154 step-time 0.817 loss 4.464
12/30 18:50:22 starting evaluation
12/30 18:55:21 test bleu=34.62 loss=61.01 penalty=0.992 ratio=0.992
12/30 18:55:21 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 18:55:21 finished saving model
12/30 19:05:54   decaying learning rate to: 0.146
12/30 19:22:43 step 60000 epoch 25 learning rate 0.146 step-time 0.819 loss 4.064
12/30 19:22:43 starting evaluation
12/30 19:27:40 test bleu=35.36 loss=62.60 penalty=0.965 ratio=0.966
12/30 19:27:40 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 19:27:40 finished saving model
12/30 19:27:40 new best model
12/30 19:44:35   decaying learning rate to: 0.139
12/30 19:55:02 step 62000 epoch 26 learning rate 0.139 step-time 0.819 loss 3.644
12/30 19:55:02 starting evaluation
12/30 20:00:00 test bleu=35.22 loss=65.83 penalty=0.999 ratio=0.999
12/30 20:00:00 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 20:00:01 finished saving model
12/30 20:23:00   decaying learning rate to: 0.132
12/30 20:27:22 step 64000 epoch 27 learning rate 0.132 step-time 0.819 loss 3.301
12/30 20:27:22 starting evaluation
12/30 20:32:22 test bleu=35.21 loss=68.10 penalty=1.000 ratio=1.004
12/30 20:32:22 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 20:32:22 finished saving model
12/30 20:59:45 step 66000 epoch 27 learning rate 0.132 step-time 0.819 loss 2.914
12/30 20:59:45 starting evaluation
12/30 21:04:33 test bleu=35.58 loss=69.24 penalty=1.000 ratio=1.000
12/30 21:04:33 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 21:04:33 finished saving model
12/30 21:04:33 new best model
12/30 21:06:22   decaying learning rate to: 0.125
12/30 21:31:59 step 68000 epoch 28 learning rate 0.125 step-time 0.821 loss 2.460
12/30 21:31:59 starting evaluation
12/30 21:36:54 test bleu=35.24 loss=72.47 penalty=0.959 ratio=0.960
12/30 21:36:54 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 21:36:54 finished saving model
12/30 21:44:47   decaying learning rate to: 0.119
12/30 22:04:17 step 70000 epoch 29 learning rate 0.119 step-time 0.819 loss 2.235
12/30 22:04:17 starting evaluation
12/30 22:09:15 test bleu=35.52 loss=74.30 penalty=0.991 ratio=0.991
12/30 22:09:15 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 22:09:15 finished saving model
12/30 22:23:07   decaying learning rate to: 0.113
12/30 22:36:34 step 72000 epoch 30 learning rate 0.113 step-time 0.817 loss 2.030
12/30 22:36:34 starting evaluation
12/30 22:41:31 test bleu=35.92 loss=76.61 penalty=0.988 ratio=0.988
12/30 22:41:31 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 22:41:31 finished saving model
12/30 22:41:31 new best model
12/30 23:01:43   decaying learning rate to: 0.107
12/30 23:08:52 step 74000 epoch 31 learning rate 0.107 step-time 0.818 loss 1.856
12/30 23:08:52 starting evaluation
12/30 23:13:51 test bleu=35.59 loss=77.81 penalty=1.000 ratio=1.017
12/30 23:13:51 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 23:13:51 finished saving model
12/30 23:40:11   decaying learning rate to: 0.102
12/30 23:41:16 step 76000 epoch 32 learning rate 0.102 step-time 0.820 loss 1.687
12/30 23:41:16 starting evaluation
12/30 23:46:12 test bleu=36.00 loss=80.47 penalty=1.000 ratio=1.004
12/30 23:46:12 saving model to models/10_fold_hybrid_pnl/checkpoints
12/30 23:46:13 finished saving model
12/30 23:46:13 new best model
12/31 00:13:25 step 78000 epoch 32 learning rate 0.102 step-time 0.814 loss 1.424
12/31 00:13:25 starting evaluation
12/31 00:18:19 test bleu=35.32 loss=81.96 penalty=1.000 ratio=1.029
12/31 00:18:19 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 00:18:20 finished saving model
12/31 00:23:20   decaying learning rate to: 0.0969
12/31 00:44:56 step 80000 epoch 33 learning rate 0.0969 step-time 0.796 loss 1.286
12/31 00:44:56 starting evaluation
12/31 00:49:47 test bleu=35.20 loss=84.99 penalty=1.000 ratio=1.034
12/31 00:49:47 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 00:49:48 finished saving model
12/31 01:00:54   decaying learning rate to: 0.092
12/31 01:16:28 step 82000 epoch 34 learning rate 0.092 step-time 0.798 loss 1.188
12/31 01:16:28 starting evaluation
12/31 01:21:17 test bleu=35.95 loss=86.32 penalty=1.000 ratio=1.019
12/31 01:21:17 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 01:21:17 finished saving model
12/31 01:38:04   decaying learning rate to: 0.0874
12/31 01:47:46 step 84000 epoch 35 learning rate 0.0874 step-time 0.792 loss 1.100
12/31 01:47:46 starting evaluation
12/31 01:52:50 test bleu=36.26 loss=87.67 penalty=1.000 ratio=1.014
12/31 01:52:50 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 01:52:50 finished saving model
12/31 01:52:50 new best model
12/31 02:15:33   decaying learning rate to: 0.083
12/31 02:19:33 step 86000 epoch 36 learning rate 0.083 step-time 0.799 loss 1.010
12/31 02:19:33 starting evaluation
12/31 02:24:27 test bleu=36.46 loss=89.53 penalty=0.983 ratio=0.983
12/31 02:24:27 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 02:24:27 finished saving model
12/31 02:24:27 new best model
12/31 02:51:11 step 88000 epoch 36 learning rate 0.083 step-time 0.800 loss 0.913
12/31 02:51:11 starting evaluation
12/31 02:56:06 test bleu=35.65 loss=89.93 penalty=1.000 ratio=1.027
12/31 02:56:06 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 02:56:06 finished saving model
12/31 02:58:15   decaying learning rate to: 0.0789
12/31 03:22:46 step 90000 epoch 37 learning rate 0.0789 step-time 0.798 loss 0.808
12/31 03:22:46 starting evaluation
12/31 03:27:41 test bleu=35.93 loss=91.85 penalty=1.000 ratio=1.026
12/31 03:27:41 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 03:27:41 finished saving model
12/31 03:35:47   decaying learning rate to: 0.0749
12/31 03:54:28 step 92000 epoch 38 learning rate 0.0749 step-time 0.801 loss 0.760
12/31 03:54:28 starting evaluation
12/31 03:59:24 test bleu=36.54 loss=93.68 penalty=1.000 ratio=1.009
12/31 03:59:24 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 03:59:24 finished saving model
12/31 03:59:24 new best model
12/31 04:13:45   decaying learning rate to: 0.0712
12/31 04:26:11 step 94000 epoch 39 learning rate 0.0712 step-time 0.801 loss 0.703
12/31 04:26:11 starting evaluation
12/31 04:31:07 test bleu=35.63 loss=94.61 penalty=1.000 ratio=1.037
12/31 04:31:07 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 04:31:07 finished saving model
12/31 04:51:26   decaying learning rate to: 0.0676
12/31 04:57:47 step 96000 epoch 40 learning rate 0.0676 step-time 0.798 loss 0.667
12/31 04:57:47 starting evaluation
12/31 05:02:40 test bleu=36.18 loss=95.14 penalty=1.000 ratio=1.018
12/31 05:02:40 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 05:02:40 finished saving model
12/31 05:28:51   decaying learning rate to: 0.0643
12/31 05:29:24 step 98000 epoch 41 learning rate 0.0643 step-time 0.800 loss 0.629
12/31 05:29:24 starting evaluation
12/31 05:34:12 test bleu=36.48 loss=96.43 penalty=1.000 ratio=1.012
12/31 05:34:12 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 05:34:12 finished saving model
12/31 06:00:51 step 100000 epoch 41 learning rate 0.0643 step-time 0.797 loss 0.557
12/31 06:00:51 starting evaluation
12/31 06:05:56 test bleu=36.87 loss=96.92 penalty=1.000 ratio=1.010
12/31 06:05:56 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 06:05:56 finished saving model
12/31 06:05:56 new best model
12/31 06:11:24   decaying learning rate to: 0.061
12/31 06:32:47 step 102000 epoch 42 learning rate 0.061 step-time 0.803 loss 0.518
12/31 06:32:47 starting evaluation
12/31 06:37:45 test bleu=36.10 loss=98.10 penalty=1.000 ratio=1.028
12/31 06:37:45 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 06:37:45 finished saving model
12/31 06:49:13   decaying learning rate to: 0.058
12/31 07:04:37 step 104000 epoch 43 learning rate 0.058 step-time 0.804 loss 0.494
12/31 07:04:37 starting evaluation
12/31 07:09:30 test bleu=37.11 loss=99.06 penalty=1.000 ratio=1.002
12/31 07:09:30 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 07:09:30 finished saving model
12/31 07:09:30 new best model
12/31 07:27:11   decaying learning rate to: 0.0551
12/31 07:36:23 step 106000 epoch 44 learning rate 0.0551 step-time 0.804 loss 0.476
12/31 07:36:23 starting evaluation
12/31 07:41:17 test bleu=37.19 loss=99.78 penalty=0.995 ratio=0.995
12/31 07:41:17 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 07:41:17 finished saving model
12/31 07:41:17 new best model
12/31 08:04:48   decaying learning rate to: 0.0523
12/31 08:08:05 step 108000 epoch 45 learning rate 0.0523 step-time 0.802 loss 0.458
12/31 08:08:05 starting evaluation
12/31 08:12:59 test bleu=36.76 loss=100.05 penalty=1.000 ratio=1.012
12/31 08:12:59 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 08:12:59 finished saving model
12/31 08:39:49 step 110000 epoch 45 learning rate 0.0523 step-time 0.803 loss 0.425
12/31 08:39:49 starting evaluation
12/31 08:44:43 test bleu=37.11 loss=100.18 penalty=1.000 ratio=1.002
12/31 08:44:43 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 08:44:43 finished saving model
12/31 08:47:24   decaying learning rate to: 0.0497
12/31 09:11:31 step 112000 epoch 46 learning rate 0.0497 step-time 0.802 loss 0.386
12/31 09:11:31 starting evaluation
12/31 09:16:18 test bleu=36.93 loss=101.28 penalty=1.000 ratio=1.010
12/31 09:16:18 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 09:16:19 finished saving model
12/31 09:25:18   decaying learning rate to: 0.0472
12/31 09:43:05 step 114000 epoch 47 learning rate 0.0472 step-time 0.801 loss 0.365
12/31 09:43:05 starting evaluation
12/31 09:47:55 test bleu=36.60 loss=100.91 penalty=1.000 ratio=1.023
12/31 09:47:55 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 09:47:55 finished saving model
12/31 10:02:39   decaying learning rate to: 0.0449
12/31 10:14:29 step 116000 epoch 48 learning rate 0.0449 step-time 0.795 loss 0.353
12/31 10:14:29 starting evaluation
12/31 10:19:34 test bleu=36.99 loss=101.71 penalty=1.000 ratio=1.015
12/31 10:19:34 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 10:19:34 finished saving model
12/31 10:40:13   decaying learning rate to: 0.0426
12/31 10:46:21 step 118000 epoch 49 learning rate 0.0426 step-time 0.802 loss 0.339
12/31 10:46:21 starting evaluation
12/31 10:51:18 test bleu=36.36 loss=102.24 penalty=1.000 ratio=1.031
12/31 10:51:18 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 10:51:18 finished saving model
12/31 11:18:07 step 120000 epoch 50 learning rate 0.0426 step-time 0.803 loss 0.324
12/31 11:18:07 starting evaluation
12/31 11:22:59 test bleu=36.85 loss=101.17 penalty=1.000 ratio=1.017
12/31 11:22:59 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 11:22:59 finished saving model
12/31 11:23:00   decaying learning rate to: 0.0405
12/31 11:49:46 step 122000 epoch 50 learning rate 0.0405 step-time 0.802 loss 0.290
12/31 11:49:46 starting evaluation
12/31 11:54:41 test bleu=37.06 loss=101.96 penalty=1.000 ratio=1.018
12/31 11:54:41 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 11:54:41 finished saving model
12/31 12:00:43   decaying learning rate to: 0.0385
12/31 12:21:30 step 124000 epoch 51 learning rate 0.0385 step-time 0.802 loss 0.274
12/31 12:21:30 starting evaluation
12/31 12:26:26 test bleu=36.76 loss=102.23 penalty=1.000 ratio=1.023
12/31 12:26:26 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 12:26:27 finished saving model
12/31 12:38:39   decaying learning rate to: 0.0365
12/31 12:53:15 step 126000 epoch 52 learning rate 0.0365 step-time 0.802 loss 0.263
12/31 12:53:15 starting evaluation
12/31 12:58:09 test bleu=37.04 loss=101.62 penalty=1.000 ratio=1.016
12/31 12:58:09 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 12:58:09 finished saving model
12/31 13:16:23   decaying learning rate to: 0.0347
12/31 13:25:03 step 128000 epoch 53 learning rate 0.0347 step-time 0.805 loss 0.247
12/31 13:25:03 starting evaluation
12/31 13:29:53 test bleu=37.61 loss=101.77 penalty=1.000 ratio=1.005
12/31 13:29:53 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 13:29:53 finished saving model
12/31 13:29:53 new best model
12/31 13:53:56   decaying learning rate to: 0.033
12/31 13:56:35 step 130000 epoch 54 learning rate 0.033 step-time 0.799 loss 0.245
12/31 13:56:35 starting evaluation
12/31 14:01:18 test bleu=37.76 loss=102.28 penalty=0.987 ratio=0.987
12/31 14:01:18 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 14:01:18 finished saving model
12/31 14:01:18 new best model
12/31 14:27:22 step 132000 epoch 54 learning rate 0.033 step-time 0.780 loss 0.220
12/31 14:27:22 starting evaluation
12/31 14:32:19 test bleu=37.50 loss=101.84 penalty=1.000 ratio=1.008
12/31 14:32:19 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 14:32:19 finished saving model
12/31 14:35:31   decaying learning rate to: 0.0313
12/31 14:58:30 step 134000 epoch 55 learning rate 0.0313 step-time 0.783 loss 0.205
12/31 14:58:30 starting evaluation
12/31 15:03:22 test bleu=37.20 loss=102.23 penalty=1.000 ratio=1.018
12/31 15:03:22 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 15:03:22 finished saving model
12/31 15:12:23   decaying learning rate to: 0.0298
12/31 15:29:34 step 136000 epoch 56 learning rate 0.0298 step-time 0.784 loss 0.200
12/31 15:29:34 starting evaluation
12/31 15:34:26 test bleu=37.75 loss=102.16 penalty=1.000 ratio=1.004
12/31 15:34:26 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 15:34:27 finished saving model
12/31 15:49:20   decaying learning rate to: 0.0283
12/31 16:00:44 step 138000 epoch 57 learning rate 0.0283 step-time 0.787 loss 0.192
12/31 16:00:44 starting evaluation
12/31 16:05:36 test bleu=37.82 loss=102.06 penalty=1.000 ratio=1.003
12/31 16:05:36 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 16:05:36 finished saving model
12/31 16:05:36 new best model
12/31 16:26:35   decaying learning rate to: 0.0269
12/31 16:31:52 step 140000 epoch 58 learning rate 0.0269 step-time 0.786 loss 0.185
12/31 16:31:52 starting evaluation
12/31 16:36:43 test bleu=37.44 loss=102.32 penalty=1.000 ratio=1.010
12/31 16:36:43 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 16:36:43 finished saving model
12/31 17:02:59 step 142000 epoch 58 learning rate 0.0269 step-time 0.786 loss 0.183
12/31 17:02:59 starting evaluation
12/31 17:07:50 test bleu=37.59 loss=102.73 penalty=1.000 ratio=1.006
12/31 17:07:50 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 17:07:51 finished saving model
12/31 17:08:24   decaying learning rate to: 0.0255
12/31 17:34:02 step 144000 epoch 59 learning rate 0.0255 step-time 0.784 loss 0.160
12/31 17:34:02 starting evaluation
12/31 17:38:54 test bleu=37.88 loss=102.77 penalty=0.997 ratio=0.997
12/31 17:38:54 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 17:38:54 finished saving model
12/31 17:38:54 new best model
12/31 17:45:31   decaying learning rate to: 0.0242
12/31 18:05:09 step 146000 epoch 60 learning rate 0.0242 step-time 0.785 loss 0.157
12/31 18:05:09 starting evaluation
12/31 18:09:57 test bleu=37.54 loss=102.50 penalty=1.000 ratio=1.009
12/31 18:09:57 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 18:09:57 finished saving model
12/31 18:22:28   decaying learning rate to: 0.023
12/31 18:35:03 step 148000 epoch 61 learning rate 0.023 step-time 0.751 loss 0.153
12/31 18:35:03 starting evaluation
12/31 18:38:30 test bleu=37.88 loss=102.76 penalty=0.997 ratio=0.997
12/31 18:38:30 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 18:38:30 finished saving model
12/31 18:38:30 new best model
12/31 18:54:44   decaying learning rate to: 0.0219
12/31 19:01:51 step 150000 epoch 62 learning rate 0.0219 step-time 0.698 loss 0.149
12/31 19:01:51 starting evaluation
12/31 19:05:18 test bleu=37.59 loss=102.78 penalty=1.000 ratio=1.010
12/31 19:05:18 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 19:05:18 finished saving model
12/31 19:26:53   decaying learning rate to: 0.0208
12/31 19:28:46 step 152000 epoch 63 learning rate 0.0208 step-time 0.702 loss 0.146
12/31 19:28:46 starting evaluation
12/31 19:32:15 test bleu=37.36 loss=103.07 penalty=1.000 ratio=1.016
12/31 19:32:15 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 19:32:15 finished saving model
12/31 19:55:30 step 154000 epoch 63 learning rate 0.0208 step-time 0.696 loss 0.136
12/31 19:55:30 starting evaluation
12/31 19:58:58 test bleu=37.94 loss=102.87 penalty=1.000 ratio=1.002
12/31 19:58:58 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 19:58:58 finished saving model
12/31 19:58:58 new best model
12/31 20:02:20   decaying learning rate to: 0.0197
12/31 20:22:24 step 156000 epoch 64 learning rate 0.0197 step-time 0.701 loss 0.130
12/31 20:22:24 starting evaluation
12/31 20:25:53 test bleu=37.45 loss=103.11 penalty=1.000 ratio=1.010
12/31 20:25:53 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 20:25:53 finished saving model
12/31 20:34:46   decaying learning rate to: 0.0188
12/31 20:49:49 step 158000 epoch 65 learning rate 0.0188 step-time 0.716 loss 0.126
12/31 20:49:49 starting evaluation
12/31 20:53:19 test bleu=37.96 loss=103.52 penalty=0.998 ratio=0.998
12/31 20:53:19 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 20:53:20 finished saving model
12/31 20:53:20 new best model
12/31 21:07:28   decaying learning rate to: 0.0178
12/31 21:17:10 step 160000 epoch 66 learning rate 0.0178 step-time 0.713 loss 0.124
12/31 21:17:10 starting evaluation
12/31 21:20:42 test bleu=37.22 loss=103.56 penalty=1.000 ratio=1.020
12/31 21:20:42 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 21:20:42 finished saving model
12/31 21:41:52   decaying learning rate to: 0.0169
12/31 21:46:33 step 162000 epoch 67 learning rate 0.0169 step-time 0.772 loss 0.123
12/31 21:46:33 starting evaluation
12/31 21:50:28 test bleu=37.96 loss=103.46 penalty=0.999 ratio=0.999
12/31 21:50:28 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 21:50:28 finished saving model
12/31 21:50:28 new best model
12/31 22:16:38 step 164000 epoch 67 learning rate 0.0169 step-time 0.782 loss 0.120
12/31 22:16:38 starting evaluation
12/31 22:20:32 test bleu=37.19 loss=103.52 penalty=1.000 ratio=1.017
12/31 22:20:32 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 22:20:32 finished saving model
12/31 22:21:38   decaying learning rate to: 0.0161
12/31 22:46:33 step 166000 epoch 68 learning rate 0.0161 step-time 0.778 loss 0.110
12/31 22:46:33 starting evaluation
12/31 22:50:28 test bleu=37.83 loss=103.80 penalty=1.000 ratio=1.003
12/31 22:50:28 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 22:50:28 finished saving model
12/31 22:57:29   decaying learning rate to: 0.0153
12/31 23:16:35 step 168000 epoch 69 learning rate 0.0153 step-time 0.780 loss 0.109
12/31 23:16:35 starting evaluation
12/31 23:20:27 test bleu=37.63 loss=103.93 penalty=1.000 ratio=1.008
12/31 23:20:27 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 23:20:27 finished saving model
12/31 23:33:37   decaying learning rate to: 0.0145
12/31 23:47:53 step 170000 epoch 70 learning rate 0.0145 step-time 0.820 loss 0.107
12/31 23:47:53 starting evaluation
12/31 23:52:16 test bleu=37.81 loss=103.92 penalty=1.000 ratio=1.005
12/31 23:52:16 saving model to models/10_fold_hybrid_pnl/checkpoints
12/31 23:52:16 finished saving model
01/01 00:12:31   decaying learning rate to: 0.0138
01/01 00:20:25 step 172000 epoch 71 learning rate 0.0138 step-time 0.841 loss 0.105
01/01 00:20:25 starting evaluation
01/01 00:24:51 test bleu=37.86 loss=104.43 penalty=1.000 ratio=1.003
01/01 00:24:51 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 00:24:51 finished saving model
01/01 00:51:23   decaying learning rate to: 0.0131
01/01 00:53:07 step 174000 epoch 72 learning rate 0.0131 step-time 0.844 loss 0.106
01/01 00:53:07 starting evaluation
01/01 00:57:32 test bleu=37.80 loss=104.41 penalty=1.000 ratio=1.005
01/01 00:57:32 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 00:57:33 finished saving model
01/01 01:25:50 step 176000 epoch 72 learning rate 0.0131 step-time 0.846 loss 0.100
01/01 01:25:50 starting evaluation
01/01 01:30:16 test bleu=37.57 loss=104.23 penalty=1.000 ratio=1.009
01/01 01:30:16 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 01:30:16 finished saving model
01/01 01:34:51   decaying learning rate to: 0.0124
01/01 01:58:23 step 178000 epoch 73 learning rate 0.0124 step-time 0.840 loss 0.094
01/01 01:58:23 starting evaluation
01/01 02:02:49 test bleu=37.43 loss=104.50 penalty=1.000 ratio=1.015
01/01 02:02:49 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 02:02:50 finished saving model
01/01 02:13:48   decaying learning rate to: 0.0118
01/01 02:30:59 step 180000 epoch 74 learning rate 0.0118 step-time 0.842 loss 0.093
01/01 02:30:59 starting evaluation
01/01 02:35:25 test bleu=37.57 loss=104.99 penalty=1.000 ratio=1.008
01/01 02:35:25 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 02:35:25 finished saving model
01/01 02:52:35   decaying learning rate to: 0.0112
01/01 03:03:41 step 182000 epoch 75 learning rate 0.0112 step-time 0.845 loss 0.096
01/01 03:03:41 starting evaluation
01/01 03:07:59 test bleu=37.42 loss=104.80 penalty=1.000 ratio=1.013
01/01 03:07:59 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 03:07:59 finished saving model
01/01 03:31:54   decaying learning rate to: 0.0107
01/01 03:36:34 step 184000 epoch 76 learning rate 0.0107 step-time 0.854 loss 0.092
01/01 03:36:34 starting evaluation
01/01 03:40:52 test bleu=37.33 loss=105.27 penalty=1.000 ratio=1.018
01/01 03:40:52 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 03:40:52 finished saving model
01/01 04:09:33 step 186000 epoch 76 learning rate 0.0107 step-time 0.857 loss 0.091
01/01 04:09:33 starting evaluation
01/01 04:13:54 test bleu=37.63 loss=104.95 penalty=1.000 ratio=1.009
01/01 04:13:54 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 04:13:54 finished saving model
01/01 04:15:41   decaying learning rate to: 0.0101
01/01 04:42:33 step 188000 epoch 77 learning rate 0.0101 step-time 0.856 loss 0.085
01/01 04:42:33 starting evaluation
01/01 04:46:53 test bleu=37.75 loss=105.16 penalty=1.000 ratio=1.008
01/01 04:46:53 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 04:46:53 finished saving model
01/01 04:55:07   decaying learning rate to: 0.00963
01/01 05:15:35 step 190000 epoch 78 learning rate 0.00963 step-time 0.857 loss 0.083
01/01 05:15:35 starting evaluation
01/01 05:19:55 test bleu=37.70 loss=105.43 penalty=1.000 ratio=1.008
01/01 05:19:55 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 05:19:55 finished saving model
01/01 05:34:37   decaying learning rate to: 0.00915
01/01 05:48:40 step 192000 epoch 79 learning rate 0.00915 step-time 0.859 loss 0.084
01/01 05:48:40 starting evaluation
01/01 05:53:01 test bleu=37.32 loss=105.53 penalty=1.000 ratio=1.017
01/01 05:53:01 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 05:53:01 finished saving model
01/01 06:14:10   decaying learning rate to: 0.00869
01/01 06:21:46 step 194000 epoch 80 learning rate 0.00869 step-time 0.859 loss 0.083
01/01 06:21:46 starting evaluation
01/01 06:26:06 test bleu=37.53 loss=105.70 penalty=1.000 ratio=1.010
01/01 06:26:06 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 06:26:06 finished saving model
01/01 06:53:11   decaying learning rate to: 0.00826
01/01 06:54:19 step 196000 epoch 81 learning rate 0.00826 step-time 0.843 loss 0.083
01/01 06:54:19 starting evaluation
01/01 06:58:34 test bleu=37.55 loss=105.89 penalty=1.000 ratio=1.010
01/01 06:58:34 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 06:58:34 finished saving model
01/01 07:26:52 step 198000 epoch 81 learning rate 0.00826 step-time 0.845 loss 0.078
01/01 07:26:52 starting evaluation
01/01 07:31:09 test bleu=37.53 loss=105.61 penalty=1.000 ratio=1.012
01/01 07:31:09 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 07:31:09 finished saving model
01/01 07:36:21   decaying learning rate to: 0.00784
01/01 07:59:21 step 200000 epoch 82 learning rate 0.00784 step-time 0.843 loss 0.079
01/01 07:59:21 starting evaluation
01/01 08:03:37 test bleu=37.60 loss=105.99 penalty=1.000 ratio=1.010
01/01 08:03:37 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 08:03:38 finished saving model
01/01 08:14:59   decaying learning rate to: 0.00745
01/01 08:31:47 step 202000 epoch 83 learning rate 0.00745 step-time 0.841 loss 0.074
01/01 08:31:47 starting evaluation
01/01 08:36:03 test bleu=37.39 loss=106.09 penalty=1.000 ratio=1.014
01/01 08:36:03 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 08:36:03 finished saving model
01/01 08:53:50   decaying learning rate to: 0.00708
01/01 09:04:12 step 204000 epoch 84 learning rate 0.00708 step-time 0.841 loss 0.077
01/01 09:04:12 starting evaluation
01/01 09:08:30 test bleu=37.32 loss=106.23 penalty=1.000 ratio=1.016
01/01 09:08:30 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 09:08:30 finished saving model
01/01 09:32:54   decaying learning rate to: 0.00673
01/01 09:36:56 step 206000 epoch 85 learning rate 0.00673 step-time 0.850 loss 0.077
01/01 09:36:56 starting evaluation
01/01 09:41:11 test bleu=37.86 loss=106.41 penalty=1.000 ratio=1.004
01/01 09:41:11 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 09:41:11 finished saving model
01/01 10:09:24 step 208000 epoch 85 learning rate 0.00673 step-time 0.843 loss 0.075
01/01 10:09:24 starting evaluation
01/01 10:13:41 test bleu=37.03 loss=106.23 penalty=1.000 ratio=1.025
01/01 10:13:41 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 10:13:42 finished saving model
01/01 10:16:03   decaying learning rate to: 0.00639
01/01 10:41:56 step 210000 epoch 86 learning rate 0.00639 step-time 0.844 loss 0.071
01/01 10:41:56 starting evaluation
01/01 10:46:10 test bleu=37.85 loss=106.58 penalty=1.000 ratio=1.004
01/01 10:46:10 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 10:46:10 finished saving model
01/01 10:54:54   decaying learning rate to: 0.00607
01/01 11:14:14 step 212000 epoch 87 learning rate 0.00607 step-time 0.838 loss 0.073
01/01 11:14:14 starting evaluation
01/01 11:18:29 test bleu=37.59 loss=106.67 penalty=1.000 ratio=1.011
01/01 11:18:29 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 11:18:29 finished saving model
01/01 11:33:32   decaying learning rate to: 0.00577
01/01 11:46:48 step 214000 epoch 88 learning rate 0.00577 step-time 0.846 loss 0.071
01/01 11:46:48 starting evaluation
01/01 11:51:07 test bleu=37.54 loss=106.75 penalty=1.000 ratio=1.011
01/01 11:51:07 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 11:51:07 finished saving model
01/01 12:12:54   decaying learning rate to: 0.00548
01/01 12:20:01 step 216000 epoch 89 learning rate 0.00548 step-time 0.863 loss 0.071
01/01 12:20:01 starting evaluation
01/01 12:24:25 test bleu=37.35 loss=106.88 penalty=1.000 ratio=1.015
01/01 12:24:25 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 12:24:25 finished saving model
01/01 12:52:57   decaying learning rate to: 0.0052
01/01 12:53:31 step 218000 epoch 90 learning rate 0.0052 step-time 0.870 loss 0.071
01/01 12:53:31 starting evaluation
01/01 12:57:55 test bleu=37.70 loss=106.91 penalty=1.000 ratio=1.008
01/01 12:57:55 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 12:57:56 finished saving model
01/01 13:27:02 step 220000 epoch 90 learning rate 0.0052 step-time 0.870 loss 0.067
01/01 13:27:02 starting evaluation
01/01 13:31:26 test bleu=37.71 loss=106.97 penalty=1.000 ratio=1.007
01/01 13:31:26 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 13:31:26 finished saving model
01/01 13:37:24   decaying learning rate to: 0.00494
01/01 14:00:34 step 222000 epoch 91 learning rate 0.00494 step-time 0.870 loss 0.067
01/01 14:00:34 starting evaluation
01/01 14:04:58 test bleu=37.74 loss=107.14 penalty=1.000 ratio=1.007
01/01 14:04:58 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 14:04:59 finished saving model
01/01 14:17:27   decaying learning rate to: 0.0047
01/01 14:34:05 step 224000 epoch 92 learning rate 0.0047 step-time 0.870 loss 0.068
01/01 14:34:05 starting evaluation
01/01 14:38:30 test bleu=37.45 loss=107.25 penalty=1.000 ratio=1.014
01/01 14:38:30 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 14:38:30 finished saving model
01/01 14:57:39   decaying learning rate to: 0.00446
01/01 15:07:44 step 226000 epoch 93 learning rate 0.00446 step-time 0.874 loss 0.067
01/01 15:07:44 starting evaluation
01/01 15:12:09 test bleu=37.64 loss=107.39 penalty=1.000 ratio=1.008
01/01 15:12:09 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 15:12:09 finished saving model
01/01 15:37:51   decaying learning rate to: 0.00424
01/01 15:41:24 step 228000 epoch 94 learning rate 0.00424 step-time 0.874 loss 0.068
01/01 15:41:24 starting evaluation
01/01 15:45:49 test bleu=37.54 loss=107.43 penalty=1.000 ratio=1.011
01/01 15:45:49 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 15:45:49 finished saving model
01/01 16:15:04 step 230000 epoch 94 learning rate 0.00424 step-time 0.874 loss 0.066
01/01 16:15:04 starting evaluation
01/01 16:19:28 test bleu=37.55 loss=107.40 penalty=1.000 ratio=1.011
01/01 16:19:28 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 16:19:28 finished saving model
01/01 16:22:29   decaying learning rate to: 0.00403
01/01 16:48:44 step 232000 epoch 95 learning rate 0.00403 step-time 0.875 loss 0.065
01/01 16:48:44 starting evaluation
01/01 16:53:09 test bleu=37.76 loss=107.65 penalty=1.000 ratio=1.006
01/01 16:53:09 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 16:53:09 finished saving model
01/01 17:02:43   decaying learning rate to: 0.00383
01/01 17:22:23 step 234000 epoch 96 learning rate 0.00383 step-time 0.874 loss 0.063
01/01 17:22:23 starting evaluation
01/01 17:26:48 test bleu=37.40 loss=107.47 penalty=1.000 ratio=1.015
01/01 17:26:48 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 17:26:48 finished saving model
01/01 17:42:57   decaying learning rate to: 0.00363
01/01 17:56:03 step 236000 epoch 97 learning rate 0.00363 step-time 0.874 loss 0.064
01/01 17:56:03 starting evaluation
01/01 18:00:27 test bleu=37.64 loss=107.64 penalty=1.000 ratio=1.009
01/01 18:00:27 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 18:00:27 finished saving model
01/01 18:23:07   decaying learning rate to: 0.00345
01/01 18:29:38 step 238000 epoch 98 learning rate 0.00345 step-time 0.872 loss 0.066
01/01 18:29:38 starting evaluation
01/01 18:34:02 test bleu=37.49 loss=107.65 penalty=1.000 ratio=1.012
01/01 18:34:02 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 18:34:02 finished saving model
01/01 19:03:17 step 240000 epoch 99 learning rate 0.00345 step-time 0.874 loss 0.064
01/01 19:03:17 starting evaluation
01/01 19:07:41 test bleu=37.40 loss=107.89 penalty=1.000 ratio=1.016
01/01 19:07:41 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 19:07:42 finished saving model
01/01 19:07:43   decaying learning rate to: 0.00328
01/01 19:36:54 step 242000 epoch 99 learning rate 0.00328 step-time 0.873 loss 0.062
01/01 19:36:54 starting evaluation
01/01 19:41:19 test bleu=37.57 loss=107.77 penalty=1.000 ratio=1.011
01/01 19:41:19 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 19:41:19 finished saving model
01/01 19:47:54   decaying learning rate to: 0.00312
01/01 20:10:33 step 244000 epoch 100 learning rate 0.00312 step-time 0.874 loss 0.061
01/01 20:10:33 starting evaluation
01/01 20:14:58 test bleu=37.45 loss=107.99 penalty=1.000 ratio=1.012
01/01 20:14:58 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 20:14:58 finished saving model
01/01 20:27:42 finished training
01/01 20:27:42 exiting...
01/01 20:27:42 saving model to models/10_fold_hybrid_pnl/checkpoints
01/01 20:27:42 finished saving model
