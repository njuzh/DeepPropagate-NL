nohup: ignoring input
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:107: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:30: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

12/29 04:26:52 label: default
12/29 04:26:52 description:
  default configuration
  next line of description
  last line
12/29 04:26:52 /root/icpc/icpc/translate/__main__.py config/10-folds/7_fold/hybrid_pnl/config.yaml --train -v
12/29 04:26:52 commit hash 74e0554cb3eb5df835cef993ad570ff8de651f71
12/29 04:26:52 tensorflow version: 1.14.0
12/29 04:26:52 program arguments
12/29 04:26:52   aggregation_method   'sum'
12/29 04:26:52   align_encoder_id     0
12/29 04:26:52   allow_growth         True
12/29 04:26:52   attention_type       'global'
12/29 04:26:52   attn_filter_length   0
12/29 04:26:52   attn_filters         0
12/29 04:26:52   attn_prev_word       False
12/29 04:26:52   attn_size            128
12/29 04:26:52   attn_temperature     1.0
12/29 04:26:52   attn_window_size     0
12/29 04:26:52   average              False
12/29 04:26:52   baseline_activation  None
12/29 04:26:52   baseline_learning_rate 0.001
12/29 04:26:52   baseline_optimizer   'adam'
12/29 04:26:52   baseline_steps       0
12/29 04:26:52   batch_mode           'standard'
12/29 04:26:52   batch_size           64
12/29 04:26:52   beam_size            5
12/29 04:26:52   bidir                True
12/29 04:26:52   bidir_projection     False
12/29 04:26:52   binary               False
12/29 04:26:52   cell_size            256
12/29 04:26:52   cell_type            'GRU'
12/29 04:26:52   character_level      False
12/29 04:26:52   checkpoints          []
12/29 04:26:52   conditional_rnn      False
12/29 04:26:52   config               'config/10-folds/7_fold/hybrid_pnl/config.yaml'
12/29 04:26:52   convolutions         None
12/29 04:26:52   data_dir             'data/gooddata/7_fold'
12/29 04:26:52   debug                False
12/29 04:26:52   decay_after_n_epoch  1
12/29 04:26:52   decay_every_n_epoch  1
12/29 04:26:52   decay_if_no_progress None
12/29 04:26:52   decoders             [{'max_len': 40, 'name': 'nl'}]
12/29 04:26:52   description          'default configuration\nnext line of description\nlast line\n'
12/29 04:26:52   dev_prefix           'test'
12/29 04:26:52   early_stopping       True
12/29 04:26:52   embedding_dropout    0.0
12/29 04:26:52   embedding_initializer None
12/29 04:26:52   embedding_size       256
12/29 04:26:52   embedding_weight_scale None
12/29 04:26:52   embeddings_on_cpu    True
12/29 04:26:52   encoders             [{'attention_type': 'global', 'max_len': 200, 'name': 'code'},
 {'attention_type': 'global', 'max_len': 80, 'name': 'pnl'}]
12/29 04:26:52   ensemble             False
12/29 04:26:52   eval_burn_in         0
12/29 04:26:52   feed_previous        0.0
12/29 04:26:52   final_state          'last'
12/29 04:26:52   freeze_variables     []
12/29 04:26:52   generate_first       True
12/29 04:26:52   gpu_id               1
12/29 04:26:52   highway_layers       0
12/29 04:26:52   initial_state_dropout 0.0
12/29 04:26:52   initializer          None
12/29 04:26:52   input_layer_dropout  0.0
12/29 04:26:52   input_layers         None
12/29 04:26:52   keep_best            5
12/29 04:26:52   keep_every_n_hours   0
12/29 04:26:52   label                'default'
12/29 04:26:52   layer_norm           False
12/29 04:26:52   layers               1
12/29 04:26:52   learning_rate        0.5
12/29 04:26:52   learning_rate_decay_factor 0.95
12/29 04:26:52   len_normalization    1.0
12/29 04:26:52   log_file             'log.txt'
12/29 04:26:52   loss_function        'xent'
12/29 04:26:52   max_dev_size         0
12/29 04:26:52   max_epochs           100
12/29 04:26:52   max_gradient_norm    5.0
12/29 04:26:52   max_len              50
12/29 04:26:52   max_steps            600000
12/29 04:26:52   max_test_size        0
12/29 04:26:52   max_to_keep          1
12/29 04:26:52   max_train_size       0
12/29 04:26:52   maxout_stride        None
12/29 04:26:52   mem_fraction         1.0
12/29 04:26:52   min_learning_rate    1e-06
12/29 04:26:52   model_dir            'models/7_fold_hybrid_pnl'
12/29 04:26:52   moving_average       None
12/29 04:26:52   no_gpu               False
12/29 04:26:52   optimizer            'sgd'
12/29 04:26:52   orthogonal_init      False
12/29 04:26:52   output               None
12/29 04:26:52   output_dropout       0.0
12/29 04:26:52   parallel_iterations  16
12/29 04:26:52   pervasive_dropout    False
12/29 04:26:52   pooling_avg          True
12/29 04:26:52   post_process_script  None
12/29 04:26:52   pred_deep_layer      False
12/29 04:26:52   pred_edits           False
12/29 04:26:52   pred_embed_proj      True
12/29 04:26:52   pred_maxout_layer    True
12/29 04:26:52   purge                False
12/29 04:26:52   raw_output           False
12/29 04:26:52   read_ahead           1
12/29 04:26:52   reconstruction_attn_weight 0.05
12/29 04:26:52   reconstruction_decoders False
12/29 04:26:52   reconstruction_weight 1.0
12/29 04:26:52   reinforce_after_n_epoch None
12/29 04:26:52   remove_unk           False
12/29 04:26:52   reverse              False
12/29 04:26:52   reverse_input        True
12/29 04:26:52   reward_function      'sentence_bleu'
12/29 04:26:52   rnn_feed_attn        True
12/29 04:26:52   rnn_input_dropout    0.0
12/29 04:26:52   rnn_output_dropout   0.0
12/29 04:26:52   rnn_state_dropout    0.0
12/29 04:26:52   save                 False
12/29 04:26:52   score_function       'corpus_bleu'
12/29 04:26:52   score_functions      ['bleu', 'loss']
12/29 04:26:52   script_dir           'scripts'
12/29 04:26:52   sgd_after_n_epoch    None
12/29 04:26:52   sgd_learning_rate    1.0
12/29 04:26:52   shuffle              True
12/29 04:26:52   softmax_temperature  1.0
12/29 04:26:52   steps_per_checkpoint 2000
12/29 04:26:52   steps_per_eval       2000
12/29 04:26:52   swap_memory          True
12/29 04:26:52   tie_embeddings       False
12/29 04:26:52   time_pooling         None
12/29 04:26:52   train                True
12/29 04:26:52   train_initial_states True
12/29 04:26:52   train_prefix         'train'
12/29 04:26:52   truncate_lines       True
12/29 04:26:52   update_first         False
12/29 04:26:52   use_baseline         False
12/29 04:26:52   use_dropout          False
12/29 04:26:52   use_lstm_full_state  False
12/29 04:26:52   use_previous_word    True
12/29 04:26:52   verbose              True
12/29 04:26:52   vocab_prefix         'vocab'
12/29 04:26:52   weight_scale         None
12/29 04:26:52   word_dropout         0.0
12/29 04:26:52 python random seed: 8474838297582716485
12/29 04:26:52 tf random seed:     5904977839693596137
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:203: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

12/29 04:26:52 creating model
12/29 04:26:52 using device: /gpu:1
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:230: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

12/29 04:26:52 copying vocab to models/7_fold_hybrid_pnl/data/vocab.code
12/29 04:26:52 copying vocab to models/7_fold_hybrid_pnl/data/vocab.pnl
12/29 04:26:52 copying vocab to models/7_fold_hybrid_pnl/data/vocab.nl
12/29 04:26:52 reading vocabularies
12/29 04:26:52 creating model
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:60: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:111: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:33: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fea8f4409b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fea8f4409b0>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:226: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fea8f440f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7fea8f440f98>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7feb15ef8b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7feb15ef8b00>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7feb15ef8f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7feb15ef8f98>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15ea6be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15ea6be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:838: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15b96208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15b96208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15b93c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15b93c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:432: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:435: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15ab38d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15ab38d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15ab38d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15ab38d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb1597be80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb1597be80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15934f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15934f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb1565ff98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb1565ff98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15611c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15611c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb1566eb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb1566eb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15645278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15645278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15645278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feb15645278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:919: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7feb154f0438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7feb154f0438>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:131: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:223: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7feab6ead6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7feab6ead6d8>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feab6e53978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feab6e53978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feab6e64978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feab6e64978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feab6db0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feab6db0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feab6dc1b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feab6dc1b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feab6dc1128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feab6dc1128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feab6cf34e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feab6cf34e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feab6cf34e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feab6cf34e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
12/29 04:27:00 model parameters (45)
12/29 04:27:00   baseline_step:0 ()
12/29 04:27:00   decoder_nl/attention_code/U_a/kernel:0 (512, 128)
12/29 04:27:00   decoder_nl/attention_code/W_a/bias:0 (128,)
12/29 04:27:00   decoder_nl/attention_code/W_a/kernel:0 (256, 128)
12/29 04:27:00   decoder_nl/attention_code/v_a:0 (128,)
12/29 04:27:00   decoder_nl/attention_pnl/U_a/kernel:0 (512, 128)
12/29 04:27:00   decoder_nl/attention_pnl/W_a/bias:0 (128,)
12/29 04:27:00   decoder_nl/attention_pnl/W_a/kernel:0 (256, 128)
12/29 04:27:00   decoder_nl/attention_pnl/v_a:0 (128,)
12/29 04:27:00   decoder_nl/code_pnl/initial_state_projection/bias:0 (256,)
12/29 04:27:00   decoder_nl/code_pnl/initial_state_projection/kernel:0 (512, 256)
12/29 04:27:00   decoder_nl/gru_cell/candidate/bias:0 (256,)
12/29 04:27:00   decoder_nl/gru_cell/candidate/kernel:0 (1024, 256)
12/29 04:27:00   decoder_nl/gru_cell/gates/bias:0 (512,)
12/29 04:27:00   decoder_nl/gru_cell/gates/kernel:0 (1024, 512)
12/29 04:27:00   decoder_nl/maxout/bias:0 (256,)
12/29 04:27:00   decoder_nl/maxout/kernel:0 (1024, 256)
12/29 04:27:00   decoder_nl/softmax0/kernel:0 (128, 256)
12/29 04:27:00   decoder_nl/softmax1/bias:0 (38019,)
12/29 04:27:00   decoder_nl/softmax1/kernel:0 (256, 38019)
12/29 04:27:00   embedding_code:0 (50000, 256)
12/29 04:27:00   embedding_nl:0 (38019, 256)
12/29 04:27:00   embedding_pnl:0 (37556, 256)
12/29 04:27:00   encoder_code/initial_state_bw:0 (256,)
12/29 04:27:00   encoder_code/initial_state_fw:0 (256,)
12/29 04:27:00   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/29 04:27:00   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/29 04:27:00   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/29 04:27:00   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/29 04:27:00   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/29 04:27:00   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/29 04:27:00   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/29 04:27:00   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/29 04:27:00   encoder_pnl/initial_state_bw:0 (256,)
12/29 04:27:00   encoder_pnl/initial_state_fw:0 (256,)
12/29 04:27:00   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/29 04:27:00   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/29 04:27:00   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/29 04:27:00   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/29 04:27:00   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/29 04:27:00   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/29 04:27:00   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/29 04:27:00   encoder_pnl/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/29 04:27:00   global_step:0 ()
12/29 04:27:00   learning_rate:0 ()
12/29 04:27:00 number of parameters: 44.91M
WARNING:tensorflow:From /root/icpc/icpc/translate/translation_model.py:666: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

12/29 04:27:01 global step: 0
12/29 04:27:01 baseline step: 0
12/29 04:27:01 reading training data
12/29 04:27:01 total line count: 156721
12/29 04:27:07   lines read: 100000
12/29 04:27:10 files: data/gooddata/7_fold/train.code data/gooddata/7_fold/train.pnl data/gooddata/7_fold/train.nl
12/29 04:27:10 lines reads: 156721
12/29 04:27:10 reading development data
12/29 04:27:11 files: data/gooddata/7_fold/test.code data/gooddata/7_fold/test.pnl data/gooddata/7_fold/test.nl
12/29 04:27:11 lines reads: 17413
12/29 04:27:11 starting training
12/29 04:54:34 step 2000 epoch 1 learning rate 0.5 step-time 0.820 loss 80.955
12/29 04:54:34 starting evaluation
12/29 04:59:03 test bleu=1.34 loss=63.44 penalty=0.586 ratio=0.651
12/29 04:59:03 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 04:59:03 finished saving model
12/29 04:59:03 new best model
12/29 05:05:12   decaying learning rate to: 0.475
12/29 05:26:44 step 4000 epoch 2 learning rate 0.475 step-time 0.828 loss 59.778
12/29 05:26:44 starting evaluation
12/29 05:31:47 test bleu=3.60 loss=56.20 penalty=1.000 ratio=1.480
12/29 05:31:47 saving model to models/7_fold_hybrid_pnl/checkpoints
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
12/29 05:31:47 finished saving model
12/29 05:31:47 new best model
12/29 05:44:09   decaying learning rate to: 0.451
12/29 05:59:28 step 6000 epoch 3 learning rate 0.451 step-time 0.828 loss 52.268
12/29 05:59:28 starting evaluation
12/29 06:03:58 test bleu=8.04 loss=49.58 penalty=0.647 ratio=0.696
12/29 06:03:58 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 06:03:58 finished saving model
12/29 06:03:58 new best model
12/29 06:22:37   decaying learning rate to: 0.429
12/29 06:31:37 step 8000 epoch 4 learning rate 0.429 step-time 0.827 loss 46.819
12/29 06:31:37 starting evaluation
12/29 06:36:40 test bleu=12.47 loss=45.64 penalty=1.000 ratio=1.050
12/29 06:36:40 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 06:36:40 finished saving model
12/29 06:36:40 new best model
12/29 07:01:30   decaying learning rate to: 0.407
12/29 07:04:14 step 10000 epoch 5 learning rate 0.407 step-time 0.825 loss 42.769
12/29 07:04:14 starting evaluation
12/29 07:09:13 test bleu=16.81 loss=42.46 penalty=0.957 ratio=0.957
12/29 07:09:13 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 07:09:13 finished saving model
12/29 07:09:13 new best model
12/29 07:36:15 step 12000 epoch 5 learning rate 0.407 step-time 0.809 loss 39.065
12/29 07:36:15 starting evaluation
12/29 07:41:15 test bleu=17.99 loss=40.89 penalty=1.000 ratio=1.044
12/29 07:41:15 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 07:41:15 finished saving model
12/29 07:41:15 new best model
12/29 07:44:39   decaying learning rate to: 0.387
12/29 08:08:03 step 14000 epoch 6 learning rate 0.387 step-time 0.802 loss 36.052
12/29 08:08:03 starting evaluation
12/29 08:12:49 test bleu=19.91 loss=39.61 penalty=0.898 ratio=0.903
12/29 08:12:49 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 08:12:50 finished saving model
12/29 08:12:50 new best model
12/29 08:21:53   decaying learning rate to: 0.368
12/29 08:38:46 step 16000 epoch 7 learning rate 0.368 step-time 0.776 loss 33.453
12/29 08:38:46 starting evaluation
12/29 08:43:37 test bleu=22.43 loss=38.04 penalty=0.847 ratio=0.857
12/29 08:43:37 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 08:43:37 finished saving model
12/29 08:43:37 new best model
12/29 08:59:20   decaying learning rate to: 0.349
12/29 09:11:24 step 18000 epoch 8 learning rate 0.349 step-time 0.831 loss 31.402
12/29 09:11:24 starting evaluation
12/29 09:16:29 test bleu=24.52 loss=37.35 penalty=0.940 ratio=0.942
12/29 09:16:29 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 09:16:29 finished saving model
12/29 09:16:29 new best model
12/29 09:38:27   decaying learning rate to: 0.332
12/29 09:44:01 step 20000 epoch 9 learning rate 0.332 step-time 0.824 loss 29.183
12/29 09:44:01 starting evaluation
12/29 09:48:57 test bleu=26.15 loss=37.14 penalty=0.890 ratio=0.895
12/29 09:48:57 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 09:48:57 finished saving model
12/29 09:48:57 new best model
12/29 10:16:28 step 22000 epoch 9 learning rate 0.332 step-time 0.823 loss 27.394
12/29 10:16:28 starting evaluation
12/29 10:21:27 test bleu=26.97 loss=36.16 penalty=0.924 ratio=0.927
12/29 10:21:27 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 10:21:27 finished saving model
12/29 10:21:27 new best model
12/29 10:22:01   decaying learning rate to: 0.315
12/29 10:49:07 step 24000 epoch 10 learning rate 0.315 step-time 0.828 loss 24.746
12/29 10:49:07 starting evaluation
12/29 10:54:04 test bleu=27.41 loss=36.19 penalty=0.889 ratio=0.895
12/29 10:54:04 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 10:54:04 finished saving model
12/29 10:54:04 new best model
12/29 11:00:59   decaying learning rate to: 0.299
12/29 11:21:47 step 26000 epoch 11 learning rate 0.299 step-time 0.829 loss 22.965
12/29 11:21:47 starting evaluation
12/29 11:26:33 test bleu=28.94 loss=36.79 penalty=0.878 ratio=0.885
12/29 11:26:33 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 11:26:33 finished saving model
12/29 11:26:33 new best model
12/29 11:39:32   decaying learning rate to: 0.284
12/29 11:54:13 step 28000 epoch 12 learning rate 0.284 step-time 0.828 loss 21.325
12/29 11:54:13 starting evaluation
12/29 11:59:16 test bleu=29.59 loss=37.63 penalty=0.913 ratio=0.916
12/29 11:59:16 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 11:59:16 finished saving model
12/29 11:59:16 new best model
12/29 12:18:20   decaying learning rate to: 0.27
12/29 12:26:54 step 30000 epoch 13 learning rate 0.27 step-time 0.827 loss 19.757
12/29 12:26:54 starting evaluation
12/29 12:31:45 test bleu=30.39 loss=37.73 penalty=0.901 ratio=0.906
12/29 12:31:45 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 12:31:46 finished saving model
12/29 12:31:46 new best model
12/29 12:57:12   decaying learning rate to: 0.257
12/29 12:59:25 step 32000 epoch 14 learning rate 0.257 step-time 0.828 loss 18.361
12/29 12:59:25 starting evaluation
12/29 13:04:21 test bleu=31.07 loss=38.58 penalty=0.871 ratio=0.879
12/29 13:04:21 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 13:04:21 finished saving model
12/29 13:04:21 new best model
12/29 13:32:03 step 34000 epoch 14 learning rate 0.257 step-time 0.829 loss 16.454
12/29 13:32:03 starting evaluation
12/29 13:36:54 test bleu=31.18 loss=38.30 penalty=0.863 ratio=0.872
12/29 13:36:54 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 13:36:55 finished saving model
12/29 13:36:55 new best model
12/29 13:40:51   decaying learning rate to: 0.244
12/29 14:04:30 step 36000 epoch 15 learning rate 0.244 step-time 0.825 loss 14.768
12/29 14:04:30 starting evaluation
12/29 14:09:33 test bleu=32.09 loss=39.41 penalty=1.000 ratio=1.010
12/29 14:09:33 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 14:09:33 finished saving model
12/29 14:09:33 new best model
12/29 14:19:46   decaying learning rate to: 0.232
12/29 14:37:02 step 38000 epoch 16 learning rate 0.232 step-time 0.822 loss 13.624
12/29 14:37:02 starting evaluation
12/29 14:41:57 test bleu=32.38 loss=41.50 penalty=0.992 ratio=0.992
12/29 14:41:57 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 14:41:58 finished saving model
12/29 14:41:58 new best model
12/29 14:58:17   decaying learning rate to: 0.22
12/29 15:09:34 step 40000 epoch 17 learning rate 0.22 step-time 0.826 loss 12.470
12/29 15:09:34 starting evaluation
12/29 15:14:29 test bleu=32.99 loss=43.44 penalty=0.955 ratio=0.956
12/29 15:14:29 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 15:14:29 finished saving model
12/29 15:14:29 new best model
12/29 15:36:56   decaying learning rate to: 0.209
12/29 15:42:11 step 42000 epoch 18 learning rate 0.209 step-time 0.829 loss 11.481
12/29 15:42:11 starting evaluation
12/29 15:47:07 test bleu=33.26 loss=44.51 penalty=0.895 ratio=0.900
12/29 15:47:07 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 15:47:07 finished saving model
12/29 15:47:07 new best model
12/29 16:14:42 step 44000 epoch 18 learning rate 0.209 step-time 0.825 loss 10.303
12/29 16:14:42 starting evaluation
12/29 16:19:37 test bleu=33.60 loss=44.30 penalty=0.925 ratio=0.927
12/29 16:19:37 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 16:19:37 finished saving model
12/29 16:19:37 new best model
12/29 16:20:46   decaying learning rate to: 0.199
12/29 16:47:16 step 46000 epoch 19 learning rate 0.199 step-time 0.827 loss 8.821
12/29 16:47:16 starting evaluation
12/29 16:52:16 test bleu=33.41 loss=47.19 penalty=0.938 ratio=0.939
12/29 16:52:16 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 16:52:16 finished saving model
12/29 16:59:34   decaying learning rate to: 0.189
12/29 17:19:57 step 48000 epoch 20 learning rate 0.189 step-time 0.828 loss 8.061
12/29 17:19:57 starting evaluation
12/29 17:24:58 test bleu=33.93 loss=49.27 penalty=0.990 ratio=0.990
12/29 17:24:58 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 17:24:58 finished saving model
12/29 17:24:58 new best model
12/29 17:38:35   decaying learning rate to: 0.179
12/29 17:52:40 step 50000 epoch 21 learning rate 0.179 step-time 0.829 loss 7.303
12/29 17:52:40 starting evaluation
12/29 17:57:41 test bleu=34.48 loss=51.76 penalty=0.979 ratio=0.980
12/29 17:57:41 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 17:57:42 finished saving model
12/29 17:57:42 new best model
12/29 18:17:26   decaying learning rate to: 0.17
12/29 18:25:23 step 52000 epoch 22 learning rate 0.17 step-time 0.828 loss 6.627
12/29 18:25:23 starting evaluation
12/29 18:30:17 test bleu=34.83 loss=53.97 penalty=0.963 ratio=0.963
12/29 18:30:17 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 18:30:17 finished saving model
12/29 18:30:17 new best model
12/29 18:56:05   decaying learning rate to: 0.162
12/29 18:57:52 step 54000 epoch 23 learning rate 0.162 step-time 0.825 loss 6.010
12/29 18:57:52 starting evaluation
12/29 19:02:53 test bleu=35.07 loss=55.75 penalty=0.949 ratio=0.950
12/29 19:02:53 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 19:02:54 finished saving model
12/29 19:02:54 new best model
12/29 19:30:34 step 56000 epoch 23 learning rate 0.162 step-time 0.828 loss 5.104
12/29 19:30:34 starting evaluation
12/29 19:35:30 test bleu=35.37 loss=57.16 penalty=0.988 ratio=0.988
12/29 19:35:30 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 19:35:30 finished saving model
12/29 19:35:30 new best model
12/29 19:39:55   decaying learning rate to: 0.154
12/29 20:03:09 step 58000 epoch 24 learning rate 0.154 step-time 0.827 loss 4.492
12/29 20:03:09 starting evaluation
12/29 20:08:10 test bleu=35.42 loss=60.03 penalty=0.987 ratio=0.987
12/29 20:08:10 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 20:08:10 finished saving model
12/29 20:08:10 new best model
12/29 20:18:48   decaying learning rate to: 0.146
12/29 20:35:51 step 60000 epoch 25 learning rate 0.146 step-time 0.828 loss 4.068
12/29 20:35:51 starting evaluation
12/29 20:40:52 test bleu=35.39 loss=61.76 penalty=1.000 ratio=1.004
12/29 20:40:52 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 20:40:52 finished saving model
12/29 20:57:49   decaying learning rate to: 0.139
12/29 21:08:28 step 62000 epoch 26 learning rate 0.139 step-time 0.826 loss 3.676
12/29 21:08:28 starting evaluation
12/29 21:13:29 test bleu=35.66 loss=64.49 penalty=1.000 ratio=1.003
12/29 21:13:29 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 21:13:29 finished saving model
12/29 21:13:29 new best model
12/29 21:36:39   decaying learning rate to: 0.132
12/29 21:41:12 step 64000 epoch 27 learning rate 0.132 step-time 0.829 loss 3.335
12/29 21:41:12 starting evaluation
12/29 21:46:00 test bleu=35.74 loss=66.97 penalty=0.955 ratio=0.956
12/29 21:46:00 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 21:46:01 finished saving model
12/29 21:46:01 new best model
12/29 22:13:39 step 66000 epoch 27 learning rate 0.132 step-time 0.827 loss 2.929
12/29 22:13:39 starting evaluation
12/29 22:18:44 test bleu=36.08 loss=68.29 penalty=0.987 ratio=0.987
12/29 22:18:44 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 22:18:44 finished saving model
12/29 22:18:44 new best model
12/29 22:20:24   decaying learning rate to: 0.125
12/29 22:46:20 step 68000 epoch 28 learning rate 0.125 step-time 0.826 loss 2.489
12/29 22:46:20 starting evaluation
12/29 22:51:19 test bleu=36.18 loss=71.07 penalty=0.990 ratio=0.990
12/29 22:51:19 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 22:51:19 finished saving model
12/29 22:51:19 new best model
12/29 22:59:04   decaying learning rate to: 0.119
12/29 23:18:59 step 70000 epoch 29 learning rate 0.119 step-time 0.828 loss 2.255
12/29 23:18:59 starting evaluation
12/29 23:23:52 test bleu=36.55 loss=73.29 penalty=0.992 ratio=0.992
12/29 23:23:52 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 23:23:53 finished saving model
12/29 23:23:53 new best model
12/29 23:38:02   decaying learning rate to: 0.113
12/29 23:51:28 step 72000 epoch 30 learning rate 0.113 step-time 0.825 loss 2.050
12/29 23:51:28 starting evaluation
12/29 23:56:23 test bleu=35.27 loss=75.54 penalty=1.000 ratio=1.032
12/29 23:56:23 saving model to models/7_fold_hybrid_pnl/checkpoints
12/29 23:56:23 finished saving model
12/30 00:16:16   decaying learning rate to: 0.107
12/30 00:23:22 step 74000 epoch 31 learning rate 0.107 step-time 0.807 loss 1.866
12/30 00:23:22 starting evaluation
12/30 00:28:18 test bleu=35.66 loss=77.55 penalty=1.000 ratio=1.029
12/30 00:28:18 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 00:28:18 finished saving model
12/30 00:54:13   decaying learning rate to: 0.102
12/30 00:55:19 step 76000 epoch 32 learning rate 0.102 step-time 0.809 loss 1.703
12/30 00:55:19 starting evaluation
12/30 01:00:13 test bleu=36.26 loss=79.28 penalty=1.000 ratio=1.017
12/30 01:00:13 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 01:00:14 finished saving model
12/30 01:27:15 step 78000 epoch 32 learning rate 0.102 step-time 0.809 loss 1.439
12/30 01:27:15 starting evaluation
12/30 01:32:03 test bleu=35.39 loss=81.14 penalty=1.000 ratio=1.042
12/30 01:32:03 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 01:32:03 finished saving model
12/30 01:37:04   decaying learning rate to: 0.0969
12/30 01:59:02 step 80000 epoch 33 learning rate 0.0969 step-time 0.808 loss 1.307
12/30 01:59:02 starting evaluation
12/30 02:04:02 test bleu=36.39 loss=82.97 penalty=0.983 ratio=0.983
12/30 02:04:02 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 02:04:03 finished saving model
12/30 02:14:56   decaying learning rate to: 0.092
12/30 02:30:59 step 82000 epoch 34 learning rate 0.092 step-time 0.806 loss 1.206
12/30 02:30:59 starting evaluation
12/30 02:35:53 test bleu=36.75 loss=84.81 penalty=0.993 ratio=0.993
12/30 02:35:53 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 02:35:53 finished saving model
12/30 02:35:53 new best model
12/30 02:52:50   decaying learning rate to: 0.0874
12/30 03:02:51 step 84000 epoch 35 learning rate 0.0874 step-time 0.807 loss 1.095
12/30 03:02:51 starting evaluation
12/30 03:07:42 test bleu=36.17 loss=86.64 penalty=1.000 ratio=1.019
12/30 03:07:42 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 03:07:42 finished saving model
12/30 03:31:34   decaying learning rate to: 0.083
12/30 03:35:22 step 86000 epoch 36 learning rate 0.083 step-time 0.828 loss 1.027
12/30 03:35:22 starting evaluation
12/30 03:40:21 test bleu=36.12 loss=87.69 penalty=1.000 ratio=1.022
12/30 03:40:21 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 03:40:21 finished saving model
12/30 04:08:05 step 88000 epoch 36 learning rate 0.083 step-time 0.830 loss 0.923
12/30 04:08:05 starting evaluation
12/30 04:13:05 test bleu=36.03 loss=89.00 penalty=1.000 ratio=1.026
12/30 04:13:05 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 04:13:06 finished saving model
12/30 04:15:23   decaying learning rate to: 0.0789
12/30 04:40:47 step 90000 epoch 37 learning rate 0.0789 step-time 0.829 loss 0.819
12/30 04:40:47 starting evaluation
12/30 04:45:42 test bleu=36.21 loss=90.46 penalty=1.000 ratio=1.025
12/30 04:45:42 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 04:45:42 finished saving model
12/30 04:54:23   decaying learning rate to: 0.0749
12/30 05:13:13 step 92000 epoch 38 learning rate 0.0749 step-time 0.824 loss 0.747
12/30 05:13:13 starting evaluation
12/30 05:18:05 test bleu=36.89 loss=91.92 penalty=1.000 ratio=1.007
12/30 05:18:05 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 05:18:05 finished saving model
12/30 05:18:05 new best model
12/30 05:32:39   decaying learning rate to: 0.0712
12/30 05:45:41 step 94000 epoch 39 learning rate 0.0712 step-time 0.826 loss 0.718
12/30 05:45:41 starting evaluation
12/30 05:50:43 test bleu=36.55 loss=93.67 penalty=1.000 ratio=1.020
12/30 05:50:43 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 05:50:43 finished saving model
12/30 06:11:33   decaying learning rate to: 0.0676
12/30 06:18:25 step 96000 epoch 40 learning rate 0.0676 step-time 0.829 loss 0.678
12/30 06:18:25 starting evaluation
12/30 06:23:24 test bleu=36.60 loss=94.52 penalty=1.000 ratio=1.016
12/30 06:23:24 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 06:23:24 finished saving model
12/30 06:50:30   decaying learning rate to: 0.0643
12/30 06:51:02 step 98000 epoch 41 learning rate 0.0643 step-time 0.827 loss 0.632
12/30 06:51:02 starting evaluation
12/30 06:56:00 test bleu=36.62 loss=95.44 penalty=1.000 ratio=1.015
12/30 06:56:00 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 06:56:00 finished saving model
12/30 07:23:51 step 100000 epoch 41 learning rate 0.0643 step-time 0.833 loss 0.561
12/30 07:23:51 starting evaluation
12/30 07:28:56 test bleu=36.61 loss=95.97 penalty=1.000 ratio=1.025
12/30 07:28:56 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 07:28:56 finished saving model
12/30 07:35:24   decaying learning rate to: 0.061
12/30 08:01:35 step 102000 epoch 42 learning rate 0.061 step-time 0.977 loss 0.528
12/30 08:01:35 starting evaluation
12/30 08:06:52 test bleu=37.33 loss=96.72 penalty=1.000 ratio=1.009
12/30 08:06:52 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 08:06:52 finished saving model
12/30 08:06:52 new best model
12/30 08:21:12   decaying learning rate to: 0.058
12/30 08:40:06 step 104000 epoch 43 learning rate 0.058 step-time 0.994 loss 0.506
12/30 08:40:06 starting evaluation
12/30 08:45:25 test bleu=36.29 loss=98.43 penalty=1.000 ratio=1.030
12/30 08:45:25 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 08:45:25 finished saving model
12/30 09:05:23   decaying learning rate to: 0.0551
12/30 09:14:57 step 106000 epoch 44 learning rate 0.0551 step-time 0.884 loss 0.476
12/30 09:14:57 starting evaluation
12/30 09:19:57 test bleu=36.75 loss=98.79 penalty=1.000 ratio=1.022
12/30 09:19:57 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 09:19:57 finished saving model
12/30 09:44:18   decaying learning rate to: 0.0523
12/30 09:47:35 step 108000 epoch 45 learning rate 0.0523 step-time 0.827 loss 0.457
12/30 09:47:35 starting evaluation
12/30 09:52:34 test bleu=37.17 loss=99.54 penalty=1.000 ratio=1.010
12/30 09:52:34 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 09:52:34 finished saving model
12/30 10:20:19 step 110000 epoch 45 learning rate 0.0523 step-time 0.831 loss 0.433
12/30 10:20:19 starting evaluation
12/30 10:25:20 test bleu=36.74 loss=99.39 penalty=1.000 ratio=1.026
12/30 10:25:20 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 10:25:20 finished saving model
12/30 10:28:13   decaying learning rate to: 0.0497
12/30 10:53:08 step 112000 epoch 46 learning rate 0.0497 step-time 0.832 loss 0.386
12/30 10:53:08 starting evaluation
12/30 10:58:02 test bleu=37.34 loss=99.85 penalty=1.000 ratio=1.010
12/30 10:58:02 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 10:58:02 finished saving model
12/30 10:58:02 new best model
12/30 11:07:13   decaying learning rate to: 0.0472
12/30 11:25:42 step 114000 epoch 47 learning rate 0.0472 step-time 0.828 loss 0.376
12/30 11:25:42 starting evaluation
12/30 11:30:51 test bleu=36.77 loss=100.37 penalty=1.000 ratio=1.025
12/30 11:30:51 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 11:30:51 finished saving model
12/30 11:46:05   decaying learning rate to: 0.0449
12/30 11:58:37 step 116000 epoch 48 learning rate 0.0449 step-time 0.831 loss 0.359
12/30 11:58:37 starting evaluation
12/30 12:03:38 test bleu=36.99 loss=101.31 penalty=1.000 ratio=1.019
12/30 12:03:38 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 12:03:38 finished saving model
12/30 12:25:15   decaying learning rate to: 0.0426
12/30 12:31:26 step 118000 epoch 49 learning rate 0.0426 step-time 0.832 loss 0.349
12/30 12:31:26 starting evaluation
12/30 12:36:27 test bleu=37.00 loss=101.06 penalty=1.000 ratio=1.022
12/30 12:36:27 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 12:36:27 finished saving model
12/30 13:04:14 step 120000 epoch 50 learning rate 0.0426 step-time 0.831 loss 0.330
12/30 13:04:14 starting evaluation
12/30 13:09:14 test bleu=37.06 loss=100.82 penalty=1.000 ratio=1.022
12/30 13:09:14 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 13:09:14 finished saving model
12/30 13:09:15   decaying learning rate to: 0.0405
12/30 13:36:58 step 122000 epoch 50 learning rate 0.0405 step-time 0.830 loss 0.291
12/30 13:36:58 starting evaluation
12/30 13:41:57 test bleu=37.81 loss=101.13 penalty=0.981 ratio=0.981
12/30 13:41:57 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 13:41:58 finished saving model
12/30 13:41:58 new best model
12/30 13:48:23   decaying learning rate to: 0.0385
12/30 14:09:40 step 124000 epoch 51 learning rate 0.0385 step-time 0.829 loss 0.273
12/30 14:09:40 starting evaluation
12/30 14:14:29 test bleu=37.37 loss=100.64 penalty=1.000 ratio=1.013
12/30 14:14:29 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 14:14:29 finished saving model
12/30 14:27:08   decaying learning rate to: 0.0365
12/30 14:42:07 step 126000 epoch 52 learning rate 0.0365 step-time 0.827 loss 0.269
12/30 14:42:07 starting evaluation
12/30 14:47:11 test bleu=37.60 loss=101.42 penalty=1.000 ratio=1.008
12/30 14:47:11 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 14:47:12 finished saving model
12/30 15:05:42   decaying learning rate to: 0.0347
12/30 15:14:50 step 128000 epoch 53 learning rate 0.0347 step-time 0.827 loss 0.254
12/30 15:14:50 starting evaluation
12/30 15:19:51 test bleu=37.53 loss=100.97 penalty=1.000 ratio=1.009
12/30 15:19:51 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 15:19:51 finished saving model
12/30 15:44:42   decaying learning rate to: 0.033
12/30 15:47:31 step 130000 epoch 54 learning rate 0.033 step-time 0.828 loss 0.245
12/30 15:47:31 starting evaluation
12/30 15:52:32 test bleu=37.45 loss=100.68 penalty=1.000 ratio=1.017
12/30 15:52:32 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 15:52:33 finished saving model
12/30 16:20:11 step 132000 epoch 54 learning rate 0.033 step-time 0.827 loss 0.226
12/30 16:20:11 starting evaluation
12/30 16:25:10 test bleu=37.34 loss=101.37 penalty=1.000 ratio=1.014
12/30 16:25:10 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 16:25:10 finished saving model
12/30 16:28:34   decaying learning rate to: 0.0313
12/30 16:52:52 step 134000 epoch 55 learning rate 0.0313 step-time 0.829 loss 0.209
12/30 16:52:52 starting evaluation
12/30 16:57:54 test bleu=37.50 loss=100.97 penalty=1.000 ratio=1.016
12/30 16:57:54 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 16:57:54 finished saving model
12/30 17:07:37   decaying learning rate to: 0.0298
12/30 17:25:29 step 136000 epoch 56 learning rate 0.0298 step-time 0.826 loss 0.199
12/30 17:25:29 starting evaluation
12/30 17:30:29 test bleu=37.25 loss=101.00 penalty=1.000 ratio=1.021
12/30 17:30:29 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 17:30:29 finished saving model
12/30 17:46:27   decaying learning rate to: 0.0283
12/30 17:58:10 step 138000 epoch 57 learning rate 0.0283 step-time 0.828 loss 0.196
12/30 17:58:10 starting evaluation
12/30 18:03:04 test bleu=37.02 loss=100.84 penalty=1.000 ratio=1.025
12/30 18:03:04 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 18:03:05 finished saving model
12/30 18:24:59   decaying learning rate to: 0.0269
12/30 18:30:44 step 140000 epoch 58 learning rate 0.0269 step-time 0.828 loss 0.191
12/30 18:30:44 starting evaluation
12/30 18:35:48 test bleu=37.10 loss=101.26 penalty=1.000 ratio=1.026
12/30 18:35:48 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 18:35:48 finished saving model
12/30 19:03:29 step 142000 epoch 58 learning rate 0.0269 step-time 0.828 loss 0.184
12/30 19:03:29 starting evaluation
12/30 19:08:30 test bleu=36.84 loss=100.92 penalty=1.000 ratio=1.034
12/30 19:08:30 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 19:08:31 finished saving model
12/30 19:09:06   decaying learning rate to: 0.0255
12/30 19:36:15 step 144000 epoch 59 learning rate 0.0255 step-time 0.830 loss 0.163
12/30 19:36:15 starting evaluation
12/30 19:41:17 test bleu=37.42 loss=101.55 penalty=1.000 ratio=1.019
12/30 19:41:17 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 19:41:17 finished saving model
12/30 19:48:02   decaying learning rate to: 0.0242
12/30 20:08:57 step 146000 epoch 60 learning rate 0.0242 step-time 0.828 loss 0.159
12/30 20:08:57 starting evaluation
12/30 20:13:57 test bleu=37.31 loss=101.33 penalty=1.000 ratio=1.022
12/30 20:13:57 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 20:13:57 finished saving model
12/30 20:27:02   decaying learning rate to: 0.023
12/30 20:41:39 step 148000 epoch 61 learning rate 0.023 step-time 0.829 loss 0.157
12/30 20:41:39 starting evaluation
12/30 20:46:41 test bleu=37.42 loss=101.91 penalty=1.000 ratio=1.017
12/30 20:46:41 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 20:46:41 finished saving model
12/30 21:06:00   decaying learning rate to: 0.0219
12/30 21:14:26 step 150000 epoch 62 learning rate 0.0219 step-time 0.831 loss 0.151
12/30 21:14:26 starting evaluation
12/30 21:19:18 test bleu=37.82 loss=101.82 penalty=1.000 ratio=1.009
12/30 21:19:18 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 21:19:18 finished saving model
12/30 21:19:18 new best model
12/30 21:44:41   decaying learning rate to: 0.0208
12/30 21:46:58 step 152000 epoch 63 learning rate 0.0208 step-time 0.828 loss 0.151
12/30 21:46:58 starting evaluation
12/30 21:52:06 test bleu=37.07 loss=101.62 penalty=1.000 ratio=1.029
12/30 21:52:06 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 21:52:06 finished saving model
12/30 22:19:47 step 154000 epoch 63 learning rate 0.0208 step-time 0.829 loss 0.140
12/30 22:19:47 starting evaluation
12/30 22:24:49 test bleu=37.39 loss=101.52 penalty=1.000 ratio=1.020
12/30 22:24:49 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 22:24:49 finished saving model
12/30 22:28:47   decaying learning rate to: 0.0197
12/30 22:52:28 step 156000 epoch 64 learning rate 0.0197 step-time 0.828 loss 0.131
12/30 22:52:28 starting evaluation
12/30 22:57:29 test bleu=37.84 loss=101.94 penalty=1.000 ratio=1.007
12/30 22:57:29 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 22:57:29 finished saving model
12/30 22:57:29 new best model
12/30 23:07:30   decaying learning rate to: 0.0188
12/30 23:25:11 step 158000 epoch 65 learning rate 0.0188 step-time 0.829 loss 0.133
12/30 23:25:11 starting evaluation
12/30 23:30:10 test bleu=37.41 loss=102.34 penalty=1.000 ratio=1.019
12/30 23:30:10 saving model to models/7_fold_hybrid_pnl/checkpoints
12/30 23:30:10 finished saving model
12/30 23:46:36   decaying learning rate to: 0.0178
12/30 23:57:50 step 160000 epoch 66 learning rate 0.0178 step-time 0.828 loss 0.126
12/30 23:57:50 starting evaluation
12/31 00:02:51 test bleu=37.27 loss=102.41 penalty=1.000 ratio=1.025
12/31 00:02:51 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 00:02:51 finished saving model
12/31 00:25:05   decaying learning rate to: 0.0169
12/31 00:30:01 step 162000 epoch 67 learning rate 0.0169 step-time 0.813 loss 0.126
12/31 00:30:01 starting evaluation
12/31 00:34:50 test bleu=37.38 loss=102.37 penalty=1.000 ratio=1.021
12/31 00:34:50 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 00:34:50 finished saving model
12/31 01:01:51 step 164000 epoch 67 learning rate 0.0169 step-time 0.809 loss 0.121
12/31 01:01:51 starting evaluation
12/31 01:06:43 test bleu=37.84 loss=102.17 penalty=1.000 ratio=1.009
12/31 01:06:43 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 01:06:43 finished saving model
12/31 01:06:43 new best model
12/31 01:07:51   decaying learning rate to: 0.0161
12/31 01:33:39 step 166000 epoch 68 learning rate 0.0161 step-time 0.806 loss 0.116
12/31 01:33:39 starting evaluation
12/31 01:38:38 test bleu=37.92 loss=102.51 penalty=1.000 ratio=1.007
12/31 01:38:38 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 01:38:38 finished saving model
12/31 01:38:38 new best model
12/31 01:45:45   decaying learning rate to: 0.0153
12/31 02:05:39 step 168000 epoch 69 learning rate 0.0153 step-time 0.808 loss 0.111
12/31 02:05:39 starting evaluation
12/31 02:10:35 test bleu=37.73 loss=102.41 penalty=1.000 ratio=1.013
12/31 02:10:35 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 02:10:35 finished saving model
12/31 02:23:43   decaying learning rate to: 0.0145
12/31 02:37:35 step 170000 epoch 70 learning rate 0.0145 step-time 0.808 loss 0.108
12/31 02:37:35 starting evaluation
12/31 02:42:33 test bleu=37.84 loss=103.08 penalty=1.000 ratio=1.009
12/31 02:42:33 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 02:42:33 finished saving model
12/31 03:01:58   decaying learning rate to: 0.0138
12/31 03:09:36 step 172000 epoch 71 learning rate 0.0138 step-time 0.810 loss 0.106
12/31 03:09:36 starting evaluation
12/31 03:14:32 test bleu=37.83 loss=102.91 penalty=1.000 ratio=1.009
12/31 03:14:32 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 03:14:33 finished saving model
12/31 03:40:04   decaying learning rate to: 0.0131
12/31 03:41:41 step 174000 epoch 72 learning rate 0.0131 step-time 0.812 loss 0.108
12/31 03:41:41 starting evaluation
12/31 03:46:39 test bleu=37.44 loss=103.43 penalty=1.000 ratio=1.020
12/31 03:46:39 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 03:46:40 finished saving model
12/31 04:13:49 step 176000 epoch 72 learning rate 0.0131 step-time 0.813 loss 0.099
12/31 04:13:49 starting evaluation
12/31 04:18:43 test bleu=37.42 loss=103.02 penalty=1.000 ratio=1.017
12/31 04:18:43 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 04:18:43 finished saving model
12/31 04:23:18   decaying learning rate to: 0.0124
12/31 04:45:47 step 178000 epoch 73 learning rate 0.0124 step-time 0.810 loss 0.097
12/31 04:45:47 starting evaluation
12/31 04:50:38 test bleu=38.25 loss=103.57 penalty=1.000 ratio=1.000
12/31 04:50:38 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 04:50:38 finished saving model
12/31 04:50:38 new best model
12/31 05:00:56   decaying learning rate to: 0.0118
12/31 05:17:29 step 180000 epoch 74 learning rate 0.0118 step-time 0.803 loss 0.096
12/31 05:17:29 starting evaluation
12/31 05:22:32 test bleu=37.59 loss=103.86 penalty=1.000 ratio=1.018
12/31 05:22:32 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 05:22:32 finished saving model
12/31 05:38:57   decaying learning rate to: 0.0112
12/31 05:49:41 step 182000 epoch 75 learning rate 0.0112 step-time 0.813 loss 0.096
12/31 05:49:41 starting evaluation
12/31 05:54:38 test bleu=37.84 loss=104.10 penalty=1.000 ratio=1.010
12/31 05:54:38 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 05:54:38 finished saving model
12/31 06:17:25   decaying learning rate to: 0.0107
12/31 06:21:51 step 184000 epoch 76 learning rate 0.0107 step-time 0.815 loss 0.095
12/31 06:21:51 starting evaluation
12/31 06:26:46 test bleu=37.84 loss=104.17 penalty=1.000 ratio=1.007
12/31 06:26:46 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 06:26:46 finished saving model
12/31 06:53:59 step 186000 epoch 76 learning rate 0.0107 step-time 0.815 loss 0.092
12/31 06:53:59 starting evaluation
12/31 06:58:57 test bleu=37.47 loss=104.03 penalty=1.000 ratio=1.018
12/31 06:58:57 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 06:58:57 finished saving model
12/31 07:00:37   decaying learning rate to: 0.0101
12/31 07:26:09 step 188000 epoch 77 learning rate 0.0101 step-time 0.814 loss 0.086
12/31 07:26:09 starting evaluation
12/31 07:31:08 test bleu=37.28 loss=103.98 penalty=1.000 ratio=1.022
12/31 07:31:08 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 07:31:08 finished saving model
12/31 07:39:01   decaying learning rate to: 0.00963
12/31 07:58:19 step 190000 epoch 78 learning rate 0.00963 step-time 0.814 loss 0.084
12/31 07:58:19 starting evaluation
12/31 08:03:11 test bleu=38.00 loss=104.36 penalty=1.000 ratio=1.005
12/31 08:03:11 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 08:03:11 finished saving model
12/31 08:17:11   decaying learning rate to: 0.00915
12/31 08:30:27 step 192000 epoch 79 learning rate 0.00915 step-time 0.816 loss 0.085
12/31 08:30:27 starting evaluation
12/31 08:35:17 test bleu=37.53 loss=104.51 penalty=1.000 ratio=1.019
12/31 08:35:17 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 08:35:18 finished saving model
12/31 08:55:10   decaying learning rate to: 0.00869
12/31 09:02:30 step 194000 epoch 80 learning rate 0.00869 step-time 0.814 loss 0.085
12/31 09:02:30 starting evaluation
12/31 09:07:30 test bleu=37.51 loss=104.52 penalty=1.000 ratio=1.019
12/31 09:07:30 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 09:07:30 finished saving model
12/31 09:33:31   decaying learning rate to: 0.00826
12/31 09:34:38 step 196000 epoch 81 learning rate 0.00826 step-time 0.812 loss 0.085
12/31 09:34:38 starting evaluation
12/31 09:39:34 test bleu=37.39 loss=104.69 penalty=1.000 ratio=1.022
12/31 09:39:34 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 09:39:34 finished saving model
12/31 10:06:46 step 198000 epoch 81 learning rate 0.00826 step-time 0.814 loss 0.080
12/31 10:06:46 starting evaluation
12/31 10:11:42 test bleu=37.66 loss=104.60 penalty=1.000 ratio=1.017
12/31 10:11:42 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 10:11:42 finished saving model
12/31 10:16:38   decaying learning rate to: 0.00784
12/31 10:38:58 step 200000 epoch 82 learning rate 0.00784 step-time 0.816 loss 0.077
12/31 10:38:58 starting evaluation
12/31 10:43:53 test bleu=37.73 loss=104.72 penalty=1.000 ratio=1.010
12/31 10:43:53 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 10:43:53 finished saving model
12/31 10:55:07   decaying learning rate to: 0.00745
12/31 11:11:05 step 202000 epoch 83 learning rate 0.00745 step-time 0.814 loss 0.078
12/31 11:11:05 starting evaluation
12/31 11:16:02 test bleu=37.24 loss=105.17 penalty=1.000 ratio=1.023
12/31 11:16:02 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 11:16:02 finished saving model
12/31 11:33:22   decaying learning rate to: 0.00708
12/31 11:43:18 step 204000 epoch 84 learning rate 0.00708 step-time 0.816 loss 0.079
12/31 11:43:18 starting evaluation
12/31 11:48:05 test bleu=37.42 loss=105.28 penalty=1.000 ratio=1.019
12/31 11:48:05 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 11:48:05 finished saving model
12/31 12:11:28   decaying learning rate to: 0.00673
12/31 12:15:16 step 206000 epoch 85 learning rate 0.00673 step-time 0.814 loss 0.078
12/31 12:15:16 starting evaluation
12/31 12:20:10 test bleu=37.71 loss=105.14 penalty=1.000 ratio=1.013
12/31 12:20:10 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 12:20:10 finished saving model
12/31 12:47:16 step 208000 epoch 85 learning rate 0.00673 step-time 0.811 loss 0.075
12/31 12:47:16 starting evaluation
12/31 12:52:13 test bleu=37.60 loss=105.48 penalty=1.000 ratio=1.016
12/31 12:52:13 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 12:52:13 finished saving model
12/31 12:54:28   decaying learning rate to: 0.00639
12/31 13:19:27 step 210000 epoch 86 learning rate 0.00639 step-time 0.815 loss 0.072
12/31 13:19:27 starting evaluation
12/31 13:24:23 test bleu=37.61 loss=105.38 penalty=1.000 ratio=1.017
12/31 13:24:23 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 13:24:23 finished saving model
12/31 13:32:41   decaying learning rate to: 0.00607
12/31 13:51:30 step 212000 epoch 87 learning rate 0.00607 step-time 0.811 loss 0.074
12/31 13:51:30 starting evaluation
12/31 13:55:36 test bleu=37.44 loss=105.52 penalty=1.000 ratio=1.020
12/31 13:55:36 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 13:55:36 finished saving model
12/31 14:08:16   decaying learning rate to: 0.00577
12/31 14:19:23 step 214000 epoch 88 learning rate 0.00577 step-time 0.712 loss 0.071
12/31 14:19:23 starting evaluation
12/31 14:22:55 test bleu=37.62 loss=105.65 penalty=1.000 ratio=1.015
12/31 14:22:55 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 14:22:55 finished saving model
12/31 14:40:58   decaying learning rate to: 0.00548
12/31 14:46:43 step 216000 epoch 89 learning rate 0.00548 step-time 0.712 loss 0.072
12/31 14:46:43 starting evaluation
12/31 14:50:16 test bleu=37.44 loss=105.64 penalty=1.000 ratio=1.019
12/31 14:50:16 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 14:50:16 finished saving model
12/31 15:13:37   decaying learning rate to: 0.0052
12/31 15:14:05 step 218000 epoch 90 learning rate 0.0052 step-time 0.712 loss 0.073
12/31 15:14:05 starting evaluation
12/31 15:17:38 test bleu=37.33 loss=105.90 penalty=1.000 ratio=1.023
12/31 15:17:38 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 15:17:38 finished saving model
12/31 15:41:18 step 220000 epoch 90 learning rate 0.0052 step-time 0.708 loss 0.069
12/31 15:41:18 starting evaluation
12/31 15:44:51 test bleu=37.47 loss=105.96 penalty=1.000 ratio=1.019
12/31 15:44:51 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 15:44:51 finished saving model
12/31 15:49:46   decaying learning rate to: 0.00494
12/31 16:08:41 step 222000 epoch 91 learning rate 0.00494 step-time 0.713 loss 0.066
12/31 16:08:41 starting evaluation
12/31 16:12:14 test bleu=37.58 loss=105.99 penalty=1.000 ratio=1.016
12/31 16:12:14 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 16:12:14 finished saving model
12/31 16:22:26   decaying learning rate to: 0.0047
12/31 16:36:01 step 224000 epoch 92 learning rate 0.0047 step-time 0.712 loss 0.068
12/31 16:36:01 starting evaluation
12/31 16:39:35 test bleu=37.35 loss=106.12 penalty=1.000 ratio=1.022
12/31 16:39:35 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 16:39:35 finished saving model
12/31 16:55:10   decaying learning rate to: 0.00446
12/31 17:03:23 step 226000 epoch 93 learning rate 0.00446 step-time 0.712 loss 0.069
12/31 17:03:23 starting evaluation
12/31 17:06:55 test bleu=37.60 loss=106.10 penalty=1.000 ratio=1.015
12/31 17:06:55 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 17:06:56 finished saving model
12/31 17:27:45   decaying learning rate to: 0.00424
12/31 17:30:41 step 228000 epoch 94 learning rate 0.00424 step-time 0.711 loss 0.068
12/31 17:30:41 starting evaluation
12/31 17:34:14 test bleu=37.58 loss=106.28 penalty=1.000 ratio=1.017
12/31 17:34:14 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 17:34:15 finished saving model
12/31 17:58:03 step 230000 epoch 94 learning rate 0.00424 step-time 0.712 loss 0.067
12/31 17:58:03 starting evaluation
12/31 18:01:35 test bleu=37.53 loss=106.30 penalty=1.000 ratio=1.018
12/31 18:01:35 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 18:01:35 finished saving model
12/31 18:04:05   decaying learning rate to: 0.00403
12/31 18:25:27 step 232000 epoch 95 learning rate 0.00403 step-time 0.714 loss 0.065
12/31 18:25:27 starting evaluation
12/31 18:28:57 test bleu=37.64 loss=106.38 penalty=1.000 ratio=1.015
12/31 18:28:57 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 18:28:57 finished saving model
12/31 18:36:44   decaying learning rate to: 0.00383
12/31 18:52:42 step 234000 epoch 96 learning rate 0.00383 step-time 0.710 loss 0.064
12/31 18:52:42 starting evaluation
12/31 18:56:07 test bleu=37.67 loss=106.50 penalty=1.000 ratio=1.016
12/31 18:56:07 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 18:56:07 finished saving model
12/31 19:09:14   decaying learning rate to: 0.00363
12/31 19:19:58 step 236000 epoch 97 learning rate 0.00363 step-time 0.714 loss 0.065
12/31 19:19:58 starting evaluation
12/31 19:23:29 test bleu=37.37 loss=106.53 penalty=1.000 ratio=1.020
12/31 19:23:29 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 19:23:29 finished saving model
12/31 19:41:48   decaying learning rate to: 0.00345
12/31 19:47:09 step 238000 epoch 98 learning rate 0.00345 step-time 0.708 loss 0.065
12/31 19:47:09 starting evaluation
12/31 19:50:38 test bleu=37.62 loss=106.51 penalty=1.000 ratio=1.015
12/31 19:50:38 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 19:50:39 finished saving model
12/31 20:14:16 step 240000 epoch 99 learning rate 0.00345 step-time 0.707 loss 0.066
12/31 20:14:16 starting evaluation
12/31 20:17:47 test bleu=37.36 loss=106.65 penalty=1.000 ratio=1.021
12/31 20:17:47 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 20:17:47 finished saving model
12/31 20:17:49   decaying learning rate to: 0.00328
12/31 20:41:53 step 242000 epoch 99 learning rate 0.00328 step-time 0.721 loss 0.063
12/31 20:41:53 starting evaluation
12/31 20:45:29 test bleu=37.46 loss=106.81 penalty=1.000 ratio=1.018
12/31 20:45:29 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 20:45:30 finished saving model
12/31 20:50:58   decaying learning rate to: 0.00312
12/31 21:09:43 step 244000 epoch 100 learning rate 0.00312 step-time 0.725 loss 0.063
12/31 21:09:43 starting evaluation
12/31 21:13:14 test bleu=37.42 loss=106.82 penalty=1.000 ratio=1.022
12/31 21:13:14 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 21:13:14 finished saving model
12/31 21:23:56 finished training
12/31 21:23:56 exiting...
12/31 21:23:56 saving model to models/7_fold_hybrid_pnl/checkpoints
12/31 21:23:57 finished saving model
