nohup: ignoring input
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:107: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:30: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

12/28 16:13:48 label: default
12/28 16:13:48 description:
  default configuration
  next line of description
  last line
12/28 16:13:48 /root/icpc/icpc/translate/__main__.py config/10-folds/7_fold/codenn/config.yaml --train -v
12/28 16:13:48 commit hash 74e0554cb3eb5df835cef993ad570ff8de651f71
12/28 16:13:48 tensorflow version: 1.14.0
12/28 16:13:48 program arguments
12/28 16:13:48   aggregation_method   'sum'
12/28 16:13:48   align_encoder_id     0
12/28 16:13:48   allow_growth         True
12/28 16:13:48   attention_type       'global'
12/28 16:13:48   attn_filter_length   0
12/28 16:13:48   attn_filters         0
12/28 16:13:48   attn_prev_word       False
12/28 16:13:48   attn_size            128
12/28 16:13:48   attn_temperature     1.0
12/28 16:13:48   attn_window_size     0
12/28 16:13:48   average              False
12/28 16:13:48   baseline_activation  None
12/28 16:13:48   baseline_learning_rate 0.001
12/28 16:13:48   baseline_optimizer   'adam'
12/28 16:13:48   baseline_steps       0
12/28 16:13:48   batch_mode           'standard'
12/28 16:13:48   batch_size           64
12/28 16:13:48   beam_size            5
12/28 16:13:48   bidir                True
12/28 16:13:48   bidir_projection     False
12/28 16:13:48   binary               False
12/28 16:13:48   cell_size            256
12/28 16:13:48   cell_type            'GRU'
12/28 16:13:48   character_level      False
12/28 16:13:48   checkpoints          []
12/28 16:13:48   conditional_rnn      False
12/28 16:13:48   config               'config/10-folds/7_fold/codenn/config.yaml'
12/28 16:13:48   convolutions         None
12/28 16:13:48   data_dir             'data/gooddata/7_fold'
12/28 16:13:48   debug                False
12/28 16:13:48   decay_after_n_epoch  1
12/28 16:13:48   decay_every_n_epoch  1
12/28 16:13:48   decay_if_no_progress None
12/28 16:13:48   decoders             [{'max_len': 40, 'name': 'nl'}]
12/28 16:13:48   description          'default configuration\nnext line of description\nlast line\n'
12/28 16:13:48   dev_prefix           'test'
12/28 16:13:48   early_stopping       True
12/28 16:13:48   embedding_dropout    0.0
12/28 16:13:48   embedding_initializer None
12/28 16:13:48   embedding_size       256
12/28 16:13:48   embedding_weight_scale None
12/28 16:13:48   embeddings_on_cpu    True
12/28 16:13:48   encoders             [{'attention_type': 'global', 'max_len': 200, 'name': 'code'}]
12/28 16:13:48   ensemble             False
12/28 16:13:48   eval_burn_in         0
12/28 16:13:48   feed_previous        0.0
12/28 16:13:48   final_state          'last'
12/28 16:13:48   freeze_variables     []
12/28 16:13:48   generate_first       True
12/28 16:13:48   gpu_id               2
12/28 16:13:48   highway_layers       0
12/28 16:13:48   initial_state_dropout 0.0
12/28 16:13:48   initializer          None
12/28 16:13:48   input_layer_dropout  0.0
12/28 16:13:48   input_layers         None
12/28 16:13:48   keep_best            5
12/28 16:13:48   keep_every_n_hours   0
12/28 16:13:48   label                'default'
12/28 16:13:48   layer_norm           False
12/28 16:13:48   layers               1
12/28 16:13:48   learning_rate        0.5
12/28 16:13:48   learning_rate_decay_factor 0.95
12/28 16:13:48   len_normalization    1.0
12/28 16:13:48   log_file             'log.txt'
12/28 16:13:48   loss_function        'xent'
12/28 16:13:48   max_dev_size         0
12/28 16:13:48   max_epochs           100
12/28 16:13:48   max_gradient_norm    5.0
12/28 16:13:48   max_len              50
12/28 16:13:48   max_steps            600000
12/28 16:13:48   max_test_size        0
12/28 16:13:48   max_to_keep          1
12/28 16:13:48   max_train_size       0
12/28 16:13:48   maxout_stride        None
12/28 16:13:48   mem_fraction         1.0
12/28 16:13:48   min_learning_rate    1e-06
12/28 16:13:48   model_dir            'models/7_fold_codenn'
12/28 16:13:48   moving_average       None
12/28 16:13:48   no_gpu               False
12/28 16:13:48   optimizer            'sgd'
12/28 16:13:48   orthogonal_init      False
12/28 16:13:48   output               None
12/28 16:13:48   output_dropout       0.0
12/28 16:13:48   parallel_iterations  16
12/28 16:13:48   pervasive_dropout    False
12/28 16:13:48   pooling_avg          True
12/28 16:13:48   post_process_script  None
12/28 16:13:48   pred_deep_layer      False
12/28 16:13:48   pred_edits           False
12/28 16:13:48   pred_embed_proj      True
12/28 16:13:48   pred_maxout_layer    True
12/28 16:13:48   purge                False
12/28 16:13:48   raw_output           False
12/28 16:13:48   read_ahead           1
12/28 16:13:48   reconstruction_attn_weight 0.05
12/28 16:13:48   reconstruction_decoders False
12/28 16:13:48   reconstruction_weight 1.0
12/28 16:13:48   reinforce_after_n_epoch None
12/28 16:13:48   remove_unk           False
12/28 16:13:48   reverse              False
12/28 16:13:48   reverse_input        True
12/28 16:13:48   reward_function      'sentence_bleu'
12/28 16:13:48   rnn_feed_attn        True
12/28 16:13:48   rnn_input_dropout    0.0
12/28 16:13:48   rnn_output_dropout   0.0
12/28 16:13:48   rnn_state_dropout    0.0
12/28 16:13:48   save                 False
12/28 16:13:48   score_function       'corpus_bleu'
12/28 16:13:48   score_functions      ['bleu', 'loss']
12/28 16:13:48   script_dir           'scripts'
12/28 16:13:48   sgd_after_n_epoch    None
12/28 16:13:48   sgd_learning_rate    1.0
12/28 16:13:48   shuffle              True
12/28 16:13:48   softmax_temperature  1.0
12/28 16:13:48   steps_per_checkpoint 2000
12/28 16:13:48   steps_per_eval       2000
12/28 16:13:48   swap_memory          True
12/28 16:13:48   tie_embeddings       False
12/28 16:13:48   time_pooling         None
12/28 16:13:48   train                True
12/28 16:13:48   train_initial_states True
12/28 16:13:48   train_prefix         'train'
12/28 16:13:48   truncate_lines       True
12/28 16:13:48   update_first         False
12/28 16:13:48   use_baseline         False
12/28 16:13:48   use_dropout          False
12/28 16:13:48   use_lstm_full_state  False
12/28 16:13:48   use_previous_word    True
12/28 16:13:48   verbose              True
12/28 16:13:48   vocab_prefix         'vocab'
12/28 16:13:48   weight_scale         None
12/28 16:13:48   word_dropout         0.0
12/28 16:13:48 python random seed: 4796557587214309158
12/28 16:13:48 tf random seed:     7503699054160391259
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:203: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

12/28 16:13:48 creating model
12/28 16:13:48 using device: /gpu:2
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:230: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

12/28 16:13:48 copying vocab to models/7_fold_codenn/data/vocab.code
12/28 16:13:48 copying vocab to models/7_fold_codenn/data/vocab.nl
12/28 16:13:48 reading vocabularies
12/28 16:13:48 creating model
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:60: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:111: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:33: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ff51e284470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ff51e284470>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:226: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ff51e2847b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ff51e2847b8>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff51e279e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff51e279e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:838: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff5a7040860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff5a7040860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff5a70402b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff5a70402b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:432: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:435: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff5a6ecefd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff5a6ecefd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff5a6e85b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff5a6e85b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff5a6edfa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff5a6edfa20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff5a6edfa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff5a6edfa20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff5a6edfa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff5a6edfa20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:919: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ff5a6b8e550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ff5a6b8e550>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:131: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:223: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ff54b725fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7ff54b725fd0>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff54b65ba90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff54b65ba90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff54b65ba90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff54b65ba90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff54b601470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff54b601470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff54b690e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff54b690e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff54b690e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff54b690e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
12/28 16:13:52 model parameters (30)
12/28 16:13:52   baseline_step:0 ()
12/28 16:13:52   decoder_nl/attention_code/U_a/kernel:0 (512, 128)
12/28 16:13:52   decoder_nl/attention_code/W_a/bias:0 (128,)
12/28 16:13:52   decoder_nl/attention_code/W_a/kernel:0 (256, 128)
12/28 16:13:52   decoder_nl/attention_code/v_a:0 (128,)
12/28 16:13:52   decoder_nl/code/initial_state_projection/bias:0 (256,)
12/28 16:13:52   decoder_nl/code/initial_state_projection/kernel:0 (256, 256)
12/28 16:13:52   decoder_nl/gru_cell/candidate/bias:0 (256,)
12/28 16:13:52   decoder_nl/gru_cell/candidate/kernel:0 (1024, 256)
12/28 16:13:52   decoder_nl/gru_cell/gates/bias:0 (512,)
12/28 16:13:52   decoder_nl/gru_cell/gates/kernel:0 (1024, 512)
12/28 16:13:52   decoder_nl/maxout/bias:0 (256,)
12/28 16:13:52   decoder_nl/maxout/kernel:0 (1024, 256)
12/28 16:13:52   decoder_nl/softmax0/kernel:0 (128, 256)
12/28 16:13:52   decoder_nl/softmax1/bias:0 (38019,)
12/28 16:13:52   decoder_nl/softmax1/kernel:0 (256, 38019)
12/28 16:13:52   embedding_code:0 (50000, 256)
12/28 16:13:52   embedding_nl:0 (38019, 256)
12/28 16:13:52   encoder_code/initial_state_bw:0 (256,)
12/28 16:13:52   encoder_code/initial_state_fw:0 (256,)
12/28 16:13:52   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/28 16:13:52   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/28 16:13:52   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/28 16:13:52   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/28 16:13:52   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/28 16:13:52   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/28 16:13:52   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/28 16:13:52   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/28 16:13:52   global_step:0 ()
12/28 16:13:52   learning_rate:0 ()
12/28 16:13:52 number of parameters: 34.34M
WARNING:tensorflow:From /root/icpc/icpc/translate/translation_model.py:666: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

12/28 16:13:54 global step: 0
12/28 16:13:54 baseline step: 0
12/28 16:13:54 reading training data
12/28 16:13:54 total line count: 156721
12/28 16:13:57   lines read: 100000
12/28 16:14:00 files: data/gooddata/7_fold/train.code data/gooddata/7_fold/train.nl
12/28 16:14:00 lines reads: 156721
12/28 16:14:00 reading development data
12/28 16:14:01 files: data/gooddata/7_fold/test.code data/gooddata/7_fold/test.nl
12/28 16:14:01 lines reads: 17413
12/28 16:14:01 starting training
12/28 16:36:41 step 2000 epoch 1 learning rate 0.5 step-time 0.678 loss 78.507
12/28 16:36:41 starting evaluation
12/28 16:40:53 test bleu=1.54 loss=63.39 penalty=0.875 ratio=0.882
12/28 16:40:53 saving model to models/7_fold_codenn/checkpoints
12/28 16:40:54 finished saving model
12/28 16:40:54 new best model
12/28 16:45:59   decaying learning rate to: 0.475
12/28 17:03:28 step 4000 epoch 2 learning rate 0.475 step-time 0.675 loss 59.045
12/28 17:03:28 starting evaluation
12/28 17:07:26 test bleu=4.83 loss=55.74 penalty=0.760 ratio=0.784
12/28 17:07:26 saving model to models/7_fold_codenn/checkpoints
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
12/28 17:07:26 finished saving model
12/28 17:07:26 new best model
12/28 17:17:25   decaying learning rate to: 0.451
12/28 17:30:02 step 6000 epoch 3 learning rate 0.451 step-time 0.676 loss 52.472
12/28 17:30:02 starting evaluation
12/28 17:33:33 test bleu=6.28 loss=50.79 penalty=0.696 ratio=0.734
12/28 17:33:33 saving model to models/7_fold_codenn/checkpoints
12/28 17:33:33 finished saving model
12/28 17:33:33 new best model
12/28 17:48:41   decaying learning rate to: 0.429
12/28 17:56:01 step 8000 epoch 4 learning rate 0.429 step-time 0.672 loss 47.849
12/28 17:56:01 starting evaluation
12/28 18:00:08 test bleu=9.74 loss=47.05 penalty=0.930 ratio=0.932
12/28 18:00:08 saving model to models/7_fold_codenn/checkpoints
12/28 18:00:08 finished saving model
12/28 18:00:08 new best model
12/28 18:19:40   decaying learning rate to: 0.407
12/28 18:21:55 step 10000 epoch 5 learning rate 0.407 step-time 0.652 loss 44.349
12/28 18:21:55 starting evaluation
12/28 18:25:58 test bleu=11.19 loss=45.08 penalty=0.866 ratio=0.874
12/28 18:25:58 saving model to models/7_fold_codenn/checkpoints
12/28 18:25:58 finished saving model
12/28 18:25:58 new best model
12/28 18:47:39 step 12000 epoch 5 learning rate 0.407 step-time 0.649 loss 41.052
12/28 18:47:39 starting evaluation
12/28 18:51:47 test bleu=12.75 loss=43.03 penalty=1.000 ratio=1.087
12/28 18:51:47 saving model to models/7_fold_codenn/checkpoints
12/28 18:51:47 finished saving model
12/28 18:51:47 new best model
12/28 18:54:29   decaying learning rate to: 0.387
12/28 19:13:26 step 14000 epoch 6 learning rate 0.387 step-time 0.648 loss 37.915
12/28 19:13:26 starting evaluation
12/28 19:17:34 test bleu=15.19 loss=41.43 penalty=1.000 ratio=1.032
12/28 19:17:34 saving model to models/7_fold_codenn/checkpoints
12/28 19:17:34 finished saving model
12/28 19:17:34 new best model
12/28 19:25:04   decaying learning rate to: 0.368
12/28 19:39:17 step 16000 epoch 7 learning rate 0.368 step-time 0.649 loss 35.574
12/28 19:39:17 starting evaluation
12/28 19:42:58 test bleu=17.41 loss=40.49 penalty=0.821 ratio=0.835
12/28 19:42:58 saving model to models/7_fold_codenn/checkpoints
12/28 19:42:58 finished saving model
12/28 19:42:58 new best model
12/28 19:55:12   decaying learning rate to: 0.349
12/28 20:04:40 step 18000 epoch 8 learning rate 0.349 step-time 0.649 loss 33.329
12/28 20:04:40 starting evaluation
12/28 20:08:47 test bleu=18.52 loss=40.33 penalty=1.000 ratio=1.011
12/28 20:08:47 saving model to models/7_fold_codenn/checkpoints
12/28 20:08:47 finished saving model
12/28 20:08:47 new best model
12/28 20:25:58   decaying learning rate to: 0.332
12/28 20:30:28 step 20000 epoch 9 learning rate 0.332 step-time 0.648 loss 31.325
12/28 20:30:28 starting evaluation
12/28 20:34:32 test bleu=21.27 loss=39.20 penalty=0.948 ratio=0.950
12/28 20:34:32 saving model to models/7_fold_codenn/checkpoints
12/28 20:34:32 finished saving model
12/28 20:34:32 new best model
12/28 20:56:11 step 22000 epoch 9 learning rate 0.332 step-time 0.647 loss 29.461
12/28 20:56:11 starting evaluation
12/28 21:00:12 test bleu=21.74 loss=38.54 penalty=0.926 ratio=0.928
12/28 21:00:12 saving model to models/7_fold_codenn/checkpoints
12/28 21:00:12 finished saving model
12/28 21:00:12 new best model
12/28 21:00:39   decaying learning rate to: 0.315
12/28 21:21:53 step 24000 epoch 10 learning rate 0.315 step-time 0.649 loss 26.907
12/28 21:21:53 starting evaluation
12/28 21:25:42 test bleu=22.37 loss=38.89 penalty=0.830 ratio=0.843
12/28 21:25:42 saving model to models/7_fold_codenn/checkpoints
12/28 21:25:43 finished saving model
12/28 21:25:43 new best model
12/28 21:31:08   decaying learning rate to: 0.299
12/28 21:47:23 step 26000 epoch 11 learning rate 0.299 step-time 0.648 loss 25.291
12/28 21:47:23 starting evaluation
12/28 21:51:18 test bleu=23.30 loss=39.17 penalty=0.886 ratio=0.892
12/28 21:51:18 saving model to models/7_fold_codenn/checkpoints
12/28 21:51:18 finished saving model
12/28 21:51:18 new best model
12/28 22:01:24   decaying learning rate to: 0.284
12/28 22:12:58 step 28000 epoch 12 learning rate 0.284 step-time 0.648 loss 23.602
12/28 22:12:58 starting evaluation
12/28 22:16:51 test bleu=24.29 loss=39.65 penalty=0.881 ratio=0.888
12/28 22:16:51 saving model to models/7_fold_codenn/checkpoints
12/28 22:16:51 finished saving model
12/28 22:16:51 new best model
12/28 22:31:45   decaying learning rate to: 0.27
12/28 22:38:30 step 30000 epoch 13 learning rate 0.27 step-time 0.648 loss 22.226
12/28 22:38:30 starting evaluation
12/28 22:42:31 test bleu=26.12 loss=40.67 penalty=0.961 ratio=0.962
12/28 22:42:31 saving model to models/7_fold_codenn/checkpoints
12/28 22:42:31 finished saving model
12/28 22:42:31 new best model
12/28 23:02:23   decaying learning rate to: 0.257
12/28 23:04:10 step 32000 epoch 14 learning rate 0.257 step-time 0.648 loss 20.903
12/28 23:04:10 starting evaluation
12/28 23:08:16 test bleu=26.30 loss=40.99 penalty=1.000 ratio=1.002
12/28 23:08:16 saving model to models/7_fold_codenn/checkpoints
12/28 23:08:16 finished saving model
12/28 23:08:16 new best model
12/28 23:29:52 step 34000 epoch 14 learning rate 0.257 step-time 0.646 loss 19.045
12/28 23:29:52 starting evaluation
12/28 23:33:54 test bleu=27.30 loss=40.23 penalty=0.926 ratio=0.929
12/28 23:33:54 saving model to models/7_fold_codenn/checkpoints
12/28 23:33:54 finished saving model
12/28 23:33:54 new best model
12/28 23:37:04   decaying learning rate to: 0.244
12/28 23:55:36 step 36000 epoch 15 learning rate 0.244 step-time 0.649 loss 17.581
12/28 23:55:36 starting evaluation
12/28 23:59:34 test bleu=27.66 loss=41.60 penalty=0.899 ratio=0.903
12/28 23:59:34 saving model to models/7_fold_codenn/checkpoints
12/28 23:59:34 finished saving model
12/28 23:59:34 new best model
12/29 00:07:41   decaying learning rate to: 0.232
12/29 00:21:12 step 38000 epoch 16 learning rate 0.232 step-time 0.647 loss 16.243
12/29 00:21:12 starting evaluation
12/29 00:25:09 test bleu=28.05 loss=43.26 penalty=0.913 ratio=0.917
12/29 00:25:09 saving model to models/7_fold_codenn/checkpoints
12/29 00:25:09 finished saving model
12/29 00:25:09 new best model
12/29 00:37:51   decaying learning rate to: 0.22
12/29 00:46:49 step 40000 epoch 17 learning rate 0.22 step-time 0.648 loss 15.293
12/29 00:46:49 starting evaluation
12/29 00:50:51 test bleu=29.03 loss=44.55 penalty=0.948 ratio=0.949
12/29 00:50:51 saving model to models/7_fold_codenn/checkpoints
12/29 00:50:51 finished saving model
12/29 00:50:51 new best model
12/29 01:08:29   decaying learning rate to: 0.209
12/29 01:12:33 step 42000 epoch 18 learning rate 0.209 step-time 0.649 loss 14.338
12/29 01:12:33 starting evaluation
12/29 01:16:35 test bleu=29.27 loss=45.29 penalty=0.981 ratio=0.981
12/29 01:16:35 saving model to models/7_fold_codenn/checkpoints
12/29 01:16:36 finished saving model
12/29 01:16:36 new best model
12/29 01:38:14 step 44000 epoch 18 learning rate 0.209 step-time 0.647 loss 13.230
12/29 01:38:14 starting evaluation
12/29 01:42:16 test bleu=29.55 loss=44.79 penalty=0.949 ratio=0.950
12/29 01:42:16 saving model to models/7_fold_codenn/checkpoints
12/29 01:42:16 finished saving model
12/29 01:42:16 new best model
12/29 01:43:11   decaying learning rate to: 0.199
12/29 02:04:19 step 46000 epoch 19 learning rate 0.199 step-time 0.659 loss 11.744
12/29 02:04:19 starting evaluation
12/29 02:08:23 test bleu=30.12 loss=46.83 penalty=0.959 ratio=0.960
12/29 02:08:23 saving model to models/7_fold_codenn/checkpoints
12/29 02:08:23 finished saving model
12/29 02:08:23 new best model
12/29 02:14:36   decaying learning rate to: 0.189
12/29 02:31:35 step 48000 epoch 20 learning rate 0.189 step-time 0.694 loss 10.916
12/29 02:31:35 starting evaluation
12/29 02:35:44 test bleu=30.52 loss=48.68 penalty=0.982 ratio=0.982
12/29 02:35:44 saving model to models/7_fold_codenn/checkpoints
12/29 02:35:44 finished saving model
12/29 02:35:44 new best model
12/29 02:47:16   decaying learning rate to: 0.179
12/29 02:59:05 step 50000 epoch 21 learning rate 0.179 step-time 0.698 loss 10.188
12/29 02:59:05 starting evaluation
12/29 03:03:14 test bleu=30.86 loss=50.55 penalty=0.987 ratio=0.987
12/29 03:03:14 saving model to models/7_fold_codenn/checkpoints
12/29 03:03:14 finished saving model
12/29 03:03:14 new best model
12/29 03:19:37   decaying learning rate to: 0.17
12/29 03:26:22 step 52000 epoch 22 learning rate 0.17 step-time 0.691 loss 9.473
12/29 03:26:22 starting evaluation
12/29 03:30:28 test bleu=30.88 loss=52.70 penalty=0.961 ratio=0.962
12/29 03:30:28 saving model to models/7_fold_codenn/checkpoints
12/29 03:30:29 finished saving model
12/29 03:30:29 new best model
12/29 03:50:49   decaying learning rate to: 0.162
12/29 03:52:10 step 54000 epoch 23 learning rate 0.162 step-time 0.649 loss 8.836
12/29 03:52:10 starting evaluation
12/29 03:56:13 test bleu=31.29 loss=53.81 penalty=0.995 ratio=0.995
12/29 03:56:13 saving model to models/7_fold_codenn/checkpoints
12/29 03:56:13 finished saving model
12/29 03:56:13 new best model
12/29 04:17:54 step 56000 epoch 23 learning rate 0.162 step-time 0.649 loss 7.803
12/29 04:17:54 starting evaluation
12/29 04:21:59 test bleu=31.63 loss=54.57 penalty=0.990 ratio=0.990
12/29 04:21:59 saving model to models/7_fold_codenn/checkpoints
12/29 04:21:59 finished saving model
12/29 04:21:59 new best model
12/29 04:25:35   decaying learning rate to: 0.154
12/29 04:44:26 step 58000 epoch 24 learning rate 0.154 step-time 0.672 loss 7.118
12/29 04:44:26 starting evaluation
12/29 04:48:35 test bleu=31.89 loss=56.74 penalty=0.961 ratio=0.962
12/29 04:48:35 saving model to models/7_fold_codenn/checkpoints
12/29 04:48:35 finished saving model
12/29 04:48:35 new best model
12/29 04:57:32   decaying learning rate to: 0.146
12/29 05:11:19 step 60000 epoch 25 learning rate 0.146 step-time 0.680 loss 6.607
12/29 05:11:19 starting evaluation
12/29 05:15:27 test bleu=32.11 loss=60.14 penalty=0.969 ratio=0.970
12/29 05:15:27 saving model to models/7_fold_codenn/checkpoints
12/29 05:15:27 finished saving model
12/29 05:15:27 new best model
12/29 05:29:19   decaying learning rate to: 0.139
12/29 05:38:08 step 62000 epoch 26 learning rate 0.139 step-time 0.678 loss 6.192
12/29 05:38:08 starting evaluation
12/29 05:42:16 test bleu=32.19 loss=61.59 penalty=0.991 ratio=0.991
12/29 05:42:16 saving model to models/7_fold_codenn/checkpoints
12/29 05:42:17 finished saving model
12/29 05:42:17 new best model
12/29 06:01:13   decaying learning rate to: 0.132
12/29 06:04:56 step 64000 epoch 27 learning rate 0.132 step-time 0.678 loss 5.692
12/29 06:04:56 starting evaluation
12/29 06:09:06 test bleu=31.42 loss=63.99 penalty=1.000 ratio=1.031
12/29 06:09:06 saving model to models/7_fold_codenn/checkpoints
12/29 06:09:06 finished saving model
12/29 06:31:47 step 66000 epoch 27 learning rate 0.132 step-time 0.678 loss 5.240
12/29 06:31:47 starting evaluation
12/29 06:35:58 test bleu=31.89 loss=64.03 penalty=1.000 ratio=1.022
12/29 06:35:58 saving model to models/7_fold_codenn/checkpoints
12/29 06:35:58 finished saving model
12/29 06:37:22   decaying learning rate to: 0.125
12/29 06:58:37 step 68000 epoch 28 learning rate 0.125 step-time 0.677 loss 4.582
12/29 06:58:37 starting evaluation
12/29 07:02:49 test bleu=31.65 loss=66.74 penalty=1.000 ratio=1.033
12/29 07:02:49 saving model to models/7_fold_codenn/checkpoints
12/29 07:02:49 finished saving model
12/29 07:09:14   decaying learning rate to: 0.119
12/29 07:25:02 step 70000 epoch 29 learning rate 0.119 step-time 0.665 loss 4.259
12/29 07:25:02 starting evaluation
12/29 07:29:12 test bleu=31.99 loss=69.52 penalty=1.000 ratio=1.030
12/29 07:29:12 saving model to models/7_fold_codenn/checkpoints
12/29 07:29:12 finished saving model
12/29 07:40:43   decaying learning rate to: 0.113
12/29 07:51:29 step 72000 epoch 30 learning rate 0.113 step-time 0.666 loss 3.974
12/29 07:51:29 starting evaluation
12/29 07:55:39 test bleu=30.98 loss=71.36 penalty=1.000 ratio=1.063
12/29 07:55:39 saving model to models/7_fold_codenn/checkpoints
12/29 07:55:39 finished saving model
12/29 08:11:44   decaying learning rate to: 0.107
12/29 08:17:38 step 74000 epoch 31 learning rate 0.107 step-time 0.658 loss 3.689
12/29 08:17:38 starting evaluation
12/29 08:21:44 test bleu=31.48 loss=73.58 penalty=1.000 ratio=1.054
12/29 08:21:44 saving model to models/7_fold_codenn/checkpoints
12/29 08:21:44 finished saving model
12/29 08:42:38   decaying learning rate to: 0.102
12/29 08:43:30 step 76000 epoch 32 learning rate 0.102 step-time 0.651 loss 3.406
12/29 08:43:30 starting evaluation
12/29 08:47:14 test bleu=31.89 loss=75.11 penalty=1.000 ratio=1.049
12/29 08:47:14 saving model to models/7_fold_codenn/checkpoints
12/29 08:47:14 finished saving model
12/29 09:04:49 step 78000 epoch 32 learning rate 0.102 step-time 0.525 loss 2.992
12/29 09:04:49 starting evaluation
12/29 09:07:40 test bleu=33.11 loss=76.55 penalty=1.000 ratio=1.007
12/29 09:07:40 saving model to models/7_fold_codenn/checkpoints
12/29 09:07:40 finished saving model
12/29 09:07:40 new best model
12/29 09:10:52   decaying learning rate to: 0.0969
12/29 09:25:11 step 80000 epoch 33 learning rate 0.0969 step-time 0.523 loss 2.759
12/29 09:25:11 starting evaluation
12/29 09:27:56 test bleu=33.20 loss=79.18 penalty=1.000 ratio=1.012
12/29 09:27:56 saving model to models/7_fold_codenn/checkpoints
12/29 09:27:57 finished saving model
12/29 09:27:57 new best model
12/29 09:35:03   decaying learning rate to: 0.092
12/29 09:45:17 step 82000 epoch 34 learning rate 0.092 step-time 0.518 loss 2.561
12/29 09:45:17 starting evaluation
12/29 09:48:04 test bleu=32.43 loss=81.49 penalty=1.000 ratio=1.030
12/29 09:48:04 saving model to models/7_fold_codenn/checkpoints
12/29 09:48:04 finished saving model
12/29 09:59:09   decaying learning rate to: 0.0874
12/29 10:05:21 step 84000 epoch 35 learning rate 0.0874 step-time 0.516 loss 2.379
12/29 10:05:21 starting evaluation
12/29 10:09:34 test bleu=33.57 loss=83.12 penalty=1.000 ratio=1.003
12/29 10:09:34 saving model to models/7_fold_codenn/checkpoints
12/29 10:09:34 finished saving model
12/29 10:09:34 new best model
12/29 10:30:34   decaying learning rate to: 0.083
12/29 10:33:47 step 86000 epoch 36 learning rate 0.083 step-time 0.724 loss 2.224
12/29 10:33:47 starting evaluation
12/29 10:38:05 test bleu=32.70 loss=85.58 penalty=1.000 ratio=1.032
12/29 10:38:05 saving model to models/7_fold_codenn/checkpoints
12/29 10:38:05 finished saving model
12/29 11:02:36 step 88000 epoch 36 learning rate 0.083 step-time 0.733 loss 2.015
12/29 11:02:36 starting evaluation
12/29 11:06:32 test bleu=32.37 loss=87.00 penalty=1.000 ratio=1.044
12/29 11:06:32 saving model to models/7_fold_codenn/checkpoints
12/29 11:06:32 finished saving model
12/29 11:08:18   decaying learning rate to: 0.0789
12/29 11:30:46 step 90000 epoch 37 learning rate 0.0789 step-time 0.725 loss 1.796
12/29 11:30:46 starting evaluation
12/29 11:35:02 test bleu=32.35 loss=87.82 penalty=1.000 ratio=1.049
12/29 11:35:02 saving model to models/7_fold_codenn/checkpoints
12/29 11:35:02 finished saving model
12/29 11:41:57   decaying learning rate to: 0.0749
12/29 11:58:58 step 92000 epoch 38 learning rate 0.0749 step-time 0.716 loss 1.682
12/29 11:58:58 starting evaluation
12/29 12:03:14 test bleu=33.11 loss=90.99 penalty=1.000 ratio=1.023
12/29 12:03:14 saving model to models/7_fold_codenn/checkpoints
12/29 12:03:14 finished saving model
12/29 12:15:45   decaying learning rate to: 0.0712
12/29 12:27:13 step 94000 epoch 39 learning rate 0.0712 step-time 0.717 loss 1.559
12/29 12:27:13 starting evaluation
12/29 12:31:28 test bleu=32.67 loss=92.51 penalty=1.000 ratio=1.043
12/29 12:31:28 saving model to models/7_fold_codenn/checkpoints
12/29 12:31:28 finished saving model
12/29 12:49:33   decaying learning rate to: 0.0676
12/29 12:55:32 step 96000 epoch 40 learning rate 0.0676 step-time 0.720 loss 1.473
12/29 12:55:32 starting evaluation
12/29 12:59:48 test bleu=33.27 loss=94.57 penalty=1.000 ratio=1.025
12/29 12:59:48 saving model to models/7_fold_codenn/checkpoints
12/29 12:59:48 finished saving model
12/29 13:23:17   decaying learning rate to: 0.0643
12/29 13:23:47 step 98000 epoch 41 learning rate 0.0643 step-time 0.717 loss 1.381
12/29 13:23:47 starting evaluation
12/29 13:28:02 test bleu=33.01 loss=96.21 penalty=1.000 ratio=1.033
12/29 13:28:02 saving model to models/7_fold_codenn/checkpoints
12/29 13:28:02 finished saving model
12/29 13:51:52 step 100000 epoch 41 learning rate 0.0643 step-time 0.713 loss 1.205
12/29 13:51:52 starting evaluation
12/29 13:56:09 test bleu=32.27 loss=97.01 penalty=1.000 ratio=1.062
12/29 13:56:09 saving model to models/7_fold_codenn/checkpoints
12/29 13:56:09 finished saving model
12/29 14:01:11   decaying learning rate to: 0.061
12/29 14:20:13 step 102000 epoch 42 learning rate 0.061 step-time 0.720 loss 1.123
12/29 14:20:13 starting evaluation
12/29 14:24:28 test bleu=33.10 loss=99.30 penalty=1.000 ratio=1.037
12/29 14:24:28 saving model to models/7_fold_codenn/checkpoints
12/29 14:24:28 finished saving model
12/29 14:34:58   decaying learning rate to: 0.058
12/29 14:48:40 step 104000 epoch 43 learning rate 0.058 step-time 0.724 loss 1.072
12/29 14:48:40 starting evaluation
12/29 14:52:54 test bleu=32.84 loss=101.38 penalty=1.000 ratio=1.042
12/29 14:52:54 saving model to models/7_fold_codenn/checkpoints
12/29 14:52:55 finished saving model
12/29 15:09:00   decaying learning rate to: 0.0551
12/29 15:17:30 step 106000 epoch 44 learning rate 0.0551 step-time 0.736 loss 1.006
12/29 15:17:30 starting evaluation
12/29 15:21:27 test bleu=33.49 loss=102.63 penalty=1.000 ratio=1.025
12/29 15:21:27 saving model to models/7_fold_codenn/checkpoints
12/29 15:21:27 finished saving model
12/29 15:42:39   decaying learning rate to: 0.0523
12/29 15:45:38 step 108000 epoch 45 learning rate 0.0523 step-time 0.723 loss 0.952
12/29 15:45:38 starting evaluation
12/29 15:49:52 test bleu=33.67 loss=103.79 penalty=1.000 ratio=1.020
12/29 15:49:52 saving model to models/7_fold_codenn/checkpoints
12/29 15:49:52 finished saving model
12/29 15:49:52 new best model
12/29 16:13:51 step 110000 epoch 45 learning rate 0.0523 step-time 0.717 loss 0.878
12/29 16:13:51 starting evaluation
12/29 16:18:06 test bleu=32.86 loss=105.39 penalty=1.000 ratio=1.041
12/29 16:18:06 saving model to models/7_fold_codenn/checkpoints
12/29 16:18:06 finished saving model
12/29 16:20:37   decaying learning rate to: 0.0497
12/29 16:41:57 step 112000 epoch 46 learning rate 0.0497 step-time 0.713 loss 0.800
12/29 16:41:57 starting evaluation
12/29 16:46:13 test bleu=32.77 loss=106.33 penalty=1.000 ratio=1.049
12/29 16:46:13 saving model to models/7_fold_codenn/checkpoints
12/29 16:46:13 finished saving model
12/29 16:54:13   decaying learning rate to: 0.0472
12/29 17:10:11 step 114000 epoch 47 learning rate 0.0472 step-time 0.717 loss 0.763
12/29 17:10:11 starting evaluation
12/29 17:14:28 test bleu=32.94 loss=107.96 penalty=1.000 ratio=1.048
12/29 17:14:28 saving model to models/7_fold_codenn/checkpoints
12/29 17:14:29 finished saving model
12/29 17:27:52   decaying learning rate to: 0.0449
12/29 17:38:25 step 116000 epoch 48 learning rate 0.0449 step-time 0.716 loss 0.721
12/29 17:38:25 starting evaluation
12/29 17:42:42 test bleu=32.84 loss=109.38 penalty=1.000 ratio=1.043
12/29 17:42:42 saving model to models/7_fold_codenn/checkpoints
12/29 17:42:42 finished saving model
12/29 18:01:24   decaying learning rate to: 0.0426
12/29 18:06:38 step 118000 epoch 49 learning rate 0.0426 step-time 0.716 loss 0.699
12/29 18:06:38 starting evaluation
12/29 18:10:55 test bleu=32.26 loss=110.14 penalty=1.000 ratio=1.061
12/29 18:10:55 saving model to models/7_fold_codenn/checkpoints
12/29 18:10:55 finished saving model
12/29 18:34:57 step 120000 epoch 50 learning rate 0.0426 step-time 0.719 loss 0.664
12/29 18:34:57 starting evaluation
12/29 18:39:14 test bleu=32.15 loss=111.22 penalty=1.000 ratio=1.069
12/29 18:39:14 saving model to models/7_fold_codenn/checkpoints
12/29 18:39:15 finished saving model
12/29 18:39:15   decaying learning rate to: 0.0405
12/29 19:03:35 step 122000 epoch 50 learning rate 0.0405 step-time 0.728 loss 0.601
12/29 19:03:35 starting evaluation
12/29 19:07:44 test bleu=32.43 loss=111.68 penalty=1.000 ratio=1.060
12/29 19:07:44 saving model to models/7_fold_codenn/checkpoints
12/29 19:07:44 finished saving model
12/29 19:13:14   decaying learning rate to: 0.0385
12/29 19:32:16 step 124000 epoch 51 learning rate 0.0385 step-time 0.734 loss 0.573
12/29 19:32:16 starting evaluation
12/29 19:36:17 test bleu=32.69 loss=112.45 penalty=1.000 ratio=1.056
12/29 19:36:17 saving model to models/7_fold_codenn/checkpoints
12/29 19:36:17 finished saving model
12/29 19:46:57   decaying learning rate to: 0.0365
12/29 20:00:24 step 126000 epoch 52 learning rate 0.0365 step-time 0.721 loss 0.550
12/29 20:00:24 starting evaluation
12/29 20:04:40 test bleu=32.58 loss=113.65 penalty=1.000 ratio=1.056
12/29 20:04:40 saving model to models/7_fold_codenn/checkpoints
12/29 20:04:40 finished saving model
12/29 20:20:34   decaying learning rate to: 0.0347
12/29 20:28:34 step 128000 epoch 53 learning rate 0.0347 step-time 0.715 loss 0.532
12/29 20:28:34 starting evaluation
12/29 20:32:49 test bleu=33.14 loss=114.11 penalty=1.000 ratio=1.041
12/29 20:32:49 saving model to models/7_fold_codenn/checkpoints
12/29 20:32:49 finished saving model
12/29 20:54:19   decaying learning rate to: 0.033
12/29 20:56:49 step 130000 epoch 54 learning rate 0.033 step-time 0.718 loss 0.519
12/29 20:56:49 starting evaluation
12/29 21:01:05 test bleu=33.48 loss=115.34 penalty=1.000 ratio=1.032
12/29 21:01:05 saving model to models/7_fold_codenn/checkpoints
12/29 21:01:05 finished saving model
12/29 21:25:04 step 132000 epoch 54 learning rate 0.033 step-time 0.718 loss 0.479
12/29 21:25:04 starting evaluation
12/29 21:29:21 test bleu=32.82 loss=115.89 penalty=1.000 ratio=1.051
12/29 21:29:21 saving model to models/7_fold_codenn/checkpoints
12/29 21:29:21 finished saving model
12/29 21:32:22   decaying learning rate to: 0.0313
12/29 21:53:20 step 134000 epoch 55 learning rate 0.0313 step-time 0.718 loss 0.444
12/29 21:53:21 starting evaluation
12/29 21:57:35 test bleu=33.53 loss=116.80 penalty=1.000 ratio=1.025
12/29 21:57:35 saving model to models/7_fold_codenn/checkpoints
12/29 21:57:35 finished saving model
12/29 22:06:05   decaying learning rate to: 0.0298
12/29 22:21:32 step 136000 epoch 56 learning rate 0.0298 step-time 0.716 loss 0.439
12/29 22:21:32 starting evaluation
12/29 22:25:47 test bleu=33.32 loss=117.35 penalty=1.000 ratio=1.037
12/29 22:25:47 saving model to models/7_fold_codenn/checkpoints
12/29 22:25:47 finished saving model
12/29 22:39:50   decaying learning rate to: 0.0283
12/29 22:49:56 step 138000 epoch 57 learning rate 0.0283 step-time 0.722 loss 0.419
12/29 22:49:56 starting evaluation
12/29 22:54:16 test bleu=32.06 loss=117.21 penalty=1.000 ratio=1.077
12/29 22:54:16 saving model to models/7_fold_codenn/checkpoints
12/29 22:54:16 finished saving model
12/29 23:13:46   decaying learning rate to: 0.0269
12/29 23:18:44 step 140000 epoch 58 learning rate 0.0269 step-time 0.732 loss 0.413
12/29 23:18:44 starting evaluation
12/29 23:22:46 test bleu=32.87 loss=118.10 penalty=1.000 ratio=1.049
12/29 23:22:46 saving model to models/7_fold_codenn/checkpoints
12/29 23:22:46 finished saving model
12/29 23:47:11 step 142000 epoch 58 learning rate 0.0269 step-time 0.730 loss 0.395
12/29 23:47:11 starting evaluation
12/29 23:51:20 test bleu=33.43 loss=117.66 penalty=1.000 ratio=1.032
12/29 23:51:20 saving model to models/7_fold_codenn/checkpoints
12/29 23:51:20 finished saving model
12/29 23:51:46   decaying learning rate to: 0.0255
12/30 00:15:11 step 144000 epoch 59 learning rate 0.0255 step-time 0.714 loss 0.362
12/30 00:15:11 starting evaluation
12/30 00:19:25 test bleu=32.88 loss=118.62 penalty=1.000 ratio=1.052
12/30 00:19:25 saving model to models/7_fold_codenn/checkpoints
12/30 00:19:25 finished saving model
12/30 00:25:00   decaying learning rate to: 0.0242
12/30 00:43:13 step 146000 epoch 60 learning rate 0.0242 step-time 0.712 loss 0.354
12/30 00:43:13 starting evaluation
12/30 00:47:27 test bleu=33.32 loss=118.85 penalty=1.000 ratio=1.036
12/30 00:47:27 saving model to models/7_fold_codenn/checkpoints
12/30 00:47:27 finished saving model
12/30 00:58:19   decaying learning rate to: 0.023
12/30 01:11:14 step 148000 epoch 61 learning rate 0.023 step-time 0.712 loss 0.341
12/30 01:11:14 starting evaluation
12/30 01:15:28 test bleu=33.19 loss=119.80 penalty=1.000 ratio=1.041
12/30 01:15:28 saving model to models/7_fold_codenn/checkpoints
12/30 01:15:28 finished saving model
12/30 01:31:48   decaying learning rate to: 0.0219
12/30 01:39:12 step 150000 epoch 62 learning rate 0.0219 step-time 0.710 loss 0.338
12/30 01:39:12 starting evaluation
12/30 01:43:25 test bleu=33.28 loss=119.92 penalty=1.000 ratio=1.039
12/30 01:43:25 saving model to models/7_fold_codenn/checkpoints
12/30 01:43:25 finished saving model
12/30 02:05:16   decaying learning rate to: 0.0208
12/30 02:07:15 step 152000 epoch 63 learning rate 0.0208 step-time 0.713 loss 0.332
12/30 02:07:15 starting evaluation
12/30 02:11:30 test bleu=32.15 loss=120.75 penalty=1.000 ratio=1.072
12/30 02:11:30 saving model to models/7_fold_codenn/checkpoints
12/30 02:11:30 finished saving model
12/30 02:35:11 step 154000 epoch 63 learning rate 0.0208 step-time 0.709 loss 0.306
12/30 02:35:11 starting evaluation
12/30 02:39:26 test bleu=32.56 loss=120.46 penalty=1.000 ratio=1.057
12/30 02:39:26 saving model to models/7_fold_codenn/checkpoints
12/30 02:39:26 finished saving model
12/30 02:42:56   decaying learning rate to: 0.0197
12/30 03:03:15 step 156000 epoch 64 learning rate 0.0197 step-time 0.713 loss 0.293
12/30 03:03:15 starting evaluation
12/30 03:07:25 test bleu=33.72 loss=120.28 penalty=1.000 ratio=1.026
12/30 03:07:25 saving model to models/7_fold_codenn/checkpoints
12/30 03:07:25 finished saving model
12/30 03:07:25 new best model
12/30 03:16:25   decaying learning rate to: 0.0188
12/30 03:31:38 step 158000 epoch 65 learning rate 0.0188 step-time 0.724 loss 0.289
12/30 03:31:38 starting evaluation
12/30 03:35:54 test bleu=32.84 loss=121.15 penalty=1.000 ratio=1.052
12/30 03:35:54 saving model to models/7_fold_codenn/checkpoints
12/30 03:35:54 finished saving model
12/30 03:50:24   decaying learning rate to: 0.0178
12/30 04:00:21 step 160000 epoch 66 learning rate 0.0178 step-time 0.731 loss 0.286
12/30 04:00:21 starting evaluation
12/30 04:04:19 test bleu=32.58 loss=121.00 penalty=1.000 ratio=1.060
12/30 04:04:19 saving model to models/7_fold_codenn/checkpoints
12/30 04:04:19 finished saving model
12/30 04:24:08   decaying learning rate to: 0.0169
12/30 04:28:38 step 162000 epoch 67 learning rate 0.0169 step-time 0.727 loss 0.276
12/30 04:28:38 starting evaluation
12/30 04:32:54 test bleu=33.16 loss=121.53 penalty=1.000 ratio=1.039
12/30 04:32:54 saving model to models/7_fold_codenn/checkpoints
12/30 04:32:54 finished saving model
12/30 04:56:53 step 164000 epoch 67 learning rate 0.0169 step-time 0.718 loss 0.272
12/30 04:56:53 starting evaluation
12/30 05:01:09 test bleu=32.94 loss=121.59 penalty=1.000 ratio=1.050
12/30 05:01:09 saving model to models/7_fold_codenn/checkpoints
12/30 05:01:09 finished saving model
12/30 05:02:10   decaying learning rate to: 0.0161
12/30 05:25:05 step 166000 epoch 68 learning rate 0.0161 step-time 0.716 loss 0.251
12/30 05:25:05 starting evaluation
12/30 05:29:21 test bleu=32.87 loss=121.68 penalty=1.000 ratio=1.052
12/30 05:29:21 saving model to models/7_fold_codenn/checkpoints
12/30 05:29:21 finished saving model
12/30 05:35:52   decaying learning rate to: 0.0153
12/30 05:53:19 step 168000 epoch 69 learning rate 0.0153 step-time 0.717 loss 0.244
12/30 05:53:19 starting evaluation
12/30 05:57:35 test bleu=32.88 loss=122.06 penalty=1.000 ratio=1.054
12/30 05:57:35 saving model to models/7_fold_codenn/checkpoints
12/30 05:57:35 finished saving model
12/30 06:09:34   decaying learning rate to: 0.0145
12/30 06:21:30 step 170000 epoch 70 learning rate 0.0145 step-time 0.715 loss 0.245
12/30 06:21:30 starting evaluation
12/30 06:25:46 test bleu=33.53 loss=122.71 penalty=1.000 ratio=1.035
12/30 06:25:46 saving model to models/7_fold_codenn/checkpoints
12/30 06:25:46 finished saving model
12/30 06:43:08   decaying learning rate to: 0.0138
12/30 06:49:47 step 172000 epoch 71 learning rate 0.0138 step-time 0.719 loss 0.238
12/30 06:49:47 starting evaluation
12/30 06:54:02 test bleu=32.77 loss=122.93 penalty=1.000 ratio=1.055
12/30 06:54:02 saving model to models/7_fold_codenn/checkpoints
12/30 06:54:02 finished saving model
12/30 07:16:44   decaying learning rate to: 0.0131
12/30 07:18:09 step 174000 epoch 72 learning rate 0.0131 step-time 0.721 loss 0.236
12/30 07:18:09 starting evaluation
12/30 07:22:27 test bleu=33.33 loss=122.97 penalty=1.000 ratio=1.037
12/30 07:22:27 saving model to models/7_fold_codenn/checkpoints
12/30 07:22:27 finished saving model
12/30 07:48:00 step 176000 epoch 72 learning rate 0.0131 step-time 0.764 loss 0.223
12/30 07:48:00 starting evaluation
12/30 07:52:12 test bleu=33.21 loss=122.85 penalty=1.000 ratio=1.042
12/30 07:52:12 saving model to models/7_fold_codenn/checkpoints
12/30 07:52:12 finished saving model
12/30 07:56:18   decaying learning rate to: 0.0124
12/30 08:18:03 step 178000 epoch 73 learning rate 0.0124 step-time 0.772 loss 0.217
12/30 08:18:03 starting evaluation
12/30 08:22:27 test bleu=32.95 loss=123.27 penalty=1.000 ratio=1.051
12/30 08:22:27 saving model to models/7_fold_codenn/checkpoints
12/30 08:22:27 finished saving model
12/30 08:32:25   decaying learning rate to: 0.0118
12/30 08:47:57 step 180000 epoch 74 learning rate 0.0118 step-time 0.762 loss 0.209
12/30 08:47:57 starting evaluation
12/30 08:52:18 test bleu=32.78 loss=123.84 penalty=1.000 ratio=1.055
12/30 08:52:18 saving model to models/7_fold_codenn/checkpoints
12/30 08:52:18 finished saving model
12/30 09:07:17   decaying learning rate to: 0.0112
12/30 09:16:37 step 182000 epoch 75 learning rate 0.0112 step-time 0.727 loss 0.210
12/30 09:16:37 starting evaluation
12/30 09:20:53 test bleu=33.40 loss=123.88 penalty=1.000 ratio=1.039
12/30 09:20:53 saving model to models/7_fold_codenn/checkpoints
12/30 09:20:53 finished saving model
12/30 09:40:54   decaying learning rate to: 0.0107
12/30 09:44:48 step 184000 epoch 76 learning rate 0.0107 step-time 0.715 loss 0.207
12/30 09:44:48 starting evaluation
12/30 09:49:04 test bleu=33.12 loss=124.30 penalty=1.000 ratio=1.046
12/30 09:49:04 saving model to models/7_fold_codenn/checkpoints
12/30 09:49:04 finished saving model
12/30 10:13:09 step 186000 epoch 76 learning rate 0.0107 step-time 0.720 loss 0.204
12/30 10:13:09 starting evaluation
12/30 10:17:25 test bleu=33.53 loss=124.01 penalty=1.000 ratio=1.033
12/30 10:17:25 saving model to models/7_fold_codenn/checkpoints
12/30 10:17:25 finished saving model
12/30 10:18:56   decaying learning rate to: 0.0101
12/30 10:41:47 step 188000 epoch 77 learning rate 0.0101 step-time 0.729 loss 0.190
12/30 10:41:47 starting evaluation
12/30 10:45:58 test bleu=32.84 loss=124.44 penalty=1.000 ratio=1.053
12/30 10:45:58 saving model to models/7_fold_codenn/checkpoints
12/30 10:45:58 finished saving model
12/30 10:52:56   decaying learning rate to: 0.00963
12/30 11:10:22 step 190000 epoch 78 learning rate 0.00963 step-time 0.730 loss 0.188
12/30 11:10:22 starting evaluation
12/30 11:14:28 test bleu=33.02 loss=124.79 penalty=1.000 ratio=1.048
12/30 11:14:28 saving model to models/7_fold_codenn/checkpoints
12/30 11:14:29 finished saving model
12/30 11:26:40   decaying learning rate to: 0.00915
12/30 11:38:37 step 192000 epoch 79 learning rate 0.00915 step-time 0.722 loss 0.189
12/30 11:38:37 starting evaluation
12/30 11:42:53 test bleu=33.21 loss=125.28 penalty=1.000 ratio=1.042
12/30 11:42:53 saving model to models/7_fold_codenn/checkpoints
12/30 11:42:53 finished saving model
12/30 12:00:19   decaying learning rate to: 0.00869
12/30 12:06:49 step 194000 epoch 80 learning rate 0.00869 step-time 0.716 loss 0.186
12/30 12:06:49 starting evaluation
12/30 12:11:05 test bleu=33.14 loss=125.39 penalty=1.000 ratio=1.044
12/30 12:11:05 saving model to models/7_fold_codenn/checkpoints
12/30 12:11:05 finished saving model
12/30 12:34:01   decaying learning rate to: 0.00826
12/30 12:35:00 step 196000 epoch 81 learning rate 0.00826 step-time 0.715 loss 0.186
12/30 12:35:00 starting evaluation
12/30 12:39:16 test bleu=33.18 loss=125.51 penalty=1.000 ratio=1.042
12/30 12:39:16 saving model to models/7_fold_codenn/checkpoints
12/30 12:39:17 finished saving model
12/30 13:03:13 step 198000 epoch 81 learning rate 0.00826 step-time 0.716 loss 0.174
12/30 13:03:13 starting evaluation
12/30 13:07:29 test bleu=33.04 loss=125.46 penalty=1.000 ratio=1.047
12/30 13:07:29 saving model to models/7_fold_codenn/checkpoints
12/30 13:07:30 finished saving model
12/30 13:11:59   decaying learning rate to: 0.00784
12/30 13:31:30 step 200000 epoch 82 learning rate 0.00784 step-time 0.718 loss 0.170
12/30 13:31:30 starting evaluation
12/30 13:35:45 test bleu=33.18 loss=125.72 penalty=1.000 ratio=1.043
12/30 13:35:45 saving model to models/7_fold_codenn/checkpoints
12/30 13:35:45 finished saving model
12/30 13:45:45   decaying learning rate to: 0.00745
12/30 13:59:45 step 202000 epoch 83 learning rate 0.00745 step-time 0.718 loss 0.172
12/30 13:59:45 starting evaluation
12/30 14:04:01 test bleu=33.21 loss=126.16 penalty=1.000 ratio=1.040
12/30 14:04:01 saving model to models/7_fold_codenn/checkpoints
12/30 14:04:01 finished saving model
12/30 14:19:30   decaying learning rate to: 0.00708
12/30 14:28:13 step 204000 epoch 84 learning rate 0.00708 step-time 0.724 loss 0.169
12/30 14:28:13 starting evaluation
12/30 14:32:33 test bleu=33.13 loss=126.11 penalty=1.000 ratio=1.043
12/30 14:32:33 saving model to models/7_fold_codenn/checkpoints
12/30 14:32:33 finished saving model
12/30 14:53:31   decaying learning rate to: 0.00673
12/30 14:57:00 step 206000 epoch 85 learning rate 0.00673 step-time 0.731 loss 0.169
12/30 14:57:00 starting evaluation
12/30 15:00:58 test bleu=33.28 loss=126.40 penalty=1.000 ratio=1.040
12/30 15:00:58 saving model to models/7_fold_codenn/checkpoints
12/30 15:00:59 finished saving model
12/30 15:25:16 step 208000 epoch 85 learning rate 0.00673 step-time 0.727 loss 0.166
12/30 15:25:16 starting evaluation
12/30 15:29:34 test bleu=33.01 loss=126.00 penalty=1.000 ratio=1.048
12/30 15:29:34 saving model to models/7_fold_codenn/checkpoints
12/30 15:29:34 finished saving model
12/30 15:31:27   decaying learning rate to: 0.00639
12/30 15:53:32 step 210000 epoch 86 learning rate 0.00639 step-time 0.717 loss 0.155
12/30 15:53:32 starting evaluation
12/30 15:57:49 test bleu=33.05 loss=126.43 penalty=1.000 ratio=1.045
12/30 15:57:49 saving model to models/7_fold_codenn/checkpoints
12/30 15:57:49 finished saving model
12/30 16:04:58   decaying learning rate to: 0.00607
12/30 16:21:45 step 212000 epoch 87 learning rate 0.00607 step-time 0.716 loss 0.158
12/30 16:21:45 starting evaluation
12/30 16:26:01 test bleu=33.61 loss=126.69 penalty=1.000 ratio=1.031
12/30 16:26:01 saving model to models/7_fold_codenn/checkpoints
12/30 16:26:01 finished saving model
12/30 16:38:35   decaying learning rate to: 0.00577
12/30 16:49:54 step 214000 epoch 88 learning rate 0.00577 step-time 0.715 loss 0.157
12/30 16:49:54 starting evaluation
12/30 16:54:12 test bleu=33.07 loss=126.92 penalty=1.000 ratio=1.046
12/30 16:54:12 saving model to models/7_fold_codenn/checkpoints
12/30 16:54:12 finished saving model
12/30 17:12:13   decaying learning rate to: 0.00548
12/30 17:18:10 step 216000 epoch 89 learning rate 0.00548 step-time 0.717 loss 0.153
12/30 17:18:10 starting evaluation
12/30 17:22:25 test bleu=33.36 loss=127.09 penalty=1.000 ratio=1.038
12/30 17:22:25 saving model to models/7_fold_codenn/checkpoints
12/30 17:22:25 finished saving model
12/30 17:45:57   decaying learning rate to: 0.0052
12/30 17:46:25 step 218000 epoch 90 learning rate 0.0052 step-time 0.718 loss 0.155
12/30 17:46:25 starting evaluation
12/30 17:50:40 test bleu=33.41 loss=127.16 penalty=1.000 ratio=1.036
12/30 17:50:40 saving model to models/7_fold_codenn/checkpoints
12/30 17:50:40 finished saving model
12/30 18:14:47 step 220000 epoch 90 learning rate 0.0052 step-time 0.722 loss 0.145
12/30 18:14:47 starting evaluation
12/30 18:19:05 test bleu=33.26 loss=127.33 penalty=1.000 ratio=1.039
12/30 18:19:05 saving model to models/7_fold_codenn/checkpoints
12/30 18:19:05 finished saving model
12/30 18:24:05   decaying learning rate to: 0.00494
12/30 18:43:30 step 222000 epoch 91 learning rate 0.00494 step-time 0.730 loss 0.148
12/30 18:43:30 starting evaluation
12/30 18:47:41 test bleu=33.26 loss=127.33 penalty=1.000 ratio=1.041
12/30 18:47:41 saving model to models/7_fold_codenn/checkpoints
12/30 18:47:41 finished saving model
12/30 18:58:09   decaying learning rate to: 0.0047
12/30 19:12:02 step 224000 epoch 92 learning rate 0.0047 step-time 0.728 loss 0.143
12/30 19:12:02 starting evaluation
12/30 19:16:08 test bleu=33.10 loss=127.63 penalty=1.000 ratio=1.046
12/30 19:16:08 saving model to models/7_fold_codenn/checkpoints
12/30 19:16:08 finished saving model
12/30 19:31:50   decaying learning rate to: 0.00446
12/30 19:40:15 step 226000 epoch 93 learning rate 0.00446 step-time 0.722 loss 0.146
12/30 19:40:15 starting evaluation
12/30 19:44:31 test bleu=33.15 loss=127.75 penalty=1.000 ratio=1.043
12/30 19:44:31 saving model to models/7_fold_codenn/checkpoints
12/30 19:44:31 finished saving model
12/30 20:05:32   decaying learning rate to: 0.00424
12/30 20:08:29 step 228000 epoch 94 learning rate 0.00424 step-time 0.717 loss 0.145
12/30 20:08:29 starting evaluation
12/30 20:12:45 test bleu=32.94 loss=127.90 penalty=1.000 ratio=1.049
12/30 20:12:45 saving model to models/7_fold_codenn/checkpoints
12/30 20:12:45 finished saving model
12/30 20:36:44 step 230000 epoch 94 learning rate 0.00424 step-time 0.718 loss 0.141
12/30 20:36:44 starting evaluation
12/30 20:41:01 test bleu=33.11 loss=128.11 penalty=1.000 ratio=1.043
12/30 20:41:01 saving model to models/7_fold_codenn/checkpoints
12/30 20:41:01 finished saving model
12/30 20:43:32   decaying learning rate to: 0.00403
12/30 21:04:54 step 232000 epoch 95 learning rate 0.00403 step-time 0.715 loss 0.139
12/30 21:04:54 starting evaluation
12/30 21:09:10 test bleu=33.07 loss=128.07 penalty=1.000 ratio=1.043
12/30 21:09:10 saving model to models/7_fold_codenn/checkpoints
12/30 21:09:10 finished saving model
12/30 21:17:12   decaying learning rate to: 0.00383
12/30 21:33:08 step 234000 epoch 96 learning rate 0.00383 step-time 0.717 loss 0.137
12/30 21:33:08 starting evaluation
12/30 21:37:23 test bleu=33.09 loss=128.43 penalty=1.000 ratio=1.044
12/30 21:37:23 saving model to models/7_fold_codenn/checkpoints
12/30 21:37:24 finished saving model
12/30 21:50:57   decaying learning rate to: 0.00363
12/30 22:01:25 step 236000 epoch 97 learning rate 0.00363 step-time 0.719 loss 0.137
12/30 22:01:25 starting evaluation
12/30 22:05:41 test bleu=33.25 loss=128.46 penalty=1.000 ratio=1.040
12/30 22:05:41 saving model to models/7_fold_codenn/checkpoints
12/30 22:05:41 finished saving model
12/30 22:24:37   decaying learning rate to: 0.00345
12/30 22:29:45 step 238000 epoch 98 learning rate 0.00345 step-time 0.720 loss 0.136
12/30 22:29:45 starting evaluation
12/30 22:34:06 test bleu=33.05 loss=128.31 penalty=1.000 ratio=1.049
12/30 22:34:06 saving model to models/7_fold_codenn/checkpoints
12/30 22:34:06 finished saving model
12/30 22:58:38 step 240000 epoch 99 learning rate 0.00345 step-time 0.734 loss 0.138
12/30 22:58:38 starting evaluation
12/30 23:02:39 test bleu=33.16 loss=128.70 penalty=1.000 ratio=1.045
12/30 23:02:39 saving model to models/7_fold_codenn/checkpoints
12/30 23:02:39 finished saving model
12/30 23:02:41   decaying learning rate to: 0.00328
12/30 23:27:02 step 242000 epoch 99 learning rate 0.00328 step-time 0.729 loss 0.130
12/30 23:27:02 starting evaluation
12/30 23:31:18 test bleu=33.03 loss=128.60 penalty=1.000 ratio=1.048
12/30 23:31:18 saving model to models/7_fold_codenn/checkpoints
12/30 23:31:18 finished saving model
12/30 23:36:24   decaying learning rate to: 0.00312
12/30 23:55:15 step 244000 epoch 100 learning rate 0.00312 step-time 0.716 loss 0.131
12/30 23:55:15 starting evaluation
12/30 23:59:30 test bleu=33.04 loss=128.83 penalty=1.000 ratio=1.045
12/30 23:59:30 saving model to models/7_fold_codenn/checkpoints
12/30 23:59:31 finished saving model
12/31 00:09:45 finished training
12/31 00:09:45 exiting...
12/31 00:09:45 saving model to models/7_fold_codenn/checkpoints
12/31 00:09:45 finished saving model
