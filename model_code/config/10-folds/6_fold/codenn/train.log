nohup: ignoring input
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:107: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:30: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

12/27 03:46:42 label: default
12/27 03:46:42 description:
  default configuration
  next line of description
  last line
12/27 03:46:42 /root/icpc/icpc/translate/__main__.py config/10-folds/6_fold/codenn/config.yaml --train -v
12/27 03:46:42 commit hash 74e0554cb3eb5df835cef993ad570ff8de651f71
12/27 03:46:42 tensorflow version: 1.14.0
12/27 03:46:42 program arguments
12/27 03:46:42   aggregation_method   'sum'
12/27 03:46:42   align_encoder_id     0
12/27 03:46:42   allow_growth         True
12/27 03:46:42   attention_type       'global'
12/27 03:46:42   attn_filter_length   0
12/27 03:46:42   attn_filters         0
12/27 03:46:42   attn_prev_word       False
12/27 03:46:42   attn_size            128
12/27 03:46:42   attn_temperature     1.0
12/27 03:46:42   attn_window_size     0
12/27 03:46:42   average              False
12/27 03:46:42   baseline_activation  None
12/27 03:46:42   baseline_learning_rate 0.001
12/27 03:46:42   baseline_optimizer   'adam'
12/27 03:46:42   baseline_steps       0
12/27 03:46:42   batch_mode           'standard'
12/27 03:46:42   batch_size           64
12/27 03:46:42   beam_size            5
12/27 03:46:42   bidir                True
12/27 03:46:42   bidir_projection     False
12/27 03:46:42   binary               False
12/27 03:46:42   cell_size            256
12/27 03:46:42   cell_type            'GRU'
12/27 03:46:42   character_level      False
12/27 03:46:42   checkpoints          []
12/27 03:46:42   conditional_rnn      False
12/27 03:46:42   config               'config/10-folds/6_fold/codenn/config.yaml'
12/27 03:46:42   convolutions         None
12/27 03:46:42   data_dir             'data/gooddata/6_fold'
12/27 03:46:42   debug                False
12/27 03:46:42   decay_after_n_epoch  1
12/27 03:46:42   decay_every_n_epoch  1
12/27 03:46:42   decay_if_no_progress None
12/27 03:46:42   decoders             [{'max_len': 40, 'name': 'nl'}]
12/27 03:46:42   description          'default configuration\nnext line of description\nlast line\n'
12/27 03:46:42   dev_prefix           'test'
12/27 03:46:42   early_stopping       True
12/27 03:46:42   embedding_dropout    0.0
12/27 03:46:42   embedding_initializer None
12/27 03:46:42   embedding_size       256
12/27 03:46:42   embedding_weight_scale None
12/27 03:46:42   embeddings_on_cpu    True
12/27 03:46:42   encoders             [{'attention_type': 'global', 'max_len': 200, 'name': 'code'}]
12/27 03:46:42   ensemble             False
12/27 03:46:42   eval_burn_in         0
12/27 03:46:42   feed_previous        0.0
12/27 03:46:42   final_state          'last'
12/27 03:46:42   freeze_variables     []
12/27 03:46:42   generate_first       True
12/27 03:46:42   gpu_id               2
12/27 03:46:42   highway_layers       0
12/27 03:46:42   initial_state_dropout 0.0
12/27 03:46:42   initializer          None
12/27 03:46:42   input_layer_dropout  0.0
12/27 03:46:42   input_layers         None
12/27 03:46:42   keep_best            5
12/27 03:46:42   keep_every_n_hours   0
12/27 03:46:42   label                'default'
12/27 03:46:42   layer_norm           False
12/27 03:46:42   layers               1
12/27 03:46:42   learning_rate        0.5
12/27 03:46:42   learning_rate_decay_factor 0.95
12/27 03:46:42   len_normalization    1.0
12/27 03:46:42   log_file             'log.txt'
12/27 03:46:42   loss_function        'xent'
12/27 03:46:42   max_dev_size         0
12/27 03:46:42   max_epochs           100
12/27 03:46:42   max_gradient_norm    5.0
12/27 03:46:42   max_len              50
12/27 03:46:42   max_steps            600000
12/27 03:46:42   max_test_size        0
12/27 03:46:42   max_to_keep          1
12/27 03:46:42   max_train_size       0
12/27 03:46:42   maxout_stride        None
12/27 03:46:42   mem_fraction         1.0
12/27 03:46:42   min_learning_rate    1e-06
12/27 03:46:42   model_dir            'models/6_fold_codenn'
12/27 03:46:42   moving_average       None
12/27 03:46:42   no_gpu               False
12/27 03:46:42   optimizer            'sgd'
12/27 03:46:42   orthogonal_init      False
12/27 03:46:42   output               None
12/27 03:46:42   output_dropout       0.0
12/27 03:46:42   parallel_iterations  16
12/27 03:46:42   pervasive_dropout    False
12/27 03:46:42   pooling_avg          True
12/27 03:46:42   post_process_script  None
12/27 03:46:42   pred_deep_layer      False
12/27 03:46:42   pred_edits           False
12/27 03:46:42   pred_embed_proj      True
12/27 03:46:42   pred_maxout_layer    True
12/27 03:46:42   purge                False
12/27 03:46:42   raw_output           False
12/27 03:46:42   read_ahead           1
12/27 03:46:42   reconstruction_attn_weight 0.05
12/27 03:46:42   reconstruction_decoders False
12/27 03:46:42   reconstruction_weight 1.0
12/27 03:46:42   reinforce_after_n_epoch None
12/27 03:46:42   remove_unk           False
12/27 03:46:42   reverse              False
12/27 03:46:42   reverse_input        True
12/27 03:46:42   reward_function      'sentence_bleu'
12/27 03:46:42   rnn_feed_attn        True
12/27 03:46:42   rnn_input_dropout    0.0
12/27 03:46:42   rnn_output_dropout   0.0
12/27 03:46:42   rnn_state_dropout    0.0
12/27 03:46:42   save                 False
12/27 03:46:42   score_function       'corpus_bleu'
12/27 03:46:42   score_functions      ['bleu', 'loss']
12/27 03:46:42   script_dir           'scripts'
12/27 03:46:42   sgd_after_n_epoch    None
12/27 03:46:42   sgd_learning_rate    1.0
12/27 03:46:42   shuffle              True
12/27 03:46:42   softmax_temperature  1.0
12/27 03:46:42   steps_per_checkpoint 2000
12/27 03:46:42   steps_per_eval       2000
12/27 03:46:42   swap_memory          True
12/27 03:46:42   tie_embeddings       False
12/27 03:46:42   time_pooling         None
12/27 03:46:42   train                True
12/27 03:46:42   train_initial_states True
12/27 03:46:42   train_prefix         'train'
12/27 03:46:42   truncate_lines       True
12/27 03:46:42   update_first         False
12/27 03:46:42   use_baseline         False
12/27 03:46:42   use_dropout          False
12/27 03:46:42   use_lstm_full_state  False
12/27 03:46:42   use_previous_word    True
12/27 03:46:42   verbose              True
12/27 03:46:42   vocab_prefix         'vocab'
12/27 03:46:42   weight_scale         None
12/27 03:46:42   word_dropout         0.0
12/27 03:46:42 python random seed: 6703194331685446386
12/27 03:46:42 tf random seed:     8899896592397555767
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:203: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

12/27 03:46:42 creating model
12/27 03:46:42 using device: /gpu:2
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:230: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

12/27 03:46:42 copying vocab to models/6_fold_codenn/data/vocab.code
12/27 03:46:42 copying vocab to models/6_fold_codenn/data/vocab.nl
12/27 03:46:42 reading vocabularies
12/27 03:46:42 creating model
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:60: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:111: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:33: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5f55db7358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5f55db7358>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:226: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5f55db7390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5f55db7390>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5f55dacd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5f55dacd30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:838: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5fdcb52b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5fdcb52b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5fdcb90048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5fdcb90048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:432: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:435: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5fdc9cde10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5fdc9cde10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5fdc9e38d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5fdc9e38d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5fdca3c908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5fdca3c908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5fdca3c908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5fdca3c908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5fdca3c908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5fdca3c908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:919: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5fdc6ed048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5fdc6ed048>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:131: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:223: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5f81018d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f5f81018d30>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5f80f52668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5f80f52668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5f80ef8cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5f80ef8cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5f80ef8080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5f80ef8080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5f80ef8080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5f80ef8080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5f80ef8080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f5f80ef8080>>: AssertionError: Bad argument number for Name: 3, expecting 4
12/27 03:46:47 model parameters (30)
12/27 03:46:47   baseline_step:0 ()
12/27 03:46:47   decoder_nl/attention_code/U_a/kernel:0 (512, 128)
12/27 03:46:47   decoder_nl/attention_code/W_a/bias:0 (128,)
12/27 03:46:47   decoder_nl/attention_code/W_a/kernel:0 (256, 128)
12/27 03:46:47   decoder_nl/attention_code/v_a:0 (128,)
12/27 03:46:47   decoder_nl/code/initial_state_projection/bias:0 (256,)
12/27 03:46:47   decoder_nl/code/initial_state_projection/kernel:0 (256, 256)
12/27 03:46:47   decoder_nl/gru_cell/candidate/bias:0 (256,)
12/27 03:46:47   decoder_nl/gru_cell/candidate/kernel:0 (1024, 256)
12/27 03:46:47   decoder_nl/gru_cell/gates/bias:0 (512,)
12/27 03:46:47   decoder_nl/gru_cell/gates/kernel:0 (1024, 512)
12/27 03:46:47   decoder_nl/maxout/bias:0 (256,)
12/27 03:46:47   decoder_nl/maxout/kernel:0 (1024, 256)
12/27 03:46:47   decoder_nl/softmax0/kernel:0 (128, 256)
12/27 03:46:47   decoder_nl/softmax1/bias:0 (37989,)
12/27 03:46:47   decoder_nl/softmax1/kernel:0 (256, 37989)
12/27 03:46:47   embedding_code:0 (50000, 256)
12/27 03:46:47   embedding_nl:0 (37989, 256)
12/27 03:46:47   encoder_code/initial_state_bw:0 (256,)
12/27 03:46:47   encoder_code/initial_state_fw:0 (256,)
12/27 03:46:47   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/27 03:46:47   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/27 03:46:47   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/27 03:46:47   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/27 03:46:47   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/27 03:46:47   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/27 03:46:47   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/27 03:46:47   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/27 03:46:47   global_step:0 ()
12/27 03:46:47   learning_rate:0 ()
12/27 03:46:47 number of parameters: 34.32M
WARNING:tensorflow:From /root/icpc/icpc/translate/translation_model.py:666: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

12/27 03:46:48 global step: 0
12/27 03:46:48 baseline step: 0
12/27 03:46:48 reading training data
12/27 03:46:48 total line count: 156721
12/27 03:46:52   lines read: 100000
12/27 03:46:54 files: data/gooddata/6_fold/train.code data/gooddata/6_fold/train.nl
12/27 03:46:54 lines reads: 156721
12/27 03:46:54 reading development data
12/27 03:46:55 files: data/gooddata/6_fold/test.code data/gooddata/6_fold/test.nl
12/27 03:46:55 lines reads: 17413
12/27 03:46:55 starting training
12/27 04:09:49 step 2000 epoch 1 learning rate 0.5 step-time 0.685 loss 78.854
12/27 04:09:49 starting evaluation
12/27 04:13:48 test bleu=1.36 loss=63.09 penalty=0.722 ratio=0.755
12/27 04:13:48 saving model to models/6_fold_codenn/checkpoints
12/27 04:13:48 finished saving model
12/27 04:13:48 new best model
12/27 04:18:52   decaying learning rate to: 0.475
12/27 04:36:38 step 4000 epoch 2 learning rate 0.475 step-time 0.683 loss 58.944
12/27 04:36:38 starting evaluation
12/27 04:40:37 test bleu=4.06 loss=56.44 penalty=0.832 ratio=0.845
12/27 04:40:37 saving model to models/6_fold_codenn/checkpoints
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
12/27 04:40:37 finished saving model
12/27 04:40:37 new best model
12/27 04:50:56   decaying learning rate to: 0.451
12/27 05:03:30 step 6000 epoch 3 learning rate 0.451 step-time 0.684 loss 52.480
12/27 05:03:30 starting evaluation
12/27 05:07:27 test bleu=7.60 loss=51.37 penalty=0.875 ratio=0.882
12/27 05:07:27 saving model to models/6_fold_codenn/checkpoints
12/27 05:07:27 finished saving model
12/27 05:07:27 new best model
12/27 05:22:53   decaying learning rate to: 0.429
12/27 05:30:18 step 8000 epoch 4 learning rate 0.429 step-time 0.683 loss 47.733
12/27 05:30:18 starting evaluation
12/27 05:34:10 test bleu=9.43 loss=48.24 penalty=0.847 ratio=0.858
12/27 05:34:10 saving model to models/6_fold_codenn/checkpoints
12/27 05:34:10 finished saving model
12/27 05:34:10 new best model
12/27 05:54:45   decaying learning rate to: 0.407
12/27 05:57:05 step 10000 epoch 5 learning rate 0.407 step-time 0.686 loss 44.237
12/27 05:57:05 starting evaluation
12/27 06:00:56 test bleu=10.97 loss=45.70 penalty=0.883 ratio=0.889
12/27 06:00:56 saving model to models/6_fold_codenn/checkpoints
12/27 06:00:56 finished saving model
12/27 06:00:56 new best model
12/27 06:23:45 step 12000 epoch 5 learning rate 0.407 step-time 0.682 loss 40.929
12/27 06:23:45 starting evaluation
12/27 06:27:44 test bleu=12.48 loss=43.78 penalty=1.000 ratio=1.085
12/27 06:27:44 saving model to models/6_fold_codenn/checkpoints
12/27 06:27:44 finished saving model
12/27 06:27:44 new best model
12/27 06:30:28   decaying learning rate to: 0.387
12/27 06:50:31 step 14000 epoch 6 learning rate 0.387 step-time 0.682 loss 37.901
12/27 06:50:31 starting evaluation
12/27 06:54:29 test bleu=12.46 loss=42.97 penalty=1.000 ratio=1.225
12/27 06:54:29 saving model to models/6_fold_codenn/checkpoints
12/27 06:54:29 finished saving model
12/27 07:02:23   decaying learning rate to: 0.368
12/27 07:17:18 step 16000 epoch 7 learning rate 0.368 step-time 0.683 loss 35.496
12/27 07:17:18 starting evaluation
12/27 07:21:02 test bleu=16.47 loss=41.63 penalty=0.779 ratio=0.800
12/27 07:21:02 saving model to models/6_fold_codenn/checkpoints
12/27 07:21:02 finished saving model
12/27 07:21:02 new best model
12/27 07:34:05   decaying learning rate to: 0.349
12/27 07:43:52 step 18000 epoch 8 learning rate 0.349 step-time 0.683 loss 33.170
12/27 07:43:52 starting evaluation
12/27 07:47:41 test bleu=18.22 loss=40.94 penalty=0.882 ratio=0.888
12/27 07:47:41 saving model to models/6_fold_codenn/checkpoints
12/27 07:47:41 finished saving model
12/27 07:47:41 new best model
12/27 08:05:50   decaying learning rate to: 0.332
12/27 08:10:31 step 20000 epoch 9 learning rate 0.332 step-time 0.683 loss 31.321
12/27 08:10:31 starting evaluation
12/27 08:14:20 test bleu=19.16 loss=40.36 penalty=0.849 ratio=0.859
12/27 08:14:20 saving model to models/6_fold_codenn/checkpoints
12/27 08:14:20 finished saving model
12/27 08:14:20 new best model
12/27 08:37:03 step 22000 epoch 9 learning rate 0.332 step-time 0.679 loss 29.381
12/27 08:37:03 starting evaluation
12/27 08:41:01 test bleu=20.27 loss=39.55 penalty=0.892 ratio=0.898
12/27 08:41:02 saving model to models/6_fold_codenn/checkpoints
12/27 08:41:02 finished saving model
12/27 08:41:02 new best model
12/27 08:41:27   decaying learning rate to: 0.315
12/27 09:03:46 step 24000 epoch 10 learning rate 0.315 step-time 0.680 loss 26.880
12/27 09:03:46 starting evaluation
12/27 09:07:43 test bleu=21.68 loss=39.94 penalty=0.906 ratio=0.910
12/27 09:07:43 saving model to models/6_fold_codenn/checkpoints
12/27 09:07:43 finished saving model
12/27 09:07:43 new best model
12/27 09:13:08   decaying learning rate to: 0.299
12/27 09:30:25 step 26000 epoch 11 learning rate 0.299 step-time 0.679 loss 25.148
12/27 09:30:25 starting evaluation
12/27 09:34:23 test bleu=22.76 loss=40.03 penalty=0.886 ratio=0.892
12/27 09:34:23 saving model to models/6_fold_codenn/checkpoints
12/27 09:34:23 finished saving model
12/27 09:34:23 new best model
12/27 09:45:00   decaying learning rate to: 0.284
12/27 09:57:10 step 28000 epoch 12 learning rate 0.284 step-time 0.681 loss 23.598
12/27 09:57:10 starting evaluation
12/27 10:01:09 test bleu=23.79 loss=40.61 penalty=0.930 ratio=0.933
12/27 10:01:09 saving model to models/6_fold_codenn/checkpoints
12/27 10:01:09 finished saving model
12/27 10:01:09 new best model
12/27 10:16:53   decaying learning rate to: 0.27
12/27 10:23:52 step 30000 epoch 13 learning rate 0.27 step-time 0.679 loss 22.129
12/27 10:23:52 starting evaluation
12/27 10:27:54 test bleu=24.62 loss=41.69 penalty=0.919 ratio=0.922
12/27 10:27:54 saving model to models/6_fold_codenn/checkpoints
12/27 10:27:54 finished saving model
12/27 10:27:54 new best model
12/27 10:48:45   decaying learning rate to: 0.257
12/27 10:50:37 step 32000 epoch 14 learning rate 0.257 step-time 0.679 loss 20.855
12/27 10:50:37 starting evaluation
12/27 10:54:33 test bleu=24.86 loss=41.85 penalty=0.848 ratio=0.859
12/27 10:54:33 saving model to models/6_fold_codenn/checkpoints
12/27 10:54:33 finished saving model
12/27 10:54:33 new best model
12/27 11:17:12 step 34000 epoch 14 learning rate 0.257 step-time 0.678 loss 18.974
12/27 11:17:12 starting evaluation
12/27 11:21:18 test bleu=25.71 loss=40.73 penalty=0.908 ratio=0.912
12/27 11:21:18 saving model to models/6_fold_codenn/checkpoints
12/27 11:21:18 finished saving model
12/27 11:21:18 new best model
12/27 11:24:23   decaying learning rate to: 0.244
12/27 11:43:57 step 36000 epoch 15 learning rate 0.244 step-time 0.677 loss 17.437
12/27 11:43:57 starting evaluation
12/27 11:48:04 test bleu=26.58 loss=42.14 penalty=0.957 ratio=0.957
12/27 11:48:04 saving model to models/6_fold_codenn/checkpoints
12/27 11:48:04 finished saving model
12/27 11:48:04 new best model
12/27 11:56:16   decaying learning rate to: 0.232
12/27 12:10:49 step 38000 epoch 16 learning rate 0.232 step-time 0.680 loss 16.367
12/27 12:10:49 starting evaluation
12/27 12:14:54 test bleu=27.12 loss=44.32 penalty=0.961 ratio=0.961
12/27 12:14:54 saving model to models/6_fold_codenn/checkpoints
12/27 12:14:54 finished saving model
12/27 12:14:54 new best model
12/27 12:28:13   decaying learning rate to: 0.22
12/27 12:37:36 step 40000 epoch 17 learning rate 0.22 step-time 0.679 loss 15.206
12/27 12:37:36 starting evaluation
12/27 12:41:34 test bleu=27.30 loss=45.52 penalty=0.894 ratio=0.899
12/27 12:41:34 saving model to models/6_fold_codenn/checkpoints
12/27 12:41:34 finished saving model
12/27 12:41:34 new best model
12/27 13:00:03   decaying learning rate to: 0.209
12/27 13:04:14 step 42000 epoch 18 learning rate 0.209 step-time 0.678 loss 14.280
12/27 13:04:14 starting evaluation
12/27 13:08:15 test bleu=27.90 loss=46.67 penalty=0.891 ratio=0.896
12/27 13:08:15 saving model to models/6_fold_codenn/checkpoints
12/27 13:08:16 finished saving model
12/27 13:08:16 new best model
12/27 13:30:53 step 44000 epoch 18 learning rate 0.209 step-time 0.676 loss 13.221
12/27 13:30:53 starting evaluation
12/27 13:35:01 test bleu=28.61 loss=46.32 penalty=0.962 ratio=0.963
12/27 13:35:01 saving model to models/6_fold_codenn/checkpoints
12/27 13:35:01 finished saving model
12/27 13:35:01 new best model
12/27 13:35:52   decaying learning rate to: 0.199
12/27 13:57:41 step 46000 epoch 19 learning rate 0.199 step-time 0.678 loss 11.735
12/27 13:57:41 starting evaluation
12/27 14:01:43 test bleu=28.91 loss=48.60 penalty=0.964 ratio=0.965
12/27 14:01:43 saving model to models/6_fold_codenn/checkpoints
12/27 14:01:43 finished saving model
12/27 14:01:43 new best model
12/27 14:07:33   decaying learning rate to: 0.189
12/27 14:24:26 step 48000 epoch 20 learning rate 0.189 step-time 0.679 loss 10.932
12/27 14:24:26 starting evaluation
12/27 14:28:32 test bleu=29.11 loss=50.23 penalty=0.944 ratio=0.945
12/27 14:28:32 saving model to models/6_fold_codenn/checkpoints
12/27 14:28:32 finished saving model
12/27 14:28:32 new best model
12/27 14:39:27   decaying learning rate to: 0.179
12/27 14:51:10 step 50000 epoch 21 learning rate 0.179 step-time 0.677 loss 10.159
12/27 14:51:10 starting evaluation
12/27 14:55:20 test bleu=28.59 loss=51.87 penalty=1.000 ratio=1.040
12/27 14:55:20 saving model to models/6_fold_codenn/checkpoints
12/27 14:55:21 finished saving model
12/27 15:11:27   decaying learning rate to: 0.17
12/27 15:18:01 step 52000 epoch 22 learning rate 0.17 step-time 0.678 loss 9.479
12/27 15:18:01 starting evaluation
12/27 15:22:06 test bleu=29.68 loss=54.80 penalty=0.933 ratio=0.935
12/27 15:22:06 saving model to models/6_fold_codenn/checkpoints
12/27 15:22:06 finished saving model
12/27 15:22:06 new best model
12/27 15:43:21   decaying learning rate to: 0.162
12/27 15:44:45 step 54000 epoch 23 learning rate 0.162 step-time 0.677 loss 8.808
12/27 15:44:45 starting evaluation
12/27 15:48:50 test bleu=30.49 loss=55.75 penalty=0.949 ratio=0.950
12/27 15:48:50 saving model to models/6_fold_codenn/checkpoints
12/27 15:48:50 finished saving model
12/27 15:48:50 new best model
12/27 16:11:33 step 56000 epoch 23 learning rate 0.162 step-time 0.679 loss 7.773
12/27 16:11:33 starting evaluation
12/27 16:15:38 test bleu=30.37 loss=56.31 penalty=0.985 ratio=0.985
12/27 16:15:38 saving model to models/6_fold_codenn/checkpoints
12/27 16:15:38 finished saving model
12/27 16:19:09   decaying learning rate to: 0.154
12/27 16:38:21 step 58000 epoch 24 learning rate 0.154 step-time 0.680 loss 7.160
12/27 16:38:21 starting evaluation
12/27 16:42:28 test bleu=30.94 loss=58.63 penalty=0.954 ratio=0.955
12/27 16:42:28 saving model to models/6_fold_codenn/checkpoints
12/27 16:42:28 finished saving model
12/27 16:42:28 new best model
12/27 16:51:02   decaying learning rate to: 0.146
12/27 17:05:07 step 60000 epoch 25 learning rate 0.146 step-time 0.677 loss 6.623
12/27 17:05:07 starting evaluation
12/27 17:09:12 test bleu=31.09 loss=61.18 penalty=0.980 ratio=0.981
12/27 17:09:12 saving model to models/6_fold_codenn/checkpoints
12/27 17:09:12 finished saving model
12/27 17:09:12 new best model
12/27 17:22:59   decaying learning rate to: 0.139
12/27 17:31:51 step 62000 epoch 26 learning rate 0.139 step-time 0.677 loss 6.144
12/27 17:31:51 starting evaluation
12/27 17:35:54 test bleu=31.31 loss=64.11 penalty=0.957 ratio=0.958
12/27 17:35:54 saving model to models/6_fold_codenn/checkpoints
12/27 17:35:55 finished saving model
12/27 17:35:55 new best model
12/27 17:54:53   decaying learning rate to: 0.132
12/27 17:58:36 step 64000 epoch 27 learning rate 0.132 step-time 0.678 loss 5.738
12/27 17:58:36 starting evaluation
12/27 18:02:44 test bleu=31.30 loss=65.10 penalty=0.998 ratio=0.998
12/27 18:02:44 saving model to models/6_fold_codenn/checkpoints
12/27 18:02:44 finished saving model
12/27 18:25:27 step 66000 epoch 27 learning rate 0.132 step-time 0.680 loss 5.228
12/27 18:25:27 starting evaluation
12/27 18:29:36 test bleu=30.79 loss=65.76 penalty=1.000 ratio=1.031
12/27 18:29:36 saving model to models/6_fold_codenn/checkpoints
12/27 18:29:36 finished saving model
12/27 18:30:51   decaying learning rate to: 0.125
12/27 18:52:16 step 68000 epoch 28 learning rate 0.125 step-time 0.678 loss 4.610
12/27 18:52:16 starting evaluation
12/27 18:56:24 test bleu=31.59 loss=68.88 penalty=1.000 ratio=1.008
12/27 18:56:24 saving model to models/6_fold_codenn/checkpoints
12/27 18:56:24 finished saving model
12/27 18:56:24 new best model
12/27 19:02:42   decaying learning rate to: 0.119
12/27 19:19:04 step 70000 epoch 29 learning rate 0.119 step-time 0.678 loss 4.284
12/27 19:19:04 starting evaluation
12/27 19:23:14 test bleu=30.31 loss=71.35 penalty=1.000 ratio=1.047
12/27 19:23:14 saving model to models/6_fold_codenn/checkpoints
12/27 19:23:14 finished saving model
12/27 19:34:42   decaying learning rate to: 0.113
12/27 19:45:53 step 72000 epoch 30 learning rate 0.113 step-time 0.678 loss 3.962
12/27 19:45:53 starting evaluation
12/27 19:50:01 test bleu=31.81 loss=73.29 penalty=1.000 ratio=1.010
12/27 19:50:01 saving model to models/6_fold_codenn/checkpoints
12/27 19:50:01 finished saving model
12/27 19:50:01 new best model
12/27 20:06:37   decaying learning rate to: 0.107
12/27 20:12:41 step 74000 epoch 31 learning rate 0.107 step-time 0.678 loss 3.694
12/27 20:12:41 starting evaluation
12/27 20:16:47 test bleu=31.52 loss=76.62 penalty=1.000 ratio=1.024
12/27 20:16:47 saving model to models/6_fold_codenn/checkpoints
12/27 20:16:47 finished saving model
12/27 20:38:31   decaying learning rate to: 0.102
12/27 20:39:27 step 76000 epoch 32 learning rate 0.102 step-time 0.678 loss 3.467
12/27 20:39:27 starting evaluation
12/27 20:43:36 test bleu=31.69 loss=77.10 penalty=1.000 ratio=1.025
12/27 20:43:36 saving model to models/6_fold_codenn/checkpoints
12/27 20:43:36 finished saving model
12/27 21:06:14 step 78000 epoch 32 learning rate 0.102 step-time 0.677 loss 3.014
12/27 21:06:14 starting evaluation
12/27 21:10:23 test bleu=31.41 loss=78.38 penalty=1.000 ratio=1.037
12/27 21:10:23 saving model to models/6_fold_codenn/checkpoints
12/27 21:10:23 finished saving model
12/27 21:14:22   decaying learning rate to: 0.0969
12/27 21:33:03 step 80000 epoch 33 learning rate 0.0969 step-time 0.678 loss 2.765
12/27 21:33:03 starting evaluation
12/27 21:37:11 test bleu=31.15 loss=82.16 penalty=1.000 ratio=1.046
12/27 21:37:11 saving model to models/6_fold_codenn/checkpoints
12/27 21:37:11 finished saving model
12/27 21:46:19   decaying learning rate to: 0.092
12/27 21:59:49 step 82000 epoch 34 learning rate 0.092 step-time 0.677 loss 2.591
12/27 21:59:49 starting evaluation
12/27 22:03:55 test bleu=32.46 loss=83.33 penalty=1.000 ratio=1.012
12/27 22:03:55 saving model to models/6_fold_codenn/checkpoints
12/27 22:03:55 finished saving model
12/27 22:03:55 new best model
12/27 22:18:14   decaying learning rate to: 0.0874
12/27 22:26:41 step 84000 epoch 35 learning rate 0.0874 step-time 0.681 loss 2.407
12/27 22:26:41 starting evaluation
12/27 22:30:47 test bleu=31.98 loss=86.01 penalty=1.000 ratio=1.027
12/27 22:30:47 saving model to models/6_fold_codenn/checkpoints
12/27 22:30:48 finished saving model
12/27 22:50:14   decaying learning rate to: 0.083
12/27 22:53:30 step 86000 epoch 36 learning rate 0.083 step-time 0.679 loss 2.258
12/27 22:53:30 starting evaluation
12/27 22:57:38 test bleu=31.60 loss=87.35 penalty=1.000 ratio=1.039
12/27 22:57:38 saving model to models/6_fold_codenn/checkpoints
12/27 22:57:38 finished saving model
12/27 23:20:18 step 88000 epoch 36 learning rate 0.083 step-time 0.678 loss 2.049
12/27 23:20:18 starting evaluation
12/27 23:24:27 test bleu=30.97 loss=88.76 penalty=1.000 ratio=1.067
12/27 23:24:27 saving model to models/6_fold_codenn/checkpoints
12/27 23:24:27 finished saving model
12/27 23:26:10   decaying learning rate to: 0.0789
12/27 23:47:04 step 90000 epoch 37 learning rate 0.0789 step-time 0.677 loss 1.819
12/27 23:47:04 starting evaluation
12/27 23:51:13 test bleu=31.75 loss=91.25 penalty=1.000 ratio=1.039
12/27 23:51:13 saving model to models/6_fold_codenn/checkpoints
12/27 23:51:13 finished saving model
12/27 23:58:00   decaying learning rate to: 0.0749
12/28 00:13:53 step 92000 epoch 38 learning rate 0.0749 step-time 0.678 loss 1.704
12/28 00:13:53 starting evaluation
12/28 00:18:01 test bleu=31.93 loss=93.33 penalty=1.000 ratio=1.033
12/28 00:18:01 saving model to models/6_fold_codenn/checkpoints
12/28 00:18:01 finished saving model
12/28 00:29:57   decaying learning rate to: 0.0712
12/28 00:40:43 step 94000 epoch 39 learning rate 0.0712 step-time 0.679 loss 1.587
12/28 00:40:43 starting evaluation
12/28 00:44:50 test bleu=31.82 loss=96.16 penalty=1.000 ratio=1.043
12/28 00:44:50 saving model to models/6_fold_codenn/checkpoints
12/28 00:44:50 finished saving model
12/28 01:01:53   decaying learning rate to: 0.0676
12/28 01:07:29 step 96000 epoch 40 learning rate 0.0676 step-time 0.677 loss 1.515
12/28 01:07:29 starting evaluation
12/28 01:11:36 test bleu=32.50 loss=97.69 penalty=1.000 ratio=1.020
12/28 01:11:36 saving model to models/6_fold_codenn/checkpoints
12/28 01:11:36 finished saving model
12/28 01:11:36 new best model
12/28 01:33:47   decaying learning rate to: 0.0643
12/28 01:34:15 step 98000 epoch 41 learning rate 0.0643 step-time 0.677 loss 1.406
12/28 01:34:15 starting evaluation
12/28 01:38:24 test bleu=31.67 loss=99.11 penalty=1.000 ratio=1.059
12/28 01:38:24 saving model to models/6_fold_codenn/checkpoints
12/28 01:38:25 finished saving model
12/28 02:01:17 step 100000 epoch 41 learning rate 0.0643 step-time 0.684 loss 1.231
12/28 02:01:17 starting evaluation
12/28 02:05:33 test bleu=31.76 loss=100.41 penalty=1.000 ratio=1.056
12/28 02:05:33 saving model to models/6_fold_codenn/checkpoints
12/28 02:05:34 finished saving model
12/28 02:10:13   decaying learning rate to: 0.061
12/28 02:29:03 step 102000 epoch 42 learning rate 0.061 step-time 0.703 loss 1.168
12/28 02:29:03 starting evaluation
12/28 02:33:18 test bleu=31.81 loss=101.99 penalty=1.000 ratio=1.049
12/28 02:33:18 saving model to models/6_fold_codenn/checkpoints
12/28 02:33:18 finished saving model
12/28 02:43:15   decaying learning rate to: 0.058
12/28 02:56:52 step 104000 epoch 43 learning rate 0.058 step-time 0.704 loss 1.093
12/28 02:56:52 starting evaluation
12/28 03:01:10 test bleu=32.19 loss=103.64 penalty=1.000 ratio=1.039
12/28 03:01:10 saving model to models/6_fold_codenn/checkpoints
12/28 03:01:10 finished saving model
12/28 03:16:22   decaying learning rate to: 0.0551
12/28 03:24:38 step 106000 epoch 44 learning rate 0.0551 step-time 0.702 loss 1.026
12/28 03:24:38 starting evaluation
12/28 03:28:54 test bleu=31.85 loss=105.91 penalty=1.000 ratio=1.051
12/28 03:28:54 saving model to models/6_fold_codenn/checkpoints
12/28 03:28:54 finished saving model
12/28 03:49:41   decaying learning rate to: 0.0523
12/28 03:52:35 step 108000 epoch 45 learning rate 0.0523 step-time 0.709 loss 0.979
12/28 03:52:35 starting evaluation
12/28 03:56:51 test bleu=31.79 loss=106.72 penalty=1.000 ratio=1.056
12/28 03:56:51 saving model to models/6_fold_codenn/checkpoints
12/28 03:56:51 finished saving model
12/28 04:20:29 step 110000 epoch 45 learning rate 0.0523 step-time 0.707 loss 0.903
12/28 04:20:29 starting evaluation
12/28 04:24:42 test bleu=32.84 loss=107.74 penalty=1.000 ratio=1.027
12/28 04:24:42 saving model to models/6_fold_codenn/checkpoints
12/28 04:24:43 finished saving model
12/28 04:24:43 new best model
12/28 04:27:03   decaying learning rate to: 0.0497
12/28 04:47:54 step 112000 epoch 46 learning rate 0.0497 step-time 0.694 loss 0.817
12/28 04:47:54 starting evaluation
12/28 04:52:01 test bleu=32.13 loss=109.25 penalty=1.000 ratio=1.043
12/28 04:52:01 saving model to models/6_fold_codenn/checkpoints
12/28 04:52:01 finished saving model
12/28 04:59:17   decaying learning rate to: 0.0472
12/28 05:14:43 step 114000 epoch 47 learning rate 0.0472 step-time 0.679 loss 0.774
12/28 05:14:43 starting evaluation
12/28 05:18:52 test bleu=31.25 loss=110.91 penalty=1.000 ratio=1.072
12/28 05:18:52 saving model to models/6_fold_codenn/checkpoints
12/28 05:18:52 finished saving model
12/28 05:31:13   decaying learning rate to: 0.0449
12/28 05:41:31 step 116000 epoch 48 learning rate 0.0449 step-time 0.678 loss 0.749
12/28 05:41:31 starting evaluation
12/28 05:45:40 test bleu=31.73 loss=112.26 penalty=1.000 ratio=1.057
12/28 05:45:40 saving model to models/6_fold_codenn/checkpoints
12/28 05:45:40 finished saving model
12/28 06:03:11   decaying learning rate to: 0.0426
12/28 06:08:18 step 118000 epoch 49 learning rate 0.0426 step-time 0.677 loss 0.716
12/28 06:08:18 starting evaluation
12/28 06:12:28 test bleu=31.78 loss=113.40 penalty=1.000 ratio=1.057
12/28 06:12:28 saving model to models/6_fold_codenn/checkpoints
12/28 06:12:28 finished saving model
12/28 06:35:27 step 120000 epoch 50 learning rate 0.0426 step-time 0.687 loss 0.683
12/28 06:35:27 starting evaluation
12/28 06:39:36 test bleu=32.50 loss=114.46 penalty=1.000 ratio=1.031
12/28 06:39:36 saving model to models/6_fold_codenn/checkpoints
12/28 06:39:36 finished saving model
12/28 06:39:37   decaying learning rate to: 0.0405
12/28 07:02:38 step 122000 epoch 50 learning rate 0.0405 step-time 0.689 loss 0.611
12/28 07:02:38 starting evaluation
12/28 07:06:49 test bleu=32.11 loss=114.89 penalty=1.000 ratio=1.046
12/28 07:06:49 saving model to models/6_fold_codenn/checkpoints
12/28 07:06:49 finished saving model
12/28 07:11:51   decaying learning rate to: 0.0385
12/28 07:29:51 step 124000 epoch 51 learning rate 0.0385 step-time 0.689 loss 0.588
12/28 07:29:51 starting evaluation
12/28 07:34:01 test bleu=32.10 loss=116.22 penalty=1.000 ratio=1.051
12/28 07:34:01 saving model to models/6_fold_codenn/checkpoints
12/28 07:34:01 finished saving model
12/28 07:44:14   decaying learning rate to: 0.0365
12/28 07:57:02 step 126000 epoch 52 learning rate 0.0365 step-time 0.689 loss 0.562
12/28 07:57:02 starting evaluation
12/28 08:01:12 test bleu=32.03 loss=116.82 penalty=1.000 ratio=1.051
12/28 08:01:12 saving model to models/6_fold_codenn/checkpoints
12/28 08:01:12 finished saving model
12/28 08:16:36   decaying learning rate to: 0.0347
12/28 08:24:10 step 128000 epoch 53 learning rate 0.0347 step-time 0.687 loss 0.549
12/28 08:24:10 starting evaluation
12/28 08:28:20 test bleu=31.98 loss=118.72 penalty=1.000 ratio=1.052
12/28 08:28:20 saving model to models/6_fold_codenn/checkpoints
12/28 08:28:20 finished saving model
12/28 08:49:02   decaying learning rate to: 0.033
12/28 08:51:22 step 130000 epoch 54 learning rate 0.033 step-time 0.689 loss 0.527
12/28 08:51:22 starting evaluation
12/28 08:55:31 test bleu=32.10 loss=119.21 penalty=1.000 ratio=1.051
12/28 08:55:31 saving model to models/6_fold_codenn/checkpoints
12/28 08:55:31 finished saving model
12/28 09:18:33 step 132000 epoch 54 learning rate 0.033 step-time 0.689 loss 0.491
12/28 09:18:33 starting evaluation
12/28 09:22:43 test bleu=32.08 loss=119.34 penalty=1.000 ratio=1.046
12/28 09:22:43 saving model to models/6_fold_codenn/checkpoints
12/28 09:22:43 finished saving model
12/28 09:25:25   decaying learning rate to: 0.0313
12/28 09:45:43 step 134000 epoch 55 learning rate 0.0313 step-time 0.688 loss 0.461
12/28 09:45:43 starting evaluation
12/28 09:49:53 test bleu=32.27 loss=119.92 penalty=1.000 ratio=1.047
12/28 09:49:53 saving model to models/6_fold_codenn/checkpoints
12/28 09:49:53 finished saving model
12/28 09:57:49   decaying learning rate to: 0.0298
12/28 10:12:55 step 136000 epoch 56 learning rate 0.0298 step-time 0.688 loss 0.442
12/28 10:12:55 starting evaluation
12/28 10:17:06 test bleu=32.25 loss=121.01 penalty=1.000 ratio=1.045
12/28 10:17:06 saving model to models/6_fold_codenn/checkpoints
12/28 10:17:06 finished saving model
12/28 10:30:12   decaying learning rate to: 0.0283
12/28 10:40:08 step 138000 epoch 57 learning rate 0.0283 step-time 0.689 loss 0.433
12/28 10:40:08 starting evaluation
12/28 10:44:16 test bleu=32.73 loss=121.08 penalty=1.000 ratio=1.031
12/28 10:44:16 saving model to models/6_fold_codenn/checkpoints
12/28 10:44:16 finished saving model
12/28 11:02:36   decaying learning rate to: 0.0269
12/28 11:07:19 step 140000 epoch 58 learning rate 0.0269 step-time 0.689 loss 0.417
12/28 11:07:19 starting evaluation
12/28 11:11:28 test bleu=32.22 loss=122.08 penalty=1.000 ratio=1.048
12/28 11:11:28 saving model to models/6_fold_codenn/checkpoints
12/28 11:11:28 finished saving model
12/28 11:34:31 step 142000 epoch 58 learning rate 0.0269 step-time 0.689 loss 0.410
12/28 11:34:31 starting evaluation
12/28 11:38:42 test bleu=32.35 loss=121.92 penalty=1.000 ratio=1.041
12/28 11:38:42 saving model to models/6_fold_codenn/checkpoints
12/28 11:38:42 finished saving model
12/28 11:39:09   decaying learning rate to: 0.0255
12/28 11:59:33 step 144000 epoch 59 learning rate 0.0255 step-time 0.623 loss 0.372
12/28 11:59:33 starting evaluation
12/28 12:02:21 test bleu=32.14 loss=122.18 penalty=1.000 ratio=1.053
12/28 12:02:22 saving model to models/6_fold_codenn/checkpoints
12/28 12:02:22 finished saving model
12/28 12:06:42   decaying learning rate to: 0.0242
12/28 12:19:42 step 146000 epoch 60 learning rate 0.0242 step-time 0.518 loss 0.357
12/28 12:19:42 starting evaluation
12/28 12:22:26 test bleu=32.32 loss=122.91 penalty=1.000 ratio=1.044
12/28 12:22:26 saving model to models/6_fold_codenn/checkpoints
12/28 12:22:26 finished saving model
12/28 12:30:33   decaying learning rate to: 0.023
12/28 12:39:30 step 148000 epoch 61 learning rate 0.023 step-time 0.510 loss 0.354
12/28 12:39:30 starting evaluation
12/28 12:42:07 test bleu=32.13 loss=123.09 penalty=1.000 ratio=1.051
12/28 12:42:07 saving model to models/6_fold_codenn/checkpoints
12/28 12:42:07 finished saving model
12/28 12:54:02   decaying learning rate to: 0.0219
12/28 12:59:12 step 150000 epoch 62 learning rate 0.0219 step-time 0.510 loss 0.341
12/28 12:59:12 starting evaluation
12/28 13:01:59 test bleu=32.22 loss=123.76 penalty=1.000 ratio=1.050
12/28 13:01:59 saving model to models/6_fold_codenn/checkpoints
12/28 13:01:59 finished saving model
12/28 13:17:35   decaying learning rate to: 0.0208
12/28 13:18:59 step 152000 epoch 63 learning rate 0.0208 step-time 0.508 loss 0.335
12/28 13:18:59 starting evaluation
12/28 13:21:39 test bleu=31.74 loss=124.44 penalty=1.000 ratio=1.062
12/28 13:21:39 saving model to models/6_fold_codenn/checkpoints
12/28 13:21:39 finished saving model
12/28 13:38:42 step 154000 epoch 63 learning rate 0.0208 step-time 0.509 loss 0.316
12/28 13:38:42 starting evaluation
12/28 13:41:26 test bleu=31.86 loss=124.17 penalty=1.000 ratio=1.059
12/28 13:41:26 saving model to models/6_fold_codenn/checkpoints
12/28 13:41:26 finished saving model
12/28 13:43:54   decaying learning rate to: 0.0197
12/28 13:58:34 step 156000 epoch 64 learning rate 0.0197 step-time 0.512 loss 0.302
12/28 13:58:34 starting evaluation
12/28 14:01:16 test bleu=31.61 loss=124.86 penalty=1.000 ratio=1.064
12/28 14:01:16 saving model to models/6_fold_codenn/checkpoints
12/28 14:01:16 finished saving model
12/28 14:07:35   decaying learning rate to: 0.0188
12/28 14:18:30 step 158000 epoch 65 learning rate 0.0188 step-time 0.515 loss 0.292
12/28 14:18:30 starting evaluation
12/28 14:21:14 test bleu=32.09 loss=124.75 penalty=1.000 ratio=1.055
12/28 14:21:14 saving model to models/6_fold_codenn/checkpoints
12/28 14:21:14 finished saving model
12/28 14:31:20   decaying learning rate to: 0.0178
12/28 14:38:19 step 160000 epoch 66 learning rate 0.0178 step-time 0.511 loss 0.292
12/28 14:38:19 starting evaluation
12/28 14:41:02 test bleu=31.88 loss=125.30 penalty=1.000 ratio=1.061
12/28 14:41:02 saving model to models/6_fold_codenn/checkpoints
12/28 14:41:02 finished saving model
12/28 14:55:02   decaying learning rate to: 0.0169
12/28 14:58:08 step 162000 epoch 67 learning rate 0.0169 step-time 0.511 loss 0.285
12/28 14:58:08 starting evaluation
12/28 15:00:52 test bleu=31.93 loss=125.71 penalty=1.000 ratio=1.058
12/28 15:00:52 saving model to models/6_fold_codenn/checkpoints
12/28 15:00:53 finished saving model
12/28 15:18:02 step 164000 epoch 67 learning rate 0.0169 step-time 0.513 loss 0.275
12/28 15:18:02 starting evaluation
12/28 15:20:45 test bleu=31.64 loss=125.80 penalty=1.000 ratio=1.066
12/28 15:20:45 saving model to models/6_fold_codenn/checkpoints
12/28 15:20:45 finished saving model
12/28 15:21:28   decaying learning rate to: 0.0161
12/28 15:37:52 step 166000 epoch 68 learning rate 0.0161 step-time 0.512 loss 0.255
12/28 15:37:52 starting evaluation
12/28 15:40:34 test bleu=31.84 loss=126.38 penalty=1.000 ratio=1.060
12/28 15:40:34 saving model to models/6_fold_codenn/checkpoints
12/28 15:40:34 finished saving model
12/28 15:45:08   decaying learning rate to: 0.0153
12/28 15:57:42 step 168000 epoch 69 learning rate 0.0153 step-time 0.512 loss 0.255
12/28 15:57:42 starting evaluation
12/28 16:00:26 test bleu=32.59 loss=126.35 penalty=1.000 ratio=1.038
12/28 16:00:26 saving model to models/6_fold_codenn/checkpoints
12/28 16:00:26 finished saving model
12/28 16:08:52   decaying learning rate to: 0.0145
12/28 16:18:44 step 170000 epoch 70 learning rate 0.0145 step-time 0.547 loss 0.248
12/28 16:18:44 starting evaluation
12/28 16:22:57 test bleu=32.08 loss=127.00 penalty=1.000 ratio=1.053
12/28 16:22:57 saving model to models/6_fold_codenn/checkpoints
12/28 16:22:57 finished saving model
12/28 16:39:09   decaying learning rate to: 0.0138
12/28 16:45:35 step 172000 epoch 71 learning rate 0.0138 step-time 0.677 loss 0.241
12/28 16:45:35 starting evaluation
12/28 16:49:47 test bleu=32.13 loss=126.93 penalty=1.000 ratio=1.052
12/28 16:49:47 saving model to models/6_fold_codenn/checkpoints
12/28 16:49:48 finished saving model
12/28 17:11:05   decaying learning rate to: 0.0131
12/28 17:12:28 step 174000 epoch 72 learning rate 0.0131 step-time 0.678 loss 0.243
12/28 17:12:28 starting evaluation
12/28 17:16:41 test bleu=32.49 loss=127.58 penalty=1.000 ratio=1.043
12/28 17:16:41 saving model to models/6_fold_codenn/checkpoints
12/28 17:16:41 finished saving model
12/28 17:39:18 step 176000 epoch 72 learning rate 0.0131 step-time 0.676 loss 0.228
12/28 17:39:18 starting evaluation
12/28 17:43:30 test bleu=32.53 loss=127.51 penalty=1.000 ratio=1.040
12/28 17:43:30 saving model to models/6_fold_codenn/checkpoints
12/28 17:43:30 finished saving model
12/28 17:47:17   decaying learning rate to: 0.0124
12/28 18:05:50 step 178000 epoch 73 learning rate 0.0124 step-time 0.668 loss 0.218
12/28 18:05:50 starting evaluation
12/28 18:09:58 test bleu=32.51 loss=127.69 penalty=1.000 ratio=1.039
12/28 18:09:58 saving model to models/6_fold_codenn/checkpoints
12/28 18:09:58 finished saving model
12/28 18:18:33   decaying learning rate to: 0.0118
12/28 18:31:42 step 180000 epoch 74 learning rate 0.0118 step-time 0.650 loss 0.217
12/28 18:31:42 starting evaluation
12/28 18:35:49 test bleu=32.05 loss=128.37 penalty=1.000 ratio=1.055
12/28 18:35:49 saving model to models/6_fold_codenn/checkpoints
12/28 18:35:49 finished saving model
12/28 18:49:11   decaying learning rate to: 0.0112
12/28 18:57:34 step 182000 epoch 75 learning rate 0.0112 step-time 0.651 loss 0.214
12/28 18:57:34 starting evaluation
12/28 19:01:40 test bleu=32.17 loss=128.27 penalty=1.000 ratio=1.050
12/28 19:01:40 saving model to models/6_fold_codenn/checkpoints
12/28 19:01:40 finished saving model
12/28 19:19:46   decaying learning rate to: 0.0107
12/28 19:23:22 step 184000 epoch 76 learning rate 0.0107 step-time 0.649 loss 0.212
12/28 19:23:22 starting evaluation
12/28 19:27:28 test bleu=32.16 loss=128.60 penalty=1.000 ratio=1.051
12/28 19:27:28 saving model to models/6_fold_codenn/checkpoints
12/28 19:27:28 finished saving model
12/28 19:49:17 step 186000 epoch 76 learning rate 0.0107 step-time 0.653 loss 0.207
12/28 19:49:17 starting evaluation
12/28 19:53:26 test bleu=31.74 loss=128.45 penalty=1.000 ratio=1.062
12/28 19:53:26 saving model to models/6_fold_codenn/checkpoints
12/28 19:53:27 finished saving model
12/28 19:54:48   decaying learning rate to: 0.0101
12/28 20:15:10 step 188000 epoch 77 learning rate 0.0101 step-time 0.650 loss 0.198
12/28 20:15:10 starting evaluation
12/28 20:19:17 test bleu=31.70 loss=128.72 penalty=1.000 ratio=1.066
12/28 20:19:17 saving model to models/6_fold_codenn/checkpoints
12/28 20:19:17 finished saving model
12/28 20:25:37   decaying learning rate to: 0.00963
12/28 20:41:02 step 190000 epoch 78 learning rate 0.00963 step-time 0.651 loss 0.191
12/28 20:41:02 starting evaluation
12/28 20:45:10 test bleu=31.96 loss=129.04 penalty=1.000 ratio=1.054
12/28 20:45:10 saving model to models/6_fold_codenn/checkpoints
12/28 20:45:10 finished saving model
12/28 20:56:25   decaying learning rate to: 0.00915
12/28 21:06:56 step 192000 epoch 79 learning rate 0.00915 step-time 0.651 loss 0.190
12/28 21:06:56 starting evaluation
12/28 21:11:03 test bleu=32.25 loss=129.83 penalty=1.000 ratio=1.050
12/28 21:11:03 saving model to models/6_fold_codenn/checkpoints
12/28 21:11:03 finished saving model
12/28 21:26:59   decaying learning rate to: 0.00869
12/28 21:32:51 step 194000 epoch 80 learning rate 0.00869 step-time 0.652 loss 0.191
12/28 21:32:51 starting evaluation
12/28 21:36:58 test bleu=32.01 loss=129.78 penalty=1.000 ratio=1.056
12/28 21:36:58 saving model to models/6_fold_codenn/checkpoints
12/28 21:36:59 finished saving model
12/28 21:57:51   decaying learning rate to: 0.00826
12/28 21:58:44 step 196000 epoch 81 learning rate 0.00826 step-time 0.651 loss 0.189
12/28 21:58:44 starting evaluation
12/28 22:02:51 test bleu=32.09 loss=129.65 penalty=1.000 ratio=1.054
12/28 22:02:51 saving model to models/6_fold_codenn/checkpoints
12/28 22:02:52 finished saving model
12/28 22:24:38 step 198000 epoch 81 learning rate 0.00826 step-time 0.651 loss 0.178
12/28 22:24:38 starting evaluation
12/28 22:28:46 test bleu=32.32 loss=130.11 penalty=1.000 ratio=1.049
12/28 22:28:46 saving model to models/6_fold_codenn/checkpoints
12/28 22:28:46 finished saving model
12/28 22:32:49   decaying learning rate to: 0.00784
12/28 22:50:30 step 200000 epoch 82 learning rate 0.00784 step-time 0.650 loss 0.173
12/28 22:50:30 starting evaluation
12/28 22:54:38 test bleu=32.14 loss=130.13 penalty=1.000 ratio=1.053
12/28 22:54:38 saving model to models/6_fold_codenn/checkpoints
12/28 22:54:38 finished saving model
12/28 23:03:39   decaying learning rate to: 0.00745
12/28 23:16:18 step 202000 epoch 83 learning rate 0.00745 step-time 0.648 loss 0.172
12/28 23:16:18 starting evaluation
12/28 23:20:27 test bleu=32.08 loss=130.41 penalty=1.000 ratio=1.053
12/28 23:20:28 saving model to models/6_fold_codenn/checkpoints
12/28 23:20:28 finished saving model
12/28 23:34:05   decaying learning rate to: 0.00708
12/28 23:42:11 step 204000 epoch 84 learning rate 0.00708 step-time 0.650 loss 0.173
12/28 23:42:11 starting evaluation
12/28 23:46:20 test bleu=31.83 loss=130.64 penalty=1.000 ratio=1.062
12/28 23:46:20 saving model to models/6_fold_codenn/checkpoints
12/28 23:46:20 finished saving model
12/29 00:04:56   decaying learning rate to: 0.00673
12/29 00:08:03 step 206000 epoch 85 learning rate 0.00673 step-time 0.650 loss 0.171
12/29 00:08:03 starting evaluation
12/29 00:12:11 test bleu=32.34 loss=130.83 penalty=1.000 ratio=1.045
12/29 00:12:11 saving model to models/6_fold_codenn/checkpoints
12/29 00:12:12 finished saving model
12/29 00:33:57 step 208000 epoch 85 learning rate 0.00673 step-time 0.651 loss 0.168
12/29 00:33:57 starting evaluation
12/29 00:38:05 test bleu=32.23 loss=130.97 penalty=1.000 ratio=1.049
12/29 00:38:05 saving model to models/6_fold_codenn/checkpoints
12/29 00:38:05 finished saving model
12/29 00:39:54   decaying learning rate to: 0.00639
12/29 00:59:50 step 210000 epoch 86 learning rate 0.00639 step-time 0.650 loss 0.158
12/29 00:59:50 starting evaluation
12/29 01:03:56 test bleu=32.22 loss=131.18 penalty=1.000 ratio=1.050
12/29 01:03:56 saving model to models/6_fold_codenn/checkpoints
12/29 01:03:56 finished saving model
12/29 01:10:44   decaying learning rate to: 0.00607
12/29 01:25:42 step 212000 epoch 87 learning rate 0.00607 step-time 0.651 loss 0.159
12/29 01:25:42 starting evaluation
12/29 01:29:51 test bleu=31.91 loss=131.29 penalty=1.000 ratio=1.055
12/29 01:29:51 saving model to models/6_fold_codenn/checkpoints
12/29 01:29:51 finished saving model
12/29 01:41:18   decaying learning rate to: 0.00577
12/29 01:51:35 step 214000 epoch 88 learning rate 0.00577 step-time 0.650 loss 0.157
12/29 01:51:35 starting evaluation
12/29 01:55:43 test bleu=32.01 loss=131.73 penalty=1.000 ratio=1.054
12/29 01:55:43 saving model to models/6_fold_codenn/checkpoints
12/29 01:55:43 finished saving model
12/29 02:12:50   decaying learning rate to: 0.00548
12/29 02:18:29 step 216000 epoch 89 learning rate 0.00548 step-time 0.680 loss 0.160
12/29 02:18:29 starting evaluation
12/29 02:22:41 test bleu=31.91 loss=131.98 penalty=1.000 ratio=1.058
12/29 02:22:41 saving model to models/6_fold_codenn/checkpoints
12/29 02:22:41 finished saving model
12/29 02:45:54   decaying learning rate to: 0.0052
12/29 02:46:22 step 218000 epoch 90 learning rate 0.0052 step-time 0.708 loss 0.158
12/29 02:46:22 starting evaluation
12/29 02:50:38 test bleu=32.50 loss=131.85 penalty=1.000 ratio=1.040
12/29 02:50:38 saving model to models/6_fold_codenn/checkpoints
12/29 02:50:38 finished saving model
12/29 03:13:45 step 220000 epoch 90 learning rate 0.0052 step-time 0.691 loss 0.150
12/29 03:13:45 starting evaluation
12/29 03:17:56 test bleu=32.21 loss=131.96 penalty=1.000 ratio=1.049
12/29 03:17:56 saving model to models/6_fold_codenn/checkpoints
12/29 03:17:57 finished saving model
12/29 03:22:44   decaying learning rate to: 0.00494
12/29 03:40:37 step 222000 epoch 91 learning rate 0.00494 step-time 0.678 loss 0.147
12/29 03:40:37 starting evaluation
12/29 03:44:45 test bleu=31.74 loss=132.23 penalty=1.000 ratio=1.064
12/29 03:44:45 saving model to models/6_fold_codenn/checkpoints
12/29 03:44:45 finished saving model
12/29 03:54:02   decaying learning rate to: 0.0047
12/29 04:06:30 step 224000 epoch 92 learning rate 0.0047 step-time 0.651 loss 0.146
12/29 04:06:30 starting evaluation
12/29 04:10:38 test bleu=32.33 loss=132.41 penalty=1.000 ratio=1.044
12/29 04:10:38 saving model to models/6_fold_codenn/checkpoints
12/29 04:10:38 finished saving model
12/29 04:24:45   decaying learning rate to: 0.00446
12/29 04:32:35 step 226000 epoch 93 learning rate 0.00446 step-time 0.656 loss 0.150
12/29 04:32:35 starting evaluation
12/29 04:36:49 test bleu=32.01 loss=132.55 penalty=1.000 ratio=1.055
12/29 04:36:49 saving model to models/6_fold_codenn/checkpoints
12/29 04:36:49 finished saving model
12/29 04:56:47   decaying learning rate to: 0.00424
12/29 04:59:32 step 228000 epoch 94 learning rate 0.00424 step-time 0.679 loss 0.147
12/29 04:59:32 starting evaluation
12/29 05:03:44 test bleu=32.11 loss=132.60 penalty=1.000 ratio=1.054
12/29 05:03:44 saving model to models/6_fold_codenn/checkpoints
12/29 05:03:45 finished saving model
12/29 05:26:30 step 230000 epoch 94 learning rate 0.00424 step-time 0.681 loss 0.144
12/29 05:26:30 starting evaluation
12/29 05:30:43 test bleu=32.19 loss=132.64 penalty=1.000 ratio=1.049
12/29 05:30:43 saving model to models/6_fold_codenn/checkpoints
12/29 05:30:43 finished saving model
12/29 05:33:04   decaying learning rate to: 0.00403
12/29 05:53:27 step 232000 epoch 95 learning rate 0.00403 step-time 0.680 loss 0.140
12/29 05:53:27 starting evaluation
12/29 05:57:40 test bleu=32.19 loss=132.87 penalty=1.000 ratio=1.050
12/29 05:57:40 saving model to models/6_fold_codenn/checkpoints
12/29 05:57:40 finished saving model
12/29 06:05:10   decaying learning rate to: 0.00383
12/29 06:20:22 step 234000 epoch 96 learning rate 0.00383 step-time 0.679 loss 0.138
12/29 06:20:22 starting evaluation
12/29 06:24:34 test bleu=32.01 loss=132.84 penalty=1.000 ratio=1.054
12/29 06:24:34 saving model to models/6_fold_codenn/checkpoints
12/29 06:24:34 finished saving model
12/29 06:37:00   decaying learning rate to: 0.00363
12/29 06:47:18 step 236000 epoch 97 learning rate 0.00363 step-time 0.680 loss 0.139
12/29 06:47:18 starting evaluation
12/29 06:51:31 test bleu=31.90 loss=133.16 penalty=1.000 ratio=1.059
12/29 06:51:31 saving model to models/6_fold_codenn/checkpoints
12/29 06:51:31 finished saving model
12/29 07:08:54   decaying learning rate to: 0.00345
12/29 07:13:58 step 238000 epoch 98 learning rate 0.00345 step-time 0.672 loss 0.138
12/29 07:13:58 starting evaluation
12/29 07:18:08 test bleu=32.14 loss=133.40 penalty=1.000 ratio=1.051
12/29 07:18:08 saving model to models/6_fold_codenn/checkpoints
12/29 07:18:08 finished saving model
12/29 07:40:29 step 240000 epoch 99 learning rate 0.00345 step-time 0.668 loss 0.140
12/29 07:40:29 starting evaluation
12/29 07:44:38 test bleu=32.27 loss=133.16 penalty=1.000 ratio=1.046
12/29 07:44:38 saving model to models/6_fold_codenn/checkpoints
12/29 07:44:39 finished saving model
12/29 07:44:40   decaying learning rate to: 0.00328
12/29 08:06:57 step 242000 epoch 99 learning rate 0.00328 step-time 0.667 loss 0.132
12/29 08:06:57 starting evaluation
12/29 08:11:07 test bleu=32.25 loss=133.36 penalty=1.000 ratio=1.049
12/29 08:11:07 saving model to models/6_fold_codenn/checkpoints
12/29 08:11:07 finished saving model
12/29 08:16:08   decaying learning rate to: 0.00312
12/29 08:33:02 step 244000 epoch 100 learning rate 0.00312 step-time 0.655 loss 0.133
12/29 08:33:02 starting evaluation
12/29 08:37:07 test bleu=32.05 loss=133.45 penalty=1.000 ratio=1.056
12/29 08:37:07 saving model to models/6_fold_codenn/checkpoints
12/29 08:37:07 finished saving model
12/29 08:46:33 finished training
12/29 08:46:33 exiting...
12/29 08:46:33 saving model to models/6_fold_codenn/checkpoints
12/29 08:46:33 finished saving model
