nohup: ignoring input
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:107: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:30: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

12/29 10:03:16 label: default
12/29 10:03:16 description:
  default configuration
  next line of description
  last line
12/29 10:03:16 /root/icpc/icpc/translate/__main__.py config/10-folds/9_fold/codenn/config.yaml --train -v
12/29 10:03:16 commit hash 74e0554cb3eb5df835cef993ad570ff8de651f71
12/29 10:03:16 tensorflow version: 1.14.0
12/29 10:03:16 program arguments
12/29 10:03:16   aggregation_method   'sum'
12/29 10:03:16   align_encoder_id     0
12/29 10:03:16   allow_growth         True
12/29 10:03:16   attention_type       'global'
12/29 10:03:16   attn_filter_length   0
12/29 10:03:16   attn_filters         0
12/29 10:03:16   attn_prev_word       False
12/29 10:03:16   attn_size            128
12/29 10:03:16   attn_temperature     1.0
12/29 10:03:16   attn_window_size     0
12/29 10:03:16   average              False
12/29 10:03:16   baseline_activation  None
12/29 10:03:16   baseline_learning_rate 0.001
12/29 10:03:16   baseline_optimizer   'adam'
12/29 10:03:16   baseline_steps       0
12/29 10:03:16   batch_mode           'standard'
12/29 10:03:16   batch_size           64
12/29 10:03:16   beam_size            5
12/29 10:03:16   bidir                True
12/29 10:03:16   bidir_projection     False
12/29 10:03:16   binary               False
12/29 10:03:16   cell_size            256
12/29 10:03:16   cell_type            'GRU'
12/29 10:03:16   character_level      False
12/29 10:03:16   checkpoints          []
12/29 10:03:16   conditional_rnn      False
12/29 10:03:16   config               'config/10-folds/9_fold/codenn/config.yaml'
12/29 10:03:16   convolutions         None
12/29 10:03:16   data_dir             'data/gooddata/9_fold'
12/29 10:03:16   debug                False
12/29 10:03:16   decay_after_n_epoch  1
12/29 10:03:16   decay_every_n_epoch  1
12/29 10:03:16   decay_if_no_progress None
12/29 10:03:16   decoders             [{'max_len': 40, 'name': 'nl'}]
12/29 10:03:16   description          'default configuration\nnext line of description\nlast line\n'
12/29 10:03:16   dev_prefix           'test'
12/29 10:03:16   early_stopping       True
12/29 10:03:16   embedding_dropout    0.0
12/29 10:03:16   embedding_initializer None
12/29 10:03:16   embedding_size       256
12/29 10:03:16   embedding_weight_scale None
12/29 10:03:16   embeddings_on_cpu    True
12/29 10:03:16   encoders             [{'attention_type': 'global', 'max_len': 200, 'name': 'code'}]
12/29 10:03:16   ensemble             False
12/29 10:03:16   eval_burn_in         0
12/29 10:03:16   feed_previous        0.0
12/29 10:03:16   final_state          'last'
12/29 10:03:16   freeze_variables     []
12/29 10:03:16   generate_first       True
12/29 10:03:16   gpu_id               0
12/29 10:03:16   highway_layers       0
12/29 10:03:16   initial_state_dropout 0.0
12/29 10:03:16   initializer          None
12/29 10:03:16   input_layer_dropout  0.0
12/29 10:03:16   input_layers         None
12/29 10:03:16   keep_best            5
12/29 10:03:16   keep_every_n_hours   0
12/29 10:03:16   label                'default'
12/29 10:03:16   layer_norm           False
12/29 10:03:16   layers               1
12/29 10:03:16   learning_rate        0.5
12/29 10:03:16   learning_rate_decay_factor 0.95
12/29 10:03:16   len_normalization    1.0
12/29 10:03:16   log_file             'log.txt'
12/29 10:03:16   loss_function        'xent'
12/29 10:03:16   max_dev_size         0
12/29 10:03:16   max_epochs           100
12/29 10:03:16   max_gradient_norm    5.0
12/29 10:03:16   max_len              50
12/29 10:03:16   max_steps            600000
12/29 10:03:16   max_test_size        0
12/29 10:03:16   max_to_keep          1
12/29 10:03:16   max_train_size       0
12/29 10:03:16   maxout_stride        None
12/29 10:03:16   mem_fraction         1.0
12/29 10:03:16   min_learning_rate    1e-06
12/29 10:03:16   model_dir            'models/9_fold_codenn'
12/29 10:03:16   moving_average       None
12/29 10:03:16   no_gpu               False
12/29 10:03:16   optimizer            'sgd'
12/29 10:03:16   orthogonal_init      False
12/29 10:03:16   output               None
12/29 10:03:16   output_dropout       0.0
12/29 10:03:16   parallel_iterations  16
12/29 10:03:16   pervasive_dropout    False
12/29 10:03:16   pooling_avg          True
12/29 10:03:16   post_process_script  None
12/29 10:03:16   pred_deep_layer      False
12/29 10:03:16   pred_edits           False
12/29 10:03:16   pred_embed_proj      True
12/29 10:03:16   pred_maxout_layer    True
12/29 10:03:16   purge                False
12/29 10:03:16   raw_output           False
12/29 10:03:16   read_ahead           1
12/29 10:03:16   reconstruction_attn_weight 0.05
12/29 10:03:16   reconstruction_decoders False
12/29 10:03:16   reconstruction_weight 1.0
12/29 10:03:16   reinforce_after_n_epoch None
12/29 10:03:16   remove_unk           False
12/29 10:03:16   reverse              False
12/29 10:03:16   reverse_input        True
12/29 10:03:16   reward_function      'sentence_bleu'
12/29 10:03:16   rnn_feed_attn        True
12/29 10:03:16   rnn_input_dropout    0.0
12/29 10:03:16   rnn_output_dropout   0.0
12/29 10:03:16   rnn_state_dropout    0.0
12/29 10:03:16   save                 False
12/29 10:03:16   score_function       'corpus_bleu'
12/29 10:03:16   score_functions      ['bleu', 'loss']
12/29 10:03:16   script_dir           'scripts'
12/29 10:03:16   sgd_after_n_epoch    None
12/29 10:03:16   sgd_learning_rate    1.0
12/29 10:03:16   shuffle              True
12/29 10:03:16   softmax_temperature  1.0
12/29 10:03:16   steps_per_checkpoint 2000
12/29 10:03:16   steps_per_eval       2000
12/29 10:03:16   swap_memory          True
12/29 10:03:16   tie_embeddings       False
12/29 10:03:16   time_pooling         None
12/29 10:03:16   train                True
12/29 10:03:16   train_initial_states True
12/29 10:03:16   train_prefix         'train'
12/29 10:03:16   truncate_lines       True
12/29 10:03:16   update_first         False
12/29 10:03:16   use_baseline         False
12/29 10:03:16   use_dropout          False
12/29 10:03:16   use_lstm_full_state  False
12/29 10:03:16   use_previous_word    True
12/29 10:03:16   verbose              True
12/29 10:03:16   vocab_prefix         'vocab'
12/29 10:03:16   weight_scale         None
12/29 10:03:16   word_dropout         0.0
12/29 10:03:16 python random seed: 7896935899507754207
12/29 10:03:16 tf random seed:     3237312545194715333
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:203: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

12/29 10:03:16 creating model
12/29 10:03:16 using device: /gpu:0
WARNING:tensorflow:From /root/icpc/icpc/translate/__main__.py:230: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

12/29 10:03:16 copying vocab to models/9_fold_codenn/data/vocab.code
12/29 10:03:16 copying vocab to models/9_fold_codenn/data/vocab.nl
12/29 10:03:16 reading vocabularies
12/29 10:03:16 creating model
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:60: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:111: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:33: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f6666161470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f6666161470>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/rnn.py:226: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f66661617b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f66661617b8>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6666156e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6666156e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:838: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66ecec5f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66ecec5f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66ecedee48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66ecedee48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:432: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:435: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66ead41160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66ead41160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66ead57128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66ead57128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66eadac898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66eadac898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66ecec4470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66ecec4470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66ecec4470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66ecec4470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /root/icpc/icpc/translate/models.py:919: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f66eaa5a358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f66eaa5a358>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /root/icpc/icpc/translate/seq2seq_model.py:131: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From /root/icpc/icpc/translate/beam_search.py:223: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f6691602e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <translate.rnn.GRUCell object at 0x7f6691602e48>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f669153bf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f669153bf98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f669153bf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f669153bf98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66914dfa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66914dfa20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66914dfa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66914dfa20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66914dfa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66914dfa20>>: AssertionError: Bad argument number for Name: 3, expecting 4
12/29 10:03:21 model parameters (30)
12/29 10:03:21   baseline_step:0 ()
12/29 10:03:21   decoder_nl/attention_code/U_a/kernel:0 (512, 128)
12/29 10:03:21   decoder_nl/attention_code/W_a/bias:0 (128,)
12/29 10:03:21   decoder_nl/attention_code/W_a/kernel:0 (256, 128)
12/29 10:03:21   decoder_nl/attention_code/v_a:0 (128,)
12/29 10:03:21   decoder_nl/code/initial_state_projection/bias:0 (256,)
12/29 10:03:21   decoder_nl/code/initial_state_projection/kernel:0 (256, 256)
12/29 10:03:21   decoder_nl/gru_cell/candidate/bias:0 (256,)
12/29 10:03:21   decoder_nl/gru_cell/candidate/kernel:0 (1024, 256)
12/29 10:03:21   decoder_nl/gru_cell/gates/bias:0 (512,)
12/29 10:03:21   decoder_nl/gru_cell/gates/kernel:0 (1024, 512)
12/29 10:03:21   decoder_nl/maxout/bias:0 (256,)
12/29 10:03:21   decoder_nl/maxout/kernel:0 (1024, 256)
12/29 10:03:21   decoder_nl/softmax0/kernel:0 (128, 256)
12/29 10:03:21   decoder_nl/softmax1/bias:0 (38060,)
12/29 10:03:21   decoder_nl/softmax1/kernel:0 (256, 38060)
12/29 10:03:21   embedding_code:0 (50000, 256)
12/29 10:03:21   embedding_nl:0 (38060, 256)
12/29 10:03:21   encoder_code/initial_state_bw:0 (256,)
12/29 10:03:21   encoder_code/initial_state_fw:0 (256,)
12/29 10:03:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/bias:0 (256,)
12/29 10:03:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/candidate/kernel:0 (512, 256)
12/29 10:03:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/bias:0 (512,)
12/29 10:03:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/gru_cell/gates/kernel:0 (512, 512)
12/29 10:03:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/bias:0 (256,)
12/29 10:03:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/candidate/kernel:0 (512, 256)
12/29 10:03:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/bias:0 (512,)
12/29 10:03:21   encoder_code/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/gru_cell/gates/kernel:0 (512, 512)
12/29 10:03:21   global_step:0 ()
12/29 10:03:21   learning_rate:0 ()
12/29 10:03:21 number of parameters: 34.36M
WARNING:tensorflow:From /root/icpc/icpc/translate/translation_model.py:666: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

12/29 10:03:22 global step: 0
12/29 10:03:22 baseline step: 0
12/29 10:03:22 reading training data
12/29 10:03:22 total line count: 156721
12/29 10:03:26   lines read: 100000
12/29 10:03:28 files: data/gooddata/9_fold/train.code data/gooddata/9_fold/train.nl
12/29 10:03:28 lines reads: 156721
12/29 10:03:28 reading development data
12/29 10:03:29 files: data/gooddata/9_fold/test.code data/gooddata/9_fold/test.nl
12/29 10:03:29 lines reads: 17413
12/29 10:03:30 starting training
12/29 10:28:11 step 2000 epoch 1 learning rate 0.5 step-time 0.739 loss 78.559
12/29 10:28:11 starting evaluation
12/29 10:32:21 test bleu=2.18 loss=63.48 penalty=0.963 ratio=0.964
12/29 10:32:21 saving model to models/9_fold_codenn/checkpoints
12/29 10:32:22 finished saving model
12/29 10:32:22 new best model
12/29 10:37:54   decaying learning rate to: 0.475
12/29 10:57:05 step 4000 epoch 2 learning rate 0.475 step-time 0.740 loss 58.869
12/29 10:57:06 starting evaluation
12/29 11:01:01 test bleu=4.24 loss=55.62 penalty=0.649 ratio=0.698
12/29 11:01:01 saving model to models/9_fold_codenn/checkpoints
WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
12/29 11:01:01 finished saving model
12/29 11:01:01 new best model
12/29 11:11:46   decaying learning rate to: 0.451
12/29 11:25:25 step 6000 epoch 3 learning rate 0.451 step-time 0.730 loss 52.177
12/29 11:25:25 starting evaluation
12/29 11:29:43 test bleu=6.54 loss=50.70 penalty=1.000 ratio=1.279
12/29 11:29:43 saving model to models/9_fold_codenn/checkpoints
12/29 11:29:44 finished saving model
12/29 11:29:44 new best model
12/29 11:45:55   decaying learning rate to: 0.429
12/29 11:54:01 step 8000 epoch 4 learning rate 0.429 step-time 0.727 loss 47.610
12/29 11:54:01 starting evaluation
12/29 11:58:01 test bleu=10.03 loss=47.33 penalty=0.765 ratio=0.789
12/29 11:58:01 saving model to models/9_fold_codenn/checkpoints
12/29 11:58:01 finished saving model
12/29 11:58:01 new best model
12/29 12:19:45   decaying learning rate to: 0.407
12/29 12:22:16 step 10000 epoch 5 learning rate 0.407 step-time 0.725 loss 44.039
12/29 12:22:16 starting evaluation
12/29 12:26:30 test bleu=12.12 loss=45.22 penalty=0.927 ratio=0.929
12/29 12:26:30 saving model to models/9_fold_codenn/checkpoints
12/29 12:26:30 finished saving model
12/29 12:26:30 new best model
12/29 12:50:43 step 12000 epoch 5 learning rate 0.407 step-time 0.725 loss 40.793
12/29 12:50:43 starting evaluation
12/29 12:55:01 test bleu=13.01 loss=43.33 penalty=1.000 ratio=1.015
12/29 12:55:01 saving model to models/9_fold_codenn/checkpoints
12/29 12:55:01 finished saving model
12/29 12:55:01 new best model
12/29 12:58:03   decaying learning rate to: 0.387
12/29 13:19:11 step 14000 epoch 6 learning rate 0.387 step-time 0.723 loss 37.778
12/29 13:19:11 starting evaluation
12/29 13:23:13 test bleu=15.00 loss=41.91 penalty=0.857 ratio=0.867
12/29 13:23:13 saving model to models/9_fold_codenn/checkpoints
12/29 13:23:13 finished saving model
12/29 13:23:13 new best model
12/29 13:31:48   decaying learning rate to: 0.368
12/29 13:47:22 step 16000 epoch 7 learning rate 0.368 step-time 0.722 loss 35.267
12/29 13:47:22 starting evaluation
12/29 13:51:30 test bleu=16.84 loss=40.90 penalty=0.868 ratio=0.876
12/29 13:51:30 saving model to models/9_fold_codenn/checkpoints
12/29 13:51:30 finished saving model
12/29 13:51:30 new best model
12/29 14:05:40   decaying learning rate to: 0.349
12/29 14:15:51 step 18000 epoch 8 learning rate 0.349 step-time 0.729 loss 33.124
12/29 14:15:51 starting evaluation
12/29 14:20:09 test bleu=18.23 loss=40.64 penalty=0.882 ratio=0.888
12/29 14:20:09 saving model to models/9_fold_codenn/checkpoints
12/29 14:20:09 finished saving model
12/29 14:20:09 new best model
12/29 14:39:53   decaying learning rate to: 0.332
12/29 14:44:54 step 20000 epoch 9 learning rate 0.332 step-time 0.740 loss 31.132
12/29 14:44:54 starting evaluation
12/29 14:48:54 test bleu=19.42 loss=40.43 penalty=0.898 ratio=0.902
12/29 14:48:54 saving model to models/9_fold_codenn/checkpoints
12/29 14:48:54 finished saving model
12/29 14:48:54 new best model
12/29 15:13:30 step 22000 epoch 9 learning rate 0.332 step-time 0.736 loss 29.260
12/29 15:13:30 starting evaluation
12/29 15:17:24 test bleu=20.09 loss=39.40 penalty=0.817 ratio=0.832
12/29 15:17:24 saving model to models/9_fold_codenn/checkpoints
12/29 15:17:24 finished saving model
12/29 15:17:24 new best model
12/29 15:17:51   decaying learning rate to: 0.315
12/29 15:41:37 step 24000 epoch 10 learning rate 0.315 step-time 0.724 loss 26.729
12/29 15:41:37 starting evaluation
12/29 15:45:34 test bleu=21.57 loss=39.44 penalty=0.762 ratio=0.787
12/29 15:45:34 saving model to models/9_fold_codenn/checkpoints
12/29 15:45:34 finished saving model
12/29 15:45:34 new best model
12/29 15:51:22   decaying learning rate to: 0.299
12/29 16:09:40 step 26000 epoch 11 learning rate 0.299 step-time 0.721 loss 24.995
12/29 16:09:40 starting evaluation
12/29 16:13:50 test bleu=23.19 loss=39.54 penalty=0.882 ratio=0.888
12/29 16:13:50 saving model to models/9_fold_codenn/checkpoints
12/29 16:13:50 finished saving model
12/29 16:13:50 new best model
12/29 16:25:05   decaying learning rate to: 0.284
12/29 16:38:04 step 28000 epoch 12 learning rate 0.284 step-time 0.725 loss 23.470
12/29 16:38:04 starting evaluation
12/29 16:42:12 test bleu=24.14 loss=40.39 penalty=0.851 ratio=0.861
12/29 16:42:12 saving model to models/9_fold_codenn/checkpoints
12/29 16:42:12 finished saving model
12/29 16:42:12 new best model
12/29 16:58:56   decaying learning rate to: 0.27
12/29 17:06:27 step 30000 epoch 13 learning rate 0.27 step-time 0.725 loss 22.134
12/29 17:06:27 starting evaluation
12/29 17:10:28 test bleu=24.51 loss=40.67 penalty=0.832 ratio=0.845
12/29 17:10:28 saving model to models/9_fold_codenn/checkpoints
12/29 17:10:29 finished saving model
12/29 17:10:29 new best model
12/29 17:32:40   decaying learning rate to: 0.257
12/29 17:34:41 step 32000 epoch 14 learning rate 0.257 step-time 0.724 loss 20.692
12/29 17:34:41 starting evaluation
12/29 17:38:53 test bleu=25.77 loss=41.70 penalty=0.910 ratio=0.914
12/29 17:38:53 saving model to models/9_fold_codenn/checkpoints
12/29 17:38:54 finished saving model
12/29 17:38:54 new best model
12/29 18:03:11 step 34000 epoch 14 learning rate 0.257 step-time 0.727 loss 18.913
12/29 18:03:11 starting evaluation
12/29 18:07:23 test bleu=26.33 loss=41.27 penalty=0.895 ratio=0.900
12/29 18:07:23 saving model to models/9_fold_codenn/checkpoints
12/29 18:07:23 finished saving model
12/29 18:07:23 new best model
12/29 18:10:56   decaying learning rate to: 0.244
12/29 18:31:58 step 36000 epoch 15 learning rate 0.244 step-time 0.736 loss 17.409
12/29 18:31:58 starting evaluation
12/29 18:36:07 test bleu=26.47 loss=42.21 penalty=0.873 ratio=0.881
12/29 18:36:07 saving model to models/9_fold_codenn/checkpoints
12/29 18:36:07 finished saving model
12/29 18:36:07 new best model
12/29 18:45:11   decaying learning rate to: 0.232
12/29 19:00:50 step 38000 epoch 16 learning rate 0.232 step-time 0.740 loss 16.168
12/29 19:00:50 starting evaluation
12/29 19:04:45 test bleu=27.53 loss=43.64 penalty=0.922 ratio=0.925
12/29 19:04:45 saving model to models/9_fold_codenn/checkpoints
12/29 19:04:45 finished saving model
12/29 19:04:45 new best model
12/29 19:19:07   decaying learning rate to: 0.22
12/29 19:29:16 step 40000 epoch 17 learning rate 0.22 step-time 0.734 loss 15.275
12/29 19:29:16 starting evaluation
12/29 19:33:30 test bleu=28.01 loss=46.05 penalty=0.957 ratio=0.958
12/29 19:33:30 saving model to models/9_fold_codenn/checkpoints
12/29 19:33:30 finished saving model
12/29 19:33:30 new best model
12/29 19:53:03   decaying learning rate to: 0.209
12/29 19:57:35 step 42000 epoch 18 learning rate 0.209 step-time 0.720 loss 14.205
12/29 19:57:35 starting evaluation
12/29 20:01:45 test bleu=28.43 loss=46.30 penalty=0.892 ratio=0.897
12/29 20:01:45 saving model to models/9_fold_codenn/checkpoints
12/29 20:01:45 finished saving model
12/29 20:01:45 new best model
12/29 20:25:56 step 44000 epoch 18 learning rate 0.209 step-time 0.724 loss 13.161
12/29 20:25:56 starting evaluation
12/29 20:30:08 test bleu=29.03 loss=45.60 penalty=0.915 ratio=0.919
12/29 20:30:08 saving model to models/9_fold_codenn/checkpoints
12/29 20:30:08 finished saving model
12/29 20:30:08 new best model
12/29 20:31:09   decaying learning rate to: 0.199
12/29 20:54:20 step 46000 epoch 19 learning rate 0.199 step-time 0.724 loss 11.693
12/29 20:54:20 starting evaluation
12/29 20:58:35 test bleu=29.85 loss=47.84 penalty=0.973 ratio=0.973
12/29 20:58:35 saving model to models/9_fold_codenn/checkpoints
12/29 20:58:35 finished saving model
12/29 20:58:35 new best model
12/29 21:05:12   decaying learning rate to: 0.189
12/29 21:22:47 step 48000 epoch 20 learning rate 0.189 step-time 0.724 loss 10.873
12/29 21:22:47 starting evaluation
12/29 21:27:01 test bleu=29.77 loss=50.34 penalty=0.964 ratio=0.964
12/29 21:27:01 saving model to models/9_fold_codenn/checkpoints
12/29 21:27:02 finished saving model
12/29 21:39:13   decaying learning rate to: 0.179
12/29 21:51:11 step 50000 epoch 21 learning rate 0.179 step-time 0.723 loss 10.165
12/29 21:51:11 starting evaluation
12/29 21:55:28 test bleu=29.08 loss=52.02 penalty=1.000 ratio=1.044
12/29 21:55:28 saving model to models/9_fold_codenn/checkpoints
12/29 21:55:28 finished saving model
12/29 22:13:09   decaying learning rate to: 0.17
12/29 22:19:44 step 52000 epoch 22 learning rate 0.17 step-time 0.726 loss 9.450
12/29 22:19:44 starting evaluation
12/29 22:24:00 test bleu=29.91 loss=54.10 penalty=0.942 ratio=0.943
12/29 22:24:00 saving model to models/9_fold_codenn/checkpoints
12/29 22:24:00 finished saving model
12/29 22:24:00 new best model
12/29 22:47:12   decaying learning rate to: 0.162
12/29 22:48:36 step 54000 epoch 23 learning rate 0.162 step-time 0.736 loss 8.838
12/29 22:48:36 starting evaluation
12/29 22:52:43 test bleu=30.76 loss=55.23 penalty=0.960 ratio=0.961
12/29 22:52:43 saving model to models/9_fold_codenn/checkpoints
12/29 22:52:43 finished saving model
12/29 22:52:43 new best model
12/29 23:17:29 step 56000 epoch 23 learning rate 0.162 step-time 0.741 loss 7.826
12/29 23:17:29 starting evaluation
12/29 23:21:31 test bleu=31.10 loss=54.96 penalty=0.987 ratio=0.988
12/29 23:21:31 saving model to models/9_fold_codenn/checkpoints
12/29 23:21:31 finished saving model
12/29 23:21:31 new best model
12/29 23:25:17   decaying learning rate to: 0.154
12/29 23:45:59 step 58000 epoch 24 learning rate 0.154 step-time 0.731 loss 7.149
12/29 23:45:59 starting evaluation
12/29 23:50:12 test bleu=30.47 loss=57.94 penalty=1.000 ratio=1.022
12/29 23:50:12 saving model to models/9_fold_codenn/checkpoints
12/29 23:50:12 finished saving model
12/29 23:59:10   decaying learning rate to: 0.146
12/30 00:14:16 step 60000 epoch 25 learning rate 0.146 step-time 0.720 loss 6.647
12/30 00:14:16 starting evaluation
12/30 00:18:30 test bleu=31.06 loss=60.14 penalty=1.000 ratio=1.019
12/30 00:18:30 saving model to models/9_fold_codenn/checkpoints
12/30 00:18:30 finished saving model
12/30 00:32:59   decaying learning rate to: 0.139
12/30 00:42:32 step 62000 epoch 26 learning rate 0.139 step-time 0.719 loss 6.171
12/30 00:42:32 starting evaluation
12/30 00:46:43 test bleu=31.67 loss=62.54 penalty=0.985 ratio=0.985
12/30 00:46:43 saving model to models/9_fold_codenn/checkpoints
12/30 00:46:43 finished saving model
12/30 00:46:43 new best model
12/30 01:06:43   decaying learning rate to: 0.132
12/30 01:10:45 step 64000 epoch 27 learning rate 0.132 step-time 0.719 loss 5.721
12/30 01:10:45 starting evaluation
12/30 01:14:59 test bleu=31.86 loss=63.79 penalty=1.000 ratio=1.008
12/30 01:14:59 saving model to models/9_fold_codenn/checkpoints
12/30 01:14:59 finished saving model
12/30 01:14:59 new best model
12/30 01:38:57 step 66000 epoch 27 learning rate 0.132 step-time 0.717 loss 5.245
12/30 01:38:57 starting evaluation
12/30 01:43:09 test bleu=32.21 loss=64.74 penalty=0.989 ratio=0.989
12/30 01:43:09 saving model to models/9_fold_codenn/checkpoints
12/30 01:43:09 finished saving model
12/30 01:43:09 new best model
12/30 01:44:39   decaying learning rate to: 0.125
12/30 02:07:07 step 68000 epoch 28 learning rate 0.125 step-time 0.717 loss 4.598
12/30 02:07:07 starting evaluation
12/30 02:11:19 test bleu=32.44 loss=68.24 penalty=0.996 ratio=0.996
12/30 02:11:19 saving model to models/9_fold_codenn/checkpoints
12/30 02:11:20 finished saving model
12/30 02:11:20 new best model
12/30 02:18:21   decaying learning rate to: 0.119
12/30 02:35:18 step 70000 epoch 29 learning rate 0.119 step-time 0.717 loss 4.307
12/30 02:35:18 starting evaluation
12/30 02:39:30 test bleu=32.31 loss=70.20 penalty=0.982 ratio=0.982
12/30 02:39:30 saving model to models/9_fold_codenn/checkpoints
12/30 02:39:30 finished saving model
12/30 02:52:03   decaying learning rate to: 0.113
12/30 03:03:40 step 72000 epoch 30 learning rate 0.113 step-time 0.723 loss 3.982
12/30 03:03:40 starting evaluation
12/30 03:07:57 test bleu=31.64 loss=72.72 penalty=1.000 ratio=1.033
12/30 03:07:57 saving model to models/9_fold_codenn/checkpoints
12/30 03:07:57 finished saving model
12/30 03:26:14   decaying learning rate to: 0.107
12/30 03:32:49 step 74000 epoch 31 learning rate 0.107 step-time 0.744 loss 3.737
12/30 03:32:49 starting evaluation
12/30 03:36:54 test bleu=31.29 loss=74.52 penalty=1.000 ratio=1.051
12/30 03:36:54 saving model to models/9_fold_codenn/checkpoints
12/30 03:36:54 finished saving model
12/30 04:00:36   decaying learning rate to: 0.102
12/30 04:01:35 step 76000 epoch 32 learning rate 0.102 step-time 0.739 loss 3.458
12/30 04:01:35 starting evaluation
12/30 04:05:42 test bleu=32.84 loss=76.06 penalty=0.998 ratio=0.998
12/30 04:05:42 saving model to models/9_fold_codenn/checkpoints
12/30 04:05:42 finished saving model
12/30 04:05:42 new best model
12/30 04:30:06 step 78000 epoch 32 learning rate 0.102 step-time 0.730 loss 3.032
12/30 04:30:06 starting evaluation
12/30 04:34:22 test bleu=31.52 loss=77.70 penalty=1.000 ratio=1.042
12/30 04:34:22 saving model to models/9_fold_codenn/checkpoints
12/30 04:34:22 finished saving model
12/30 04:38:39   decaying learning rate to: 0.0969
12/30 04:58:41 step 80000 epoch 33 learning rate 0.0969 step-time 0.727 loss 2.777
12/30 04:58:41 starting evaluation
12/30 05:02:58 test bleu=31.38 loss=81.10 penalty=1.000 ratio=1.056
12/30 05:02:58 saving model to models/9_fold_codenn/checkpoints
12/30 05:02:58 finished saving model
12/30 05:12:41   decaying learning rate to: 0.092
12/30 05:27:13 step 82000 epoch 34 learning rate 0.092 step-time 0.725 loss 2.605
12/30 05:27:13 starting evaluation
12/30 05:31:30 test bleu=32.04 loss=82.17 penalty=1.000 ratio=1.042
12/30 05:31:30 saving model to models/9_fold_codenn/checkpoints
12/30 05:31:30 finished saving model
12/30 05:46:36   decaying learning rate to: 0.0874
12/30 05:55:42 step 84000 epoch 35 learning rate 0.0874 step-time 0.724 loss 2.434
12/30 05:55:42 starting evaluation
12/30 05:59:57 test bleu=32.60 loss=84.65 penalty=1.000 ratio=1.023
12/30 05:59:57 saving model to models/9_fold_codenn/checkpoints
12/30 05:59:58 finished saving model
12/30 06:20:41   decaying learning rate to: 0.083
12/30 06:24:11 step 86000 epoch 36 learning rate 0.083 step-time 0.725 loss 2.256
12/30 06:24:11 starting evaluation
12/30 06:28:27 test bleu=32.50 loss=86.60 penalty=1.000 ratio=1.029
12/30 06:28:27 saving model to models/9_fold_codenn/checkpoints
12/30 06:28:28 finished saving model
12/30 06:52:43 step 88000 epoch 36 learning rate 0.083 step-time 0.726 loss 2.049
12/30 06:52:43 starting evaluation
12/30 06:56:55 test bleu=33.56 loss=88.30 penalty=0.995 ratio=0.995
12/30 06:56:55 saving model to models/9_fold_codenn/checkpoints
12/30 06:56:55 finished saving model
12/30 06:56:55 new best model
12/30 06:58:58   decaying learning rate to: 0.0789
12/30 07:21:17 step 90000 epoch 37 learning rate 0.0789 step-time 0.729 loss 1.840
12/30 07:21:17 starting evaluation
12/30 07:25:35 test bleu=32.89 loss=89.56 penalty=1.000 ratio=1.025
12/30 07:25:35 saving model to models/9_fold_codenn/checkpoints
12/30 07:25:35 finished saving model
12/30 07:33:22   decaying learning rate to: 0.0749
12/30 07:51:47 step 92000 epoch 38 learning rate 0.0749 step-time 0.783 loss 1.711
12/30 07:51:47 starting evaluation
12/30 07:56:03 test bleu=33.03 loss=91.47 penalty=1.000 ratio=1.022
12/30 07:56:03 saving model to models/9_fold_codenn/checkpoints
12/30 07:56:03 finished saving model
12/30 08:10:03   decaying learning rate to: 0.0712
12/30 08:22:32 step 94000 epoch 39 learning rate 0.0712 step-time 0.792 loss 1.609
12/30 08:22:32 starting evaluation
12/30 08:27:01 test bleu=33.03 loss=94.07 penalty=1.000 ratio=1.023
12/30 08:27:01 saving model to models/9_fold_codenn/checkpoints
12/30 08:27:01 finished saving model
12/30 08:46:31   decaying learning rate to: 0.0676
12/30 08:52:47 step 96000 epoch 40 learning rate 0.0676 step-time 0.770 loss 1.504
12/30 08:52:47 starting evaluation
12/30 08:57:12 test bleu=32.51 loss=95.42 penalty=1.000 ratio=1.040
12/30 08:57:12 saving model to models/9_fold_codenn/checkpoints
12/30 08:57:12 finished saving model
12/30 09:21:16   decaying learning rate to: 0.0643
12/30 09:21:46 step 98000 epoch 41 learning rate 0.0643 step-time 0.735 loss 1.420
12/30 09:21:46 starting evaluation
12/30 09:26:03 test bleu=33.35 loss=97.46 penalty=1.000 ratio=1.013
12/30 09:26:03 saving model to models/9_fold_codenn/checkpoints
12/30 09:26:03 finished saving model
12/30 09:50:21 step 100000 epoch 41 learning rate 0.0643 step-time 0.727 loss 1.242
12/30 09:50:21 starting evaluation
12/30 09:54:38 test bleu=33.25 loss=99.40 penalty=1.000 ratio=1.016
12/30 09:54:38 saving model to models/9_fold_codenn/checkpoints
12/30 09:54:38 finished saving model
12/30 09:59:43   decaying learning rate to: 0.061
12/30 10:18:59 step 102000 epoch 42 learning rate 0.061 step-time 0.729 loss 1.161
12/30 10:18:59 starting evaluation
12/30 10:23:16 test bleu=33.15 loss=100.35 penalty=1.000 ratio=1.024
12/30 10:23:16 saving model to models/9_fold_codenn/checkpoints
12/30 10:23:16 finished saving model
12/30 10:33:55   decaying learning rate to: 0.058
12/30 10:47:32 step 104000 epoch 43 learning rate 0.058 step-time 0.726 loss 1.098
12/30 10:47:32 starting evaluation
12/30 10:51:49 test bleu=33.04 loss=102.62 penalty=1.000 ratio=1.031
12/30 10:51:49 saving model to models/9_fold_codenn/checkpoints
12/30 10:51:49 finished saving model
12/30 11:08:04   decaying learning rate to: 0.0551
12/30 11:16:18 step 106000 epoch 44 learning rate 0.0551 step-time 0.732 loss 1.030
12/30 11:16:18 starting evaluation
12/30 11:20:40 test bleu=32.21 loss=104.64 penalty=1.000 ratio=1.054
12/30 11:20:40 saving model to models/9_fold_codenn/checkpoints
12/30 11:20:40 finished saving model
12/30 11:42:21   decaying learning rate to: 0.0523
12/30 11:45:21 step 108000 epoch 45 learning rate 0.0523 step-time 0.739 loss 0.976
12/30 11:45:21 starting evaluation
12/30 11:49:25 test bleu=33.03 loss=106.58 penalty=1.000 ratio=1.026
12/30 11:49:25 saving model to models/9_fold_codenn/checkpoints
12/30 11:49:25 finished saving model
12/30 12:14:00 step 110000 epoch 45 learning rate 0.0523 step-time 0.736 loss 0.903
12/30 12:14:00 starting evaluation
12/30 12:18:18 test bleu=32.92 loss=107.13 penalty=1.000 ratio=1.036
12/30 12:18:18 saving model to models/9_fold_codenn/checkpoints
12/30 12:18:18 finished saving model
12/30 12:20:32   decaying learning rate to: 0.0497
12/30 12:42:40 step 112000 epoch 46 learning rate 0.0497 step-time 0.729 loss 0.827
12/30 12:42:40 starting evaluation
12/30 12:46:56 test bleu=33.42 loss=108.18 penalty=1.000 ratio=1.020
12/30 12:46:56 saving model to models/9_fold_codenn/checkpoints
12/30 12:46:56 finished saving model
12/30 12:54:34   decaying learning rate to: 0.0472
12/30 13:11:09 step 114000 epoch 47 learning rate 0.0472 step-time 0.724 loss 0.777
12/30 13:11:09 starting evaluation
12/30 13:15:26 test bleu=33.11 loss=109.73 penalty=1.000 ratio=1.027
12/30 13:15:26 saving model to models/9_fold_codenn/checkpoints
12/30 13:15:26 finished saving model
12/30 13:28:37   decaying learning rate to: 0.0449
12/30 13:39:41 step 116000 epoch 48 learning rate 0.0449 step-time 0.726 loss 0.740
12/30 13:39:41 starting evaluation
12/30 13:43:55 test bleu=34.08 loss=110.97 penalty=0.999 ratio=0.999
12/30 13:43:55 saving model to models/9_fold_codenn/checkpoints
12/30 13:43:55 finished saving model
12/30 13:43:55 new best model
12/30 14:02:34   decaying learning rate to: 0.0426
12/30 14:08:04 step 118000 epoch 49 learning rate 0.0426 step-time 0.723 loss 0.717
12/30 14:08:04 starting evaluation
12/30 14:12:22 test bleu=32.63 loss=112.25 penalty=1.000 ratio=1.041
12/30 14:12:22 saving model to models/9_fold_codenn/checkpoints
12/30 14:12:22 finished saving model
12/30 14:36:38 step 120000 epoch 50 learning rate 0.0426 step-time 0.726 loss 0.690
12/30 14:36:38 starting evaluation
12/30 14:40:52 test bleu=33.96 loss=112.63 penalty=1.000 ratio=1.005
12/30 14:40:52 saving model to models/9_fold_codenn/checkpoints
12/30 14:40:53 finished saving model
12/30 14:40:53   decaying learning rate to: 0.0405
12/30 15:05:08 step 122000 epoch 50 learning rate 0.0405 step-time 0.726 loss 0.606
12/30 15:05:08 starting evaluation
12/30 15:09:25 test bleu=33.18 loss=114.22 penalty=1.000 ratio=1.026
12/30 15:09:25 saving model to models/9_fold_codenn/checkpoints
12/30 15:09:25 finished saving model
12/30 15:14:57   decaying learning rate to: 0.0385
12/30 15:33:49 step 124000 epoch 51 learning rate 0.0385 step-time 0.730 loss 0.586
12/30 15:33:49 starting evaluation
12/30 15:38:11 test bleu=33.76 loss=115.02 penalty=1.000 ratio=1.014
12/30 15:38:11 saving model to models/9_fold_codenn/checkpoints
12/30 15:38:11 finished saving model
12/30 15:49:19   decaying learning rate to: 0.0365
12/30 16:02:56 step 126000 epoch 52 learning rate 0.0365 step-time 0.740 loss 0.562
12/30 16:02:56 starting evaluation
12/30 16:06:56 test bleu=33.89 loss=116.60 penalty=1.000 ratio=1.010
12/30 16:06:56 saving model to models/9_fold_codenn/checkpoints
12/30 16:06:57 finished saving model
12/30 16:23:28   decaying learning rate to: 0.0347
12/30 16:31:31 step 128000 epoch 53 learning rate 0.0347 step-time 0.735 loss 0.545
12/30 16:31:31 starting evaluation
12/30 16:35:49 test bleu=32.97 loss=116.67 penalty=1.000 ratio=1.037
12/30 16:35:49 saving model to models/9_fold_codenn/checkpoints
12/30 16:35:49 finished saving model
12/30 16:57:32   decaying learning rate to: 0.033
12/30 17:00:07 step 130000 epoch 54 learning rate 0.033 step-time 0.727 loss 0.530
12/30 17:00:07 starting evaluation
12/30 17:04:24 test bleu=33.28 loss=117.53 penalty=1.000 ratio=1.025
12/30 17:04:24 saving model to models/9_fold_codenn/checkpoints
12/30 17:04:24 finished saving model
12/30 17:28:37 step 132000 epoch 54 learning rate 0.033 step-time 0.725 loss 0.492
12/30 17:28:37 starting evaluation
12/30 17:32:55 test bleu=32.72 loss=117.92 penalty=1.000 ratio=1.044
12/30 17:32:55 saving model to models/9_fold_codenn/checkpoints
12/30 17:32:55 finished saving model
12/30 17:35:58   decaying learning rate to: 0.0313
12/30 17:57:12 step 134000 epoch 55 learning rate 0.0313 step-time 0.727 loss 0.453
12/30 17:57:12 starting evaluation
12/30 18:01:29 test bleu=33.25 loss=118.93 penalty=1.000 ratio=1.028
12/30 18:01:29 saving model to models/9_fold_codenn/checkpoints
12/30 18:01:29 finished saving model
12/30 18:10:05   decaying learning rate to: 0.0298
12/30 18:25:43 step 136000 epoch 56 learning rate 0.0298 step-time 0.725 loss 0.448
12/30 18:25:43 starting evaluation
12/30 18:30:01 test bleu=32.47 loss=119.02 penalty=1.000 ratio=1.052
12/30 18:30:01 saving model to models/9_fold_codenn/checkpoints
12/30 18:30:01 finished saving model
12/30 18:44:12   decaying learning rate to: 0.0283
12/30 18:54:13 step 138000 epoch 57 learning rate 0.0283 step-time 0.724 loss 0.436
12/30 18:54:13 starting evaluation
12/30 18:58:29 test bleu=33.16 loss=119.87 penalty=1.000 ratio=1.033
12/30 18:58:29 saving model to models/9_fold_codenn/checkpoints
12/30 18:58:29 finished saving model
12/30 19:18:04   decaying learning rate to: 0.0269
12/30 19:22:45 step 140000 epoch 58 learning rate 0.0269 step-time 0.726 loss 0.410
12/30 19:22:45 starting evaluation
12/30 19:27:01 test bleu=33.13 loss=120.83 penalty=1.000 ratio=1.035
12/30 19:27:01 saving model to models/9_fold_codenn/checkpoints
12/30 19:27:01 finished saving model
12/30 19:51:29 step 142000 epoch 58 learning rate 0.0269 step-time 0.732 loss 0.407
12/30 19:51:29 starting evaluation
12/30 19:55:50 test bleu=33.23 loss=120.22 penalty=1.000 ratio=1.029
12/30 19:55:50 saving model to models/9_fold_codenn/checkpoints
12/30 19:55:51 finished saving model
12/30 19:56:22   decaying learning rate to: 0.0255
12/30 20:20:40 step 144000 epoch 59 learning rate 0.0255 step-time 0.743 loss 0.375
12/30 20:20:40 starting evaluation
12/30 20:24:41 test bleu=33.28 loss=121.04 penalty=1.000 ratio=1.030
12/30 20:24:41 saving model to models/9_fold_codenn/checkpoints
12/30 20:24:41 finished saving model
12/30 20:30:39   decaying learning rate to: 0.0242
12/30 20:49:18 step 146000 epoch 60 learning rate 0.0242 step-time 0.737 loss 0.360
12/30 20:49:18 starting evaluation
12/30 20:53:34 test bleu=32.85 loss=121.66 penalty=1.000 ratio=1.041
12/30 20:53:34 saving model to models/9_fold_codenn/checkpoints
12/30 20:53:35 finished saving model
12/30 21:04:40   decaying learning rate to: 0.023
12/30 21:17:51 step 148000 epoch 61 learning rate 0.023 step-time 0.726 loss 0.351
12/30 21:17:51 starting evaluation
12/30 21:22:07 test bleu=33.16 loss=122.25 penalty=1.000 ratio=1.031
12/30 21:22:07 saving model to models/9_fold_codenn/checkpoints
12/30 21:22:07 finished saving model
12/30 21:38:46   decaying learning rate to: 0.0219
12/30 21:46:23 step 150000 epoch 62 learning rate 0.0219 step-time 0.726 loss 0.342
12/30 21:46:23 starting evaluation
12/30 21:50:39 test bleu=32.81 loss=122.52 penalty=1.000 ratio=1.042
12/30 21:50:39 saving model to models/9_fold_codenn/checkpoints
12/30 21:50:39 finished saving model
12/30 22:12:49   decaying learning rate to: 0.0208
12/30 22:14:50 step 152000 epoch 63 learning rate 0.0208 step-time 0.724 loss 0.335
12/30 22:14:50 starting evaluation
12/30 22:19:07 test bleu=32.60 loss=122.85 penalty=1.000 ratio=1.050
12/30 22:19:07 saving model to models/9_fold_codenn/checkpoints
12/30 22:19:07 finished saving model
12/30 22:43:20 step 154000 epoch 63 learning rate 0.0208 step-time 0.724 loss 0.316
12/30 22:43:20 starting evaluation
12/30 22:47:35 test bleu=33.14 loss=122.96 penalty=1.000 ratio=1.035
12/30 22:47:35 saving model to models/9_fold_codenn/checkpoints
12/30 22:47:35 finished saving model
12/30 22:51:11   decaying learning rate to: 0.0197
12/30 23:11:51 step 156000 epoch 64 learning rate 0.0197 step-time 0.726 loss 0.297
12/30 23:11:51 starting evaluation
12/30 23:16:07 test bleu=32.60 loss=123.83 penalty=1.000 ratio=1.052
12/30 23:16:07 saving model to models/9_fold_codenn/checkpoints
12/30 23:16:07 finished saving model
12/30 23:25:16   decaying learning rate to: 0.0188
12/30 23:40:23 step 158000 epoch 65 learning rate 0.0188 step-time 0.726 loss 0.295
12/30 23:40:23 starting evaluation
12/30 23:44:39 test bleu=33.34 loss=123.41 penalty=1.000 ratio=1.029
12/30 23:44:39 saving model to models/9_fold_codenn/checkpoints
12/30 23:44:39 finished saving model
12/30 23:59:23   decaying learning rate to: 0.0178
12/31 00:09:05 step 160000 epoch 66 learning rate 0.0178 step-time 0.731 loss 0.287
12/31 00:09:05 starting evaluation
12/31 00:13:25 test bleu=33.52 loss=124.66 penalty=1.000 ratio=1.024
12/31 00:13:25 saving model to models/9_fold_codenn/checkpoints
12/31 00:13:26 finished saving model
12/31 00:33:41   decaying learning rate to: 0.0169
12/31 00:38:13 step 162000 epoch 67 learning rate 0.0169 step-time 0.742 loss 0.286
12/31 00:38:13 starting evaluation
12/31 00:42:16 test bleu=33.26 loss=124.23 penalty=1.000 ratio=1.031
12/31 00:42:16 saving model to models/9_fold_codenn/checkpoints
12/31 00:42:16 finished saving model
12/31 01:06:52 step 164000 epoch 67 learning rate 0.0169 step-time 0.736 loss 0.276
12/31 01:06:52 starting evaluation
12/31 01:10:55 test bleu=33.10 loss=124.38 penalty=1.000 ratio=1.034
12/31 01:10:55 saving model to models/9_fold_codenn/checkpoints
12/31 01:10:55 finished saving model
12/31 01:11:49   decaying learning rate to: 0.0161
12/31 01:35:15 step 166000 epoch 68 learning rate 0.0161 step-time 0.728 loss 0.256
12/31 01:35:15 starting evaluation
12/31 01:39:29 test bleu=32.41 loss=124.71 penalty=1.000 ratio=1.058
12/31 01:39:29 saving model to models/9_fold_codenn/checkpoints
12/31 01:39:29 finished saving model
12/31 01:45:33   decaying learning rate to: 0.0153
12/31 02:03:35 step 168000 epoch 69 learning rate 0.0153 step-time 0.721 loss 0.250
12/31 02:03:35 starting evaluation
12/31 02:07:50 test bleu=33.07 loss=124.97 penalty=1.000 ratio=1.038
12/31 02:07:50 saving model to models/9_fold_codenn/checkpoints
12/31 02:07:50 finished saving model
12/31 02:19:20   decaying learning rate to: 0.0145
12/31 02:31:54 step 170000 epoch 70 learning rate 0.0145 step-time 0.720 loss 0.247
12/31 02:31:54 starting evaluation
12/31 02:36:10 test bleu=32.79 loss=125.77 penalty=1.000 ratio=1.046
12/31 02:36:10 saving model to models/9_fold_codenn/checkpoints
12/31 02:36:10 finished saving model
12/31 02:53:16   decaying learning rate to: 0.0138
12/31 03:00:18 step 172000 epoch 71 learning rate 0.0138 step-time 0.722 loss 0.244
12/31 03:00:18 starting evaluation
12/31 03:04:32 test bleu=33.18 loss=126.22 penalty=1.000 ratio=1.035
12/31 03:04:32 saving model to models/9_fold_codenn/checkpoints
12/31 03:04:32 finished saving model
12/31 03:27:11   decaying learning rate to: 0.0131
12/31 03:28:41 step 174000 epoch 72 learning rate 0.0131 step-time 0.723 loss 0.240
12/31 03:28:41 starting evaluation
12/31 03:32:56 test bleu=33.19 loss=126.05 penalty=1.000 ratio=1.033
12/31 03:32:56 saving model to models/9_fold_codenn/checkpoints
12/31 03:32:56 finished saving model
12/31 03:57:04 step 176000 epoch 72 learning rate 0.0131 step-time 0.722 loss 0.229
12/31 03:57:04 starting evaluation
12/31 04:01:19 test bleu=32.89 loss=125.71 penalty=1.000 ratio=1.039
12/31 04:01:19 saving model to models/9_fold_codenn/checkpoints
12/31 04:01:19 finished saving model
12/31 04:05:25   decaying learning rate to: 0.0124
12/31 04:25:30 step 178000 epoch 73 learning rate 0.0124 step-time 0.724 loss 0.216
12/31 04:25:30 starting evaluation
12/31 04:29:45 test bleu=33.30 loss=126.89 penalty=1.000 ratio=1.030
12/31 04:29:45 saving model to models/9_fold_codenn/checkpoints
12/31 04:29:45 finished saving model
12/31 04:39:17   decaying learning rate to: 0.0118
12/31 04:53:55 step 180000 epoch 74 learning rate 0.0118 step-time 0.723 loss 0.217
12/31 04:53:55 starting evaluation
12/31 04:58:13 test bleu=32.86 loss=126.86 penalty=1.000 ratio=1.041
12/31 04:58:13 saving model to models/9_fold_codenn/checkpoints
12/31 04:58:13 finished saving model
12/31 05:13:21   decaying learning rate to: 0.0112
12/31 05:22:44 step 182000 epoch 75 learning rate 0.0112 step-time 0.733 loss 0.215
12/31 05:22:44 starting evaluation
12/31 05:26:58 test bleu=32.95 loss=127.19 penalty=1.000 ratio=1.041
12/31 05:26:58 saving model to models/9_fold_codenn/checkpoints
12/31 05:26:58 finished saving model
12/31 05:47:41   decaying learning rate to: 0.0107
12/31 05:51:41 step 184000 epoch 76 learning rate 0.0107 step-time 0.740 loss 0.213
12/31 05:51:41 starting evaluation
12/31 05:55:39 test bleu=32.83 loss=127.29 penalty=1.000 ratio=1.045
12/31 05:55:39 saving model to models/9_fold_codenn/checkpoints
12/31 05:55:39 finished saving model
12/31 06:20:15 step 186000 epoch 76 learning rate 0.0107 step-time 0.736 loss 0.206
12/31 06:20:15 starting evaluation
12/31 06:24:30 test bleu=33.20 loss=127.10 penalty=1.000 ratio=1.034
12/31 06:24:30 saving model to models/9_fold_codenn/checkpoints
12/31 06:24:30 finished saving model
12/31 06:25:50   decaying learning rate to: 0.0101
12/31 06:48:39 step 188000 epoch 77 learning rate 0.0101 step-time 0.723 loss 0.196
12/31 06:48:39 starting evaluation
12/31 06:52:55 test bleu=32.94 loss=127.54 penalty=1.000 ratio=1.040
12/31 06:52:55 saving model to models/9_fold_codenn/checkpoints
12/31 06:52:55 finished saving model
12/31 06:59:30   decaying learning rate to: 0.00963
12/31 07:17:02 step 190000 epoch 78 learning rate 0.00963 step-time 0.721 loss 0.191
12/31 07:17:02 starting evaluation
12/31 07:21:17 test bleu=33.04 loss=127.58 penalty=1.000 ratio=1.039
12/31 07:21:17 saving model to models/9_fold_codenn/checkpoints
12/31 07:21:18 finished saving model
12/31 07:33:23   decaying learning rate to: 0.00915
12/31 07:45:29 step 192000 epoch 79 learning rate 0.00915 step-time 0.724 loss 0.191
12/31 07:45:29 starting evaluation
12/31 07:49:45 test bleu=32.91 loss=128.20 penalty=1.000 ratio=1.042
12/31 07:49:45 saving model to models/9_fold_codenn/checkpoints
12/31 07:49:45 finished saving model
12/31 08:07:23   decaying learning rate to: 0.00869
12/31 08:13:55 step 194000 epoch 80 learning rate 0.00869 step-time 0.723 loss 0.191
12/31 08:13:55 starting evaluation
12/31 08:18:10 test bleu=33.13 loss=128.25 penalty=1.000 ratio=1.036
12/31 08:18:10 saving model to models/9_fold_codenn/checkpoints
12/31 08:18:11 finished saving model
12/31 08:41:17   decaying learning rate to: 0.00826
12/31 08:42:16 step 196000 epoch 81 learning rate 0.00826 step-time 0.721 loss 0.188
12/31 08:42:16 starting evaluation
12/31 08:46:31 test bleu=32.93 loss=128.33 penalty=1.000 ratio=1.042
12/31 08:46:31 saving model to models/9_fold_codenn/checkpoints
12/31 08:46:31 finished saving model
12/31 09:10:43 step 198000 epoch 81 learning rate 0.00826 step-time 0.724 loss 0.178
12/31 09:10:43 starting evaluation
12/31 09:14:58 test bleu=32.83 loss=128.48 penalty=1.000 ratio=1.044
12/31 09:14:58 saving model to models/9_fold_codenn/checkpoints
12/31 09:14:58 finished saving model
12/31 09:19:32   decaying learning rate to: 0.00784
12/31 09:39:13 step 200000 epoch 82 learning rate 0.00784 step-time 0.725 loss 0.171
12/31 09:39:13 starting evaluation
12/31 09:43:28 test bleu=33.00 loss=128.99 penalty=1.000 ratio=1.039
12/31 09:43:28 saving model to models/9_fold_codenn/checkpoints
12/31 09:43:29 finished saving model
12/31 09:53:39   decaying learning rate to: 0.00745
12/31 10:08:00 step 202000 epoch 83 learning rate 0.00745 step-time 0.734 loss 0.176
12/31 10:08:00 starting evaluation
12/31 10:12:20 test bleu=33.13 loss=128.92 penalty=1.000 ratio=1.036
12/31 10:12:20 saving model to models/9_fold_codenn/checkpoints
12/31 10:12:21 finished saving model
12/31 10:27:57   decaying learning rate to: 0.00708
12/31 10:37:04 step 204000 epoch 84 learning rate 0.00708 step-time 0.740 loss 0.170
12/31 10:37:04 starting evaluation
12/31 10:41:06 test bleu=32.88 loss=129.29 penalty=1.000 ratio=1.038
12/31 10:41:06 saving model to models/9_fold_codenn/checkpoints
12/31 10:41:06 finished saving model
12/31 11:02:15   decaying learning rate to: 0.00673
12/31 11:05:46 step 206000 epoch 85 learning rate 0.00673 step-time 0.738 loss 0.172
12/31 11:05:46 starting evaluation
12/31 11:09:53 test bleu=33.04 loss=129.40 penalty=1.000 ratio=1.039
12/31 11:09:53 saving model to models/9_fold_codenn/checkpoints
12/31 11:09:53 finished saving model
12/31 11:34:17 step 208000 epoch 85 learning rate 0.00673 step-time 0.730 loss 0.168
12/31 11:34:17 starting evaluation
12/31 11:38:33 test bleu=33.10 loss=129.50 penalty=1.000 ratio=1.033
12/31 11:38:33 saving model to models/9_fold_codenn/checkpoints
12/31 11:38:33 finished saving model
12/31 11:40:35   decaying learning rate to: 0.00639
12/31 12:02:44 step 210000 epoch 86 learning rate 0.00639 step-time 0.724 loss 0.161
12/31 12:02:44 starting evaluation
12/31 12:06:59 test bleu=32.97 loss=129.59 penalty=1.000 ratio=1.039
12/31 12:06:59 saving model to models/9_fold_codenn/checkpoints
12/31 12:06:59 finished saving model
12/31 12:14:12   decaying learning rate to: 0.00607
12/31 12:31:05 step 212000 epoch 87 learning rate 0.00607 step-time 0.721 loss 0.158
12/31 12:31:05 starting evaluation
12/31 12:35:20 test bleu=33.39 loss=129.84 penalty=1.000 ratio=1.029
12/31 12:35:20 saving model to models/9_fold_codenn/checkpoints
12/31 12:35:20 finished saving model
12/31 12:47:58   decaying learning rate to: 0.00577
12/31 12:59:26 step 214000 epoch 88 learning rate 0.00577 step-time 0.721 loss 0.157
12/31 12:59:26 starting evaluation
12/31 13:03:41 test bleu=33.20 loss=130.11 penalty=1.000 ratio=1.031
12/31 13:03:41 saving model to models/9_fold_codenn/checkpoints
12/31 13:03:41 finished saving model
12/31 13:21:52   decaying learning rate to: 0.00548
12/31 13:27:53 step 216000 epoch 89 learning rate 0.00548 step-time 0.724 loss 0.158
12/31 13:27:53 starting evaluation
12/31 13:32:07 test bleu=32.87 loss=130.28 penalty=1.000 ratio=1.043
12/31 13:32:07 saving model to models/9_fold_codenn/checkpoints
12/31 13:32:08 finished saving model
12/31 13:55:47   decaying learning rate to: 0.0052
12/31 13:56:17 step 218000 epoch 90 learning rate 0.0052 step-time 0.723 loss 0.157
12/31 13:56:17 starting evaluation
12/31 14:00:30 test bleu=33.05 loss=130.31 penalty=1.000 ratio=1.041
12/31 14:00:30 saving model to models/9_fold_codenn/checkpoints
12/31 14:00:30 finished saving model
12/31 14:24:38 step 220000 epoch 90 learning rate 0.0052 step-time 0.722 loss 0.148
12/31 14:24:38 starting evaluation
12/31 14:28:51 test bleu=33.15 loss=130.40 penalty=1.000 ratio=1.036
12/31 14:28:51 saving model to models/9_fold_codenn/checkpoints
12/31 14:28:51 finished saving model
12/31 14:33:52   decaying learning rate to: 0.00494
12/31 14:52:55 step 222000 epoch 91 learning rate 0.00494 step-time 0.720 loss 0.150
12/31 14:52:55 starting evaluation
12/31 14:57:09 test bleu=33.19 loss=130.69 penalty=1.000 ratio=1.035
12/31 14:57:09 saving model to models/9_fold_codenn/checkpoints
12/31 14:57:10 finished saving model
12/31 15:07:46   decaying learning rate to: 0.0047
12/31 15:21:30 step 224000 epoch 92 learning rate 0.0047 step-time 0.728 loss 0.147
12/31 15:21:30 starting evaluation
12/31 15:25:46 test bleu=33.04 loss=130.86 penalty=1.000 ratio=1.040
12/31 15:25:46 saving model to models/9_fold_codenn/checkpoints
12/31 15:25:47 finished saving model
12/31 15:41:52   decaying learning rate to: 0.00446
12/31 15:50:24 step 226000 epoch 93 learning rate 0.00446 step-time 0.737 loss 0.145
12/31 15:50:24 starting evaluation
12/31 15:54:26 test bleu=33.03 loss=131.04 penalty=1.000 ratio=1.038
12/31 15:54:26 saving model to models/9_fold_codenn/checkpoints
12/31 15:54:26 finished saving model
12/31 16:15:57   decaying learning rate to: 0.00424
12/31 16:18:56 step 228000 epoch 94 learning rate 0.00424 step-time 0.733 loss 0.148
12/31 16:18:56 starting evaluation
12/31 16:22:53 test bleu=32.97 loss=131.13 penalty=1.000 ratio=1.038
12/31 16:22:53 saving model to models/9_fold_codenn/checkpoints
12/31 16:22:53 finished saving model
12/31 16:47:17 step 230000 epoch 94 learning rate 0.00424 step-time 0.730 loss 0.144
12/31 16:47:17 starting evaluation
12/31 16:51:30 test bleu=33.10 loss=131.13 penalty=1.000 ratio=1.035
12/31 16:51:30 saving model to models/9_fold_codenn/checkpoints
12/31 16:51:30 finished saving model
12/31 16:53:46   decaying learning rate to: 0.00403
12/31 17:15:34 step 232000 epoch 95 learning rate 0.00403 step-time 0.720 loss 0.139
12/31 17:15:34 starting evaluation
12/31 17:19:45 test bleu=33.07 loss=131.30 penalty=1.000 ratio=1.037
12/31 17:19:45 saving model to models/9_fold_codenn/checkpoints
12/31 17:19:45 finished saving model
12/31 17:27:18   decaying learning rate to: 0.00383
12/31 17:43:55 step 234000 epoch 96 learning rate 0.00383 step-time 0.723 loss 0.141
12/31 17:43:55 starting evaluation
12/31 17:48:07 test bleu=32.92 loss=131.38 penalty=1.000 ratio=1.042
12/31 17:48:07 saving model to models/9_fold_codenn/checkpoints
12/31 17:48:07 finished saving model
12/31 18:01:06   decaying learning rate to: 0.00363
12/31 18:12:07 step 236000 epoch 97 learning rate 0.00363 step-time 0.718 loss 0.138
12/31 18:12:07 starting evaluation
12/31 18:16:20 test bleu=32.87 loss=131.48 penalty=1.000 ratio=1.043
12/31 18:16:20 saving model to models/9_fold_codenn/checkpoints
12/31 18:16:21 finished saving model
12/31 18:34:53   decaying learning rate to: 0.00345
12/31 18:40:20 step 238000 epoch 98 learning rate 0.00345 step-time 0.718 loss 0.139
12/31 18:40:20 starting evaluation
12/31 18:44:32 test bleu=33.11 loss=131.79 penalty=1.000 ratio=1.034
12/31 18:44:32 saving model to models/9_fold_codenn/checkpoints
12/31 18:44:33 finished saving model
12/31 19:08:35 step 240000 epoch 99 learning rate 0.00345 step-time 0.719 loss 0.139
12/31 19:08:35 starting evaluation
12/31 19:12:48 test bleu=32.97 loss=131.90 penalty=1.000 ratio=1.040
12/31 19:12:48 saving model to models/9_fold_codenn/checkpoints
12/31 19:12:48 finished saving model
12/31 19:12:49   decaying learning rate to: 0.00328
12/31 19:36:46 step 242000 epoch 99 learning rate 0.00328 step-time 0.717 loss 0.132
12/31 19:36:46 starting evaluation
12/31 19:40:58 test bleu=33.07 loss=131.76 penalty=1.000 ratio=1.037
12/31 19:40:58 saving model to models/9_fold_codenn/checkpoints
12/31 19:40:58 finished saving model
12/31 19:46:30   decaying learning rate to: 0.00312
12/31 20:04:55 step 244000 epoch 100 learning rate 0.00312 step-time 0.717 loss 0.132
12/31 20:04:55 starting evaluation
12/31 20:09:07 test bleu=32.98 loss=132.12 penalty=1.000 ratio=1.039
12/31 20:09:07 saving model to models/9_fold_codenn/checkpoints
12/31 20:09:07 finished saving model
12/31 20:19:52 finished training
12/31 20:19:52 exiting...
12/31 20:19:52 saving model to models/9_fold_codenn/checkpoints
12/31 20:19:52 finished saving model
